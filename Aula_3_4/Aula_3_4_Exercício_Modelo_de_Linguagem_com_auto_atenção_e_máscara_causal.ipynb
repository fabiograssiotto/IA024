{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção e máscaras causais\n",
        "\n",
        "Seguimos na mesma linha de treinar um modelo de linguagem a partir dos textos do livro \"O Guarani\", de José de Alencar.\n",
        "\n",
        "Neste exercício, vamos treinar um modelo de linguagem com auto-atenção e com máscara causal. A máscara causal é necessária para que o modelo não tenha acesso a palavras futuras, que é a abordagem usada por grandes modelos de linguagem, como o GPT.\n",
        "\n",
        "Use a implementação matricial de auto-atenção da aula passada.\n",
        "\n",
        "### Modificações necessárias\n",
        "\n",
        "* Adicione a máscara causal na função `forward` da cabeça de auto-atenção.\n",
        "* Modifique o nosso dataloader para retornar inputs (uma lista de tokens de tamanho $n$), targets (uma lista de tokens de tamanho $n$ deslocada para a esquerda em 1 token). Exemplo `input = [1, 2, 3, 4]`, `target = [2, 3, 4, 5]` para a sequência `[1, 2, 3, 4, 5]` com `seq_len=4`, por exemplo (Ver slide 50).\n",
        "\n",
        "### Extra\n",
        "* MultiHeadAttention: modifique a cabeça de auto-atenção para ter múltiplas cabeças. Isso não é obrigatório, mas pode ser interessante para ver como o modelo se comporta.\n",
        "* Diagrama da geração: fazer diagrama que mostre os passos da geração de tokens (conforme slide 47).\n",
        "\n",
        "### Dicas\n",
        "\n",
        "* Use como base o vídeo do Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY. Observe que, no vídeo, ele primeiro implementa um modelo bi-grama, depois um modelo de linguagem com auto-atenção. O modelo de auto-atenção é implementado por volta do minuto 40, mas vale a pena assistir o vídeo todo.\n",
        "* Use esta implementação como base: https://colab.research.google.com/drive/1vFTg4MSXVJwNSzPjaCcvmqhxTP7gK7HA?usp=sharing. Observe como o modelo é organizado e como a máscara é implementada na classe MultiHeadAttention.\n",
        "* Use `context_size=9`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variáveis Globais e Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global variables\n",
        "\n",
        "# Vocabulary\n",
        "vocab_size = 5000\n",
        "context_size = 5\n",
        "pattern = r'\\w+|[,;.:!?\\']'\n",
        "\n",
        "# Training\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "lr = 0.1\n",
        "\n",
        "# Model\n",
        "embedding_dim = 256\n",
        "hidden_dim = 128\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# Colab environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if (IN_COLAB):\n",
        "    %pip install colorama\n",
        "\n",
        "    # Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    project_folder=\"/content/drive/MyDrive/Classes/IA024/Aula_2_3\"\n",
        "    os.chdir(project_folder)\n",
        "    !ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "# Check if download is necessary\n",
        "if not os.path.exists(\"67724.txt.utf-8\"):\n",
        "    print(\"Downloading Gutenberg texts\")\n",
        "\n",
        "    !wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "    !wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4969"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = open(\"67724.txt.utf-8\",\"r\").read()\n",
        "text += open(\"67725.txt.utf-8\",\"r\").read()\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg eBook of O Guarany: romance brazileiro, Vol. 1 (of 2)\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n"
          ]
        }
      ],
      "source": [
        "# Checking the text\n",
        "print(paragraphs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Tua mãi fica! disse a india com um accento de resolução.\n",
            "\n",
            " PAG. 156.--=Cardo=.\n",
            "\n",
            "O silencio reinava na habitação e seus arredores, tudo estava tranquillo e sereno. Algumas estrellas brilhavão no céo; os sopros escassos da viração susurravão na folhagem.\n",
            "\n",
            "Depois desprendendo-se com um esforço, encaminhou-se apressadamente para a escada e desceu ao valle; ahi recebeu a benção de seu pai e abraçando a Alvaro saltou na sella do cavallo, que Ayres Gomes tinha pela redea.\n",
            "\n",
            "Volunteers and financial support to provide volunteers with the assistance they need are critical to reaching Project Gutenberg™’s goals and ensuring that the Project Gutenberg™ collection will remain freely available for generations to come. In 2001, the Project Gutenberg Literary Archive Foundation was created to provide a secure and permanent future for Project Gutenberg™ and future generations. To learn more about the Project Gutenberg Literary Archive Foundation and how your efforts and donations can help, see Sections 3 and 4 and the Foundation information page at www.gutenberg.org.\n",
            "\n",
            "Number of paragraphs: 4892\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4892"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "# Print 5 random paragraphs\n",
        "num_paragraphs = len(cleaned_paragraphs)\n",
        "for i in range(0,5):\n",
        "    idx = random.randrange(num_paragraphs)\n",
        "    print(f\"{cleaned_paragraphs[idx]}\\n\")\n",
        "\n",
        "print(\"Number of paragraphs: \" + str(num_paragraphs))\n",
        "\n",
        "len(cleaned_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12603"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(r'\\w+', text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever.\n",
            "[0, 139, 4376, 19, 0, 0, 6, 44, 110, 269, 259, 2662, 10, 1064, 0, 0, 2, 186, 130, 280, 3, 2257, 6, 0, 1, 2665, 0]\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(pattern, sentence.lower())]\n",
        "\n",
        "print(cleaned_paragraphs[20])\n",
        "print(encode_sentence(cleaned_paragraphs[20], vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, paragraphs, vocab, context, seq_len):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.vocab = vocab\n",
        "    self.context = context\n",
        "    self.seq_len = seq_len\n",
        "    self.tokens, self.targets = self.setup()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.tokens[idx]), torch.tensor(self.targets[idx])\n",
        "  \n",
        "  def setup(self):\n",
        "    tokens = []\n",
        "    targets = []\n",
        "    for paragraph in self.paragraphs:\n",
        "      encoded = encode_sentence(paragraph, self.vocab)\n",
        "      \n",
        "      # If paragraph is smaller than the context, skip it.\n",
        "      if len(encoded) < self.context + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(len(encoded) - self.context):\n",
        "        tks = encoded[i:i+self.context]\n",
        "        tgt = encoded[i+self.context]\n",
        "        # Only add if there are no unknown tokens in both context and target.\n",
        "        bad_token = 0\n",
        "        if not (bad_token in tks or tgt == bad_token):\n",
        "          tokens.append(tks)\n",
        "          targets.append(tgt)\n",
        "    return tokens, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Modificado para Máscara Causal\n",
        "\n",
        "Modifique o nosso dataloader para retornar inputs (uma lista de tokens de tamanho $n$), targets (uma lista de tokens de tamanho $n$ deslocada para a esquerda em 1 token). Exemplo `input = [1, 2, 3, 4]`, `target = [2, 3, 4, 5]` para a sequência `[1, 2, 3, 4, 5]` com `seq_len=4`, por exemplo (Ver slide 50)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CausalMaskDataset(Dataset):\n",
        "  def __init__(self, paragraphs, vocab, seq_len):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.vocab = vocab\n",
        "    self.seq_len = seq_len\n",
        "    self.tokens, self.targets = self.setup()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    tokens = torch.tensor(self.tokens[idx])\n",
        "    targets = torch.tensor(self.targets[idx])\n",
        "    return tokens, targets\n",
        "  \n",
        "  def setup(self):\n",
        "    tokens = []\n",
        "    targets = []\n",
        "    for paragraph in self.paragraphs:\n",
        "      encoded = encode_sentence(paragraph, self.vocab)\n",
        "      \n",
        "      # If paragraph is smaller than the sequence length, skip it.\n",
        "      if len(encoded) < self.seq_len + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(len(encoded) - self.seq_len):\n",
        "        tks = encoded[i:i+self.seq_len]\n",
        "        # Return targets with seq_len instead of a single one.\n",
        "        tgt = encoded[i+1:i+1+self.seq_len]\n",
        "        # Only add if there are no unknown tokens in both context and target.\n",
        "        bad_token = 0\n",
        "        if not (bad_token in tks or tgt == bad_token):\n",
        "          tokens.append(tks)\n",
        "          targets.append(tgt)\n",
        "    return tokens, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Verificação básica do Dataset modificado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([ 35,  67,  59, 537]), tensor([ 67,  59, 537,  47]))\n"
          ]
        }
      ],
      "source": [
        "tst_paragraphs = cleaned_paragraphs[:10]\n",
        "seq_len = 4\n",
        "tst_dataset = CausalMaskDataset(tst_paragraphs, vocab, seq_len)\n",
        "tst_dataloader = DataLoader(tst_dataset, batch_size=1, shuffle=True)\n",
        "tst_sample = next(iter(tst_dataset))\n",
        "\n",
        "print(tst_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verificação do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "y1aetOpmDeQd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 3913\n",
            "Validation samples: 979\n",
            "\n",
            "Training dataset samples: 52518\n",
            "Validation dataset samples: 13564\n"
          ]
        }
      ],
      "source": [
        "# Train/Validation split\n",
        "train_data, val_data = train_test_split(cleaned_paragraphs, test_size=0.2, random_state=18)\n",
        "\n",
        "#train_dataset = CustomDataset(train_data, vocab, context_size)\n",
        "#val_dataset = CustomDataset(val_data, vocab, context_size)\n",
        "\n",
        "seq_len = 3\n",
        "train_dataset = CausalMaskDataset(train_data, vocab, seq_len)\n",
        "val_dataset = CausalMaskDataset(val_data, vocab, seq_len)\n",
        "\n",
        "# Counting all Samples\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print()\n",
        "print(f\"Training dataset samples: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[   7,  257,    4],\n",
            "        [  19,    2,  352],\n",
            "        [ 234,  124, 3181],\n",
            "        [ 101,   34,    5],\n",
            "        [   2,  247,   53],\n",
            "        [   8,  516,    8],\n",
            "        [  33,   21, 2733],\n",
            "        [   4,   29,  749],\n",
            "        [1285,   50,    5],\n",
            "        [ 273,  209,    4],\n",
            "        [2931,    5, 3730],\n",
            "        [2555,    5,  847],\n",
            "        [  81,   50, 2143],\n",
            "        [3936,  356,    1],\n",
            "        [  27,    1,   14],\n",
            "        [  28,   40,    8],\n",
            "        [3729,   87,    2],\n",
            "        [  10,  614, 1802],\n",
            "        [ 293,    9,  240],\n",
            "        [   1,  340,    4],\n",
            "        [   4, 3552,    2],\n",
            "        [  19,   42, 1089],\n",
            "        [   6,   26,  142],\n",
            "        [  18,  810,    2],\n",
            "        [1100,   18,   10],\n",
            "        [   2, 4582,   26],\n",
            "        [  33,  312,    5],\n",
            "        [  44,   19,  451],\n",
            "        [1493,  119,  121],\n",
            "        [   1,   13,   18],\n",
            "        [   6,   15,    3],\n",
            "        [   5, 1805,   12],\n",
            "        [   6,  529,  119],\n",
            "        [ 182,    5,   12],\n",
            "        [  30,  480,   27],\n",
            "        [ 241,    1,   14],\n",
            "        [   3,   60,  128],\n",
            "        [ 491,   96,    9],\n",
            "        [  62,   40,  244],\n",
            "        [  85,  708, 3268],\n",
            "        [  26,  212,   45],\n",
            "        [  98,    1,  219],\n",
            "        [   9, 3596,  114],\n",
            "        [   3,  236,   15],\n",
            "        [   4,  684,    2],\n",
            "        [  13,    1,  638],\n",
            "        [3220,   83, 1721],\n",
            "        [   1,  177,    4],\n",
            "        [   1,  549, 4229],\n",
            "        [   5,  135,  436],\n",
            "        [ 803,    7,   90],\n",
            "        [   5, 1175,    2],\n",
            "        [ 382,   13,   12],\n",
            "        [  14,  478,    5],\n",
            "        [ 256,  149,   63],\n",
            "        [  47,   35, 2228],\n",
            "        [ 658,    8,  100],\n",
            "        [   2,    1,  326],\n",
            "        [  88, 1255, 2405],\n",
            "        [2477,    6, 1738],\n",
            "        [1676,   11,  461],\n",
            "        [ 201,    1,  912],\n",
            "        [  30,   14,   76],\n",
            "        [ 784,   45, 1166],\n",
            "        [2168,   10, 1044],\n",
            "        [   1,  132, 1473],\n",
            "        [  17,    9, 1209],\n",
            "        [   2,  467,  641],\n",
            "        [ 615,    1,  311],\n",
            "        [ 220, 2138,   23],\n",
            "        [  56, 2135,    4],\n",
            "        [1659,   36,  315],\n",
            "        [ 128,  452,    1],\n",
            "        [  30, 3798,   11],\n",
            "        [ 178,    1,   52],\n",
            "        [1913,   18,    7],\n",
            "        [ 194,   10, 2888],\n",
            "        [  76,    5,    1],\n",
            "        [   3,  208,   53],\n",
            "        [ 545,  910,   98],\n",
            "        [2312,    3,  746],\n",
            "        [  59,  274,  148],\n",
            "        [   4,  234,  301],\n",
            "        [  14,  553,  651],\n",
            "        [   4, 2121,    5],\n",
            "        [  32,    3,  627],\n",
            "        [ 652,  622,    3],\n",
            "        [   9,   12, 1227],\n",
            "        [   4,  110,   10],\n",
            "        [  89,  650,    6],\n",
            "        [ 506, 4126,    2],\n",
            "        [  19,    2, 1045],\n",
            "        [  23,   28,   94],\n",
            "        [ 696,  284,    3],\n",
            "        [3858,    4,  757],\n",
            "        [3286,  677,   97],\n",
            "        [ 886,    1,  262],\n",
            "        [2202,  109,  431],\n",
            "        [  12,   49,  106],\n",
            "        [ 185,    4,   14],\n",
            "        [   5,   20,   84],\n",
            "        [2996,    8,   95],\n",
            "        [  38,   13,    1],\n",
            "        [  18, 4664,   11],\n",
            "        [  23,    3,   16],\n",
            "        [ 204,    4,   29],\n",
            "        [ 667,   75, 2974],\n",
            "        [  20,   16,  195],\n",
            "        [  55,  438,    7],\n",
            "        [ 434,   11,   61],\n",
            "        [   5,   14,  207],\n",
            "        [  49, 4743,   10],\n",
            "        [  35,  148, 3271],\n",
            "        [   4, 1024,   13],\n",
            "        [  61, 4238,   21],\n",
            "        [  90, 1081,    5],\n",
            "        [ 247,   21,  310],\n",
            "        [ 356,   65,  962],\n",
            "        [   1, 2884, 1417],\n",
            "        [   7, 2679,  519],\n",
            "        [ 569, 1130,    3],\n",
            "        [1525,    2, 2155],\n",
            "        [   2,   28,   64],\n",
            "        [  86,   21,  310],\n",
            "        [ 109,  654,  133],\n",
            "        [ 346,    5,  475],\n",
            "        [1371, 2207,  710],\n",
            "        [ 528, 3558,    7]]), tensor([[ 257,    4, 3851],\n",
            "        [   2,  352,  161],\n",
            "        [ 124, 3181,  319],\n",
            "        [  34,    5,    1],\n",
            "        [ 247,   53,   14],\n",
            "        [ 516,    8,   60],\n",
            "        [  21, 2733,    4],\n",
            "        [  29,  749,  188],\n",
            "        [  50,    5, 1053],\n",
            "        [ 209,    4,   14],\n",
            "        [   5, 3730,    1],\n",
            "        [   5,  847,    2],\n",
            "        [  50, 2143,    0],\n",
            "        [ 356,    1, 3187],\n",
            "        [   1,   14,  219],\n",
            "        [  40,    8,    2],\n",
            "        [  87,    2, 2122],\n",
            "        [ 614, 1802,    0],\n",
            "        [   9,  240,  922],\n",
            "        [ 340,    4,   14],\n",
            "        [3552,    2,    6],\n",
            "        [  42, 1089,    7],\n",
            "        [  26,  142,  103],\n",
            "        [ 810,    2,   27],\n",
            "        [  18,   10,    1],\n",
            "        [4582,   26,   14],\n",
            "        [ 312,    5,    0],\n",
            "        [  19,  451,    4],\n",
            "        [ 119,  121,   23],\n",
            "        [  13,   18,   88],\n",
            "        [  15,    3,   37],\n",
            "        [1805,   12, 3007],\n",
            "        [ 529,  119,  786],\n",
            "        [   5,   12, 1469],\n",
            "        [ 480,   27, 4927],\n",
            "        [   1,   14,  340],\n",
            "        [  60,  128,    7],\n",
            "        [  96,    9,  962],\n",
            "        [  40,  244,   15],\n",
            "        [ 708, 3268,    1],\n",
            "        [ 212,   45,  900],\n",
            "        [   1,  219,    2],\n",
            "        [3596,  114,    0],\n",
            "        [ 236,   15,  103],\n",
            "        [ 684,    2, 1445],\n",
            "        [   1,  638,  177],\n",
            "        [  83, 1721,  710],\n",
            "        [ 177,    4, 1338],\n",
            "        [ 549, 4229,    2],\n",
            "        [ 135,  436,    2],\n",
            "        [   7,   90, 3549],\n",
            "        [1175,    2,   24],\n",
            "        [  13,   12,   49],\n",
            "        [ 478,    5,    3],\n",
            "        [ 149,   63,    0],\n",
            "        [  35, 2228,   47],\n",
            "        [   8,  100,   28],\n",
            "        [   1,  326,    0],\n",
            "        [1255, 2405,    5],\n",
            "        [   6, 1738, 1606],\n",
            "        [  11,  461,    0],\n",
            "        [   1,  912,    0],\n",
            "        [  14,   76,  495],\n",
            "        [  45, 1166,    0],\n",
            "        [  10, 1044,    0],\n",
            "        [ 132, 1473,    9],\n",
            "        [   9, 1209,  579],\n",
            "        [ 467,  641,   14],\n",
            "        [   1,  311,    2],\n",
            "        [2138,   23,    7],\n",
            "        [2135,    4, 4149],\n",
            "        [  36,  315,    0],\n",
            "        [ 452,    1,   17],\n",
            "        [3798,   11,  204],\n",
            "        [   1,   52, 3465],\n",
            "        [  18,    7,   69],\n",
            "        [  10, 2888,    0],\n",
            "        [   5,    1,   14],\n",
            "        [ 208,   53, 1189],\n",
            "        [ 910,   98,  623],\n",
            "        [   3,  746, 3516],\n",
            "        [ 274,  148,   83],\n",
            "        [ 234,  301,    9],\n",
            "        [ 553,  651,    0],\n",
            "        [2121,    5,   13],\n",
            "        [   3,  627, 2182],\n",
            "        [ 622,    3,   16],\n",
            "        [  12, 1227,   26],\n",
            "        [ 110,   10, 1230],\n",
            "        [ 650,    6,   20],\n",
            "        [4126,    2,   81],\n",
            "        [   2, 1045,   15],\n",
            "        [  28,   94,  291],\n",
            "        [ 284,    3, 4320],\n",
            "        [   4,  757,    2],\n",
            "        [ 677,   97,  777],\n",
            "        [   1,  262,   45],\n",
            "        [ 109,  431,    0],\n",
            "        [  49,  106,    0],\n",
            "        [   4,   14,  403],\n",
            "        [  20,   84,    4],\n",
            "        [   8,   95, 1066],\n",
            "        [  13,    1,  111],\n",
            "        [4664,   11,   14],\n",
            "        [   3,   16, 1196],\n",
            "        [   4,   29,    0],\n",
            "        [  75, 2974,   11],\n",
            "        [  16,  195,   18],\n",
            "        [ 438,    7, 1076],\n",
            "        [  11,   61,    0],\n",
            "        [  14,  207,    0],\n",
            "        [4743,   10,  359],\n",
            "        [ 148, 3271,  109],\n",
            "        [1024,   13,    7],\n",
            "        [4238,   21,   25],\n",
            "        [1081,    5, 1397],\n",
            "        [  21,  310,    7],\n",
            "        [  65,  962,    1],\n",
            "        [2884, 1417, 1338],\n",
            "        [2679,  519,    0],\n",
            "        [1130,    3, 1907],\n",
            "        [   2, 2155,    0],\n",
            "        [  28,   64,    4],\n",
            "        [  21,  310,    2],\n",
            "        [ 654,  133,    0],\n",
            "        [   5,  475,    1],\n",
            "        [2207,  710,   79],\n",
            "        [3558,    7, 3835]])]\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "sample = next(iter(train_loader))\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "# Positional Embedding - as described in \"Attention is All You Need\"\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_sequence, embedding_dim):\n",
        "        super().__init__()\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.positional_encoding = torch.zeros(max_sequence, embedding_dim, device=device)\n",
        "        position = torch.arange(0, max_sequence, device=device).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2, device=device) * (-math.log(10000.0) / embedding_dim))\n",
        "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.positional_encoding = self.positional_encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, seq_length, _ = x.size()\n",
        "        positional_encoding = self.positional_encoding[:, :seq_length, :]\n",
        "        positional_encoding = positional_encoding.to(x.device)\n",
        "        # Position encoding is added to the input embeddings.\n",
        "        return x + positional_encoding   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix Implementation\n",
        "class SelfAttention_Matrix(nn.Module):\n",
        "  def __init__(self, embedding_dim, vocab_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.WQ = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "    self.WK = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "    self.WV = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "    self.WO = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "  def setProjections(self, WQ, WK, WV, WO):\n",
        "    self.WQ = WQ\n",
        "    self.WK = WK\n",
        "    self.WV = WV\n",
        "    self.WO = WO\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Linear projections\n",
        "    Q = self.WQ(inputs)\n",
        "    K = self.WK(inputs)\n",
        "    V = self.WV(inputs)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    probs = F.softmax(scores, dim=-1)\n",
        "    new_embedding = torch.matmul(probs, V)\n",
        "    # Projection in WO\n",
        "    new_embedding = self.WO(new_embedding)\n",
        "    return new_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
        "        self.posencoding = PositionalEncoding(context_size, embedding_dim)\n",
        "        self.attention = SelfAttention_Matrix(embedding_dim, vocab_size)        \n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.dropout1 = nn.Dropout(p = dropout_rate)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear2 = nn.Linear(h, vocab_size+1)\n",
        "        self.dropout2 = nn.Dropout(p = dropout_rate)\n",
        "        # Softmax to scale outputs\n",
        "        self.logSoftMax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        embeds_pos = self.posencoding(embeds)\n",
        "        x = torch.stack(torch.unbind(embeds_pos, dim=1), dim=1)\n",
        "        # Camada de autoatenção\n",
        "        attention  = self.attention(x)\n",
        "        # Flatten embeddings\n",
        "        embeds = embeds.view(attention.size(0), -1)\n",
        "        # Linear layer\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.dropout1(out)\n",
        "        out = self.relu(out)\n",
        "        # Second layer\n",
        "        out = self.linear2(out)\n",
        "        out = self.dropout2(out)\n",
        "        # Softmax output\n",
        "        out = self.logSoftMax(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funções de Treinamento e Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'The model has a total of {total_params:,} parameters.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "def initial_eval(model):\n",
        "    # Initial Perplexity and Loss\n",
        "    # Before training\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss += criterion(outputs, targets).item()\n",
        "\n",
        "    loss /= len(train_loader)\n",
        "    perp = torch.exp(torch.tensor(loss))\n",
        "\n",
        "    print(f'Initial Loss: {loss:.4f}')\n",
        "    print(f'Initial Perplexity: {perp:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer):\n",
        "      # Training Loop\n",
        "      model.train()\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "            epoch_start = time.time()\n",
        "            # Metrics\n",
        "            epoch_loss = 0\n",
        "            epoch_correct = 0\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for inputs, targets in train_loader:\n",
        "                  inputs = inputs.to(device)  # Move input data to the device\n",
        "                  targets = targets.to(device)\n",
        "\n",
        "                  # Forward pass\n",
        "                  outputs = model(inputs)\n",
        "                  loss = criterion(outputs, targets)\n",
        "\n",
        "                  # Backward pass and optimization\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "                  # Loss\n",
        "                  epoch_loss += loss.item()\n",
        "\n",
        "                  # Predicted\n",
        "                  _, predicted = torch.max(outputs, 1)\n",
        "                  epoch_correct += (predicted == targets).sum().item()\n",
        "                  epoch_samples += targets.size(0)\n",
        "\n",
        "            # Calculate average loss and accuracy for epoch\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            acc = epoch_correct / epoch_samples\n",
        "\n",
        "            # Perplexity\n",
        "            perp = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "            epoch_end = time.time()\n",
        "            epoch_time = epoch_end - epoch_start\n",
        "            # Print epoch statistics\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Time:{epoch_time:.2f}, Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%, Perplexity: {perp:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval(model, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    loss_sum = 0\n",
        "    total_sum = 0\n",
        "    correct_sum = 0\n",
        "    eval_round = 0\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)      \n",
        "            loss_sum += loss\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total_sum += targets.size(0)\n",
        "            correct_sum += (predicted == targets).sum().item()\n",
        "            eval_round += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = 100 * correct_sum / total_sum\n",
        "\n",
        "    # Calculate average perplexity\n",
        "    average_loss = loss_sum / len(val_loader)\n",
        "    average_perplexity = torch.exp(average_loss)\n",
        "\n",
        "    print(f'Test Accuracy: {acc:.2f}%')\n",
        "    print(f'Average Loss: {average_loss:.2f}')\n",
        "    print(f'Average Perplexity: {average_perplexity:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model with Self Attention:\n",
            "\n",
            "The model has a total of 2,351,497 parameters.\n",
            "\n",
            "Training Start\n",
            "\n",
            "Epoch [1/10], Time:0.99, Loss: 7.7468, Accuracy: 0.05%, Perplexity: 2314.2676\n",
            "Epoch [2/10], Time:0.67, Loss: 6.9517, Accuracy: 0.09%, Perplexity: 1044.9233\n",
            "Epoch [3/10], Time:0.67, Loss: 6.5932, Accuracy: 0.11%, Perplexity: 730.1199\n",
            "Epoch [4/10], Time:0.68, Loss: 6.3114, Accuracy: 0.14%, Perplexity: 550.7919\n",
            "Epoch [5/10], Time:0.66, Loss: 6.0618, Accuracy: 0.15%, Perplexity: 429.1499\n",
            "Epoch [6/10], Time:0.67, Loss: 5.8720, Accuracy: 0.17%, Perplexity: 354.9715\n",
            "Epoch [7/10], Time:0.66, Loss: 5.6912, Accuracy: 0.19%, Perplexity: 296.2380\n",
            "Epoch [8/10], Time:0.67, Loss: 5.5215, Accuracy: 0.20%, Perplexity: 250.0023\n",
            "Epoch [9/10], Time:0.69, Loss: 5.3912, Accuracy: 0.22%, Perplexity: 219.4578\n",
            "Epoch [10/10], Time:0.66, Loss: 5.2165, Accuracy: 0.24%, Perplexity: 184.2875\n"
          ]
        }
      ],
      "source": [
        "model_attn = LanguageModel(vocab_size, embedding_dim, context_size, hidden_dim)\n",
        "print(\"Model with Self Attention:\")\n",
        "print()\n",
        "count_parameters(model_attn)\n",
        "\n",
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model_attn.parameters(), lr)\n",
        "\n",
        "model_attn.to(device)\n",
        "\n",
        "print()\n",
        "print(\"Training Start\")\n",
        "print()\n",
        "train(model_attn, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "## Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Start\n",
            "\n",
            "Test Accuracy: 12.69%\n",
            "Average Loss: 6.12\n",
            "Average Perplexity: 454.90\n"
          ]
        }
      ],
      "source": [
        "print()\n",
        "print(\"Evaluation Start\")\n",
        "print()\n",
        "eval(model_attn, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frase: A destreza e a habilidade com que os Indios atiravão a setta era tal, que os Europeus a admiravão. Para atirarem por elevação, deitavão-se, seguravão o arco com os dous dedos dos pés e lançavão ao ar a setta que, subindo, descrevia uma parabola e ia cahir no alvo. Ainda ha pouco tempo no Pará se vião, nas aldêas de indios já cathequisados, pareos deste jogo, em que o alvo era um tronco de bananeira decepado. O tenente Pimentel, filho do presidente de Matto-Grosso, foi assassinado pelos indios deste modo, cavalgando no meio de muitos cavalleiros. Nenhum foi ferido: e todas as settas abatêrão-se sobre o moço de quem os selvagens se querião vingar.\n",
            "[1, 2568, 5, 1, 3498]\n",
            "[1, 2568, 5, 1, 3498, 1059]\n",
            "[1, 2568, 5, 1, 3498, 1059, 180]\n",
            "[1, 2568, 5, 1, 3498, 1059, 180, 161]\n",
            "[1, 2568, 5, 1, 3498, 1059, 180, 161, 1643]\n",
            "[1, 2568, 5, 1, 3498, 1059, 180, 161, 1643, 1171]\n",
            "[1, 2568, 5, 1, 3498, 1059, 180, 161, 1643, 1171, 3241]\n",
            "[1, 2568, 5, 1, 3498, 1059, 180, 161, 1643, 1171, 3241, 1212]\n",
            "[1, 2568, 5, 1, 3498, 1059, 180, 161, 1643, 1171, 3241, 1212, 2250]\n",
            "[1, 2568, 5, 1, 3498, 1059, 180, 161, 1643, 1171, 3241, 1212, 2250, 4114]\n",
            "[1, 2568, 5, 1, 3498, 1059, 180, 161, 1643, 1171, 3241, 1212, 2250, 4114, 3858]\n",
            "\u001b[34m['a', 'destreza', 'e', 'a', 'habilidade']\u001b[0m \u001b[31m['romance', 'antes', 'duas', 'ligeira', 'portugal', 'prominently', 'sentado', 'tagapema', 'sympathia', 'colleira']\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "# Código adaptado da implementação do Cesar Bastos\n",
        "from colorama import Fore, Style\n",
        "\n",
        "text = cleaned_paragraphs\n",
        "model_attn.to(device)\n",
        "def generate_text(model, vocab, text, max_length, context_size):\n",
        "    words = []\n",
        "    # Ensure there are enough words for at least one sequence\n",
        "    while len(words) < context_size:\n",
        "        random_number = random.randint(1, 4891)\n",
        "        words = encode_sentence(text[random_number], vocab)\n",
        "        if not words:\n",
        "            words = []\n",
        "            continue  # Skip if the sentence cannot be encoded\n",
        "        words = words[:context_size]\n",
        "        #print(words)\n",
        "        if any(token == 0 for token in words):\n",
        "            words = []\n",
        "            continue  # Skip if any token is zero (assuming 0 is a special token)\n",
        "        context = words\n",
        "\n",
        "    print(f\"Frase: {cleaned_paragraphs[random_number]}\")\n",
        "    print(words)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        words_tensor = torch.tensor(context[-context_size:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        logits = model(words_tensor)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        context.append(next_token.item())\n",
        "        print(context)\n",
        "    frase = []\n",
        "    for i in context: ##Agradecimentos a Ramon Abilio\n",
        "        word = next((word for word, code in vocab.items() if code == i), \"<UNKNOWN>\")\n",
        "        frase.append(word)\n",
        "\n",
        "    print(f\"{Fore.BLUE}{frase[:context_size]}{Style.RESET_ALL} {Fore.RED}{frase[-max_length:]}{Style.RESET_ALL} \")\n",
        "\n",
        "\n",
        "max_length= 10\n",
        "generate_text(model_attn, vocab, text, max_length, context_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
