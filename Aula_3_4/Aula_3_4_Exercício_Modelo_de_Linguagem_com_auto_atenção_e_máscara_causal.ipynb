{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção e máscaras causais\n",
        "\n",
        "Seguimos na mesma linha de treinar um modelo de linguagem a partir dos textos do livro \"O Guarani\", de José de Alencar.\n",
        "\n",
        "Neste exercício, vamos treinar um modelo de linguagem com auto-atenção e com máscara causal. A máscara causal é necessária para que o modelo não tenha acesso a palavras futuras, que é a abordagem usada por grandes modelos de linguagem, como o GPT.\n",
        "\n",
        "Use a implementação matricial de auto-atenção da aula passada.\n",
        "\n",
        "### Modificações necessárias\n",
        "\n",
        "* Adicione a máscara causal na função `forward` da cabeça de auto-atenção.\n",
        "* Modifique o nosso dataloader para retornar inputs (uma lista de tokens de tamanho $n$), targets (uma lista de tokens de tamanho $n$ deslocada para a esquerda em 1 token). Exemplo `input = [1, 2, 3, 4]`, `target = [2, 3, 4, 5]` para a sequência `[1, 2, 3, 4, 5]` com `seq_len=4`, por exemplo (Ver slide 50).\n",
        "\n",
        "### Extra\n",
        "* MultiHeadAttention: modifique a cabeça de auto-atenção para ter múltiplas cabeças. Isso não é obrigatório, mas pode ser interessante para ver como o modelo se comporta.\n",
        "* Diagrama da geração: fazer diagrama que mostre os passos da geração de tokens (conforme slide 47).\n",
        "\n",
        "### Dicas\n",
        "\n",
        "* Use como base o vídeo do Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY. Observe que, no vídeo, ele primeiro implementa um modelo bi-grama, depois um modelo de linguagem com auto-atenção. O modelo de auto-atenção é implementado por volta do minuto 40, mas vale a pena assistir o vídeo todo.\n",
        "* Use esta implementação como base: https://colab.research.google.com/drive/1vFTg4MSXVJwNSzPjaCcvmqhxTP7gK7HA?usp=sharing. Observe como o modelo é organizado e como a máscara é implementada na classe MultiHeadAttention.\n",
        "* Use `context_size=9`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "!wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-06 23:38:57--  https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://www.gutenberg.org/cache/epub/67724/pg67724.txt [following]\n",
            "--2024-03-06 23:38:57--  http://www.gutenberg.org/cache/epub/67724/pg67724.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/cache/epub/67724/pg67724.txt [following]\n",
            "--2024-03-06 23:38:58--  https://www.gutenberg.org/cache/epub/67724/pg67724.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 372908 (364K) [text/plain]\n",
            "Saving to: ‘67724.txt.utf-8’\n",
            "\n",
            "67724.txt.utf-8     100%[===================>] 364.17K  1.53MB/s    in 0.2s    \n",
            "\n",
            "2024-03-06 23:38:58 (1.53 MB/s) - ‘67724.txt.utf-8’ saved [372908/372908]\n",
            "\n",
            "--2024-03-06 23:38:58--  https://www.gutenberg.org/ebooks/67725.txt.utf-8\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://www.gutenberg.org/cache/epub/67725/pg67725.txt [following]\n",
            "--2024-03-06 23:38:58--  http://www.gutenberg.org/cache/epub/67725/pg67725.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/cache/epub/67725/pg67725.txt [following]\n",
            "--2024-03-06 23:38:59--  https://www.gutenberg.org/cache/epub/67725/pg67725.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 345264 (337K) [text/plain]\n",
            "Saving to: ‘67725.txt.utf-8’\n",
            "\n",
            "67725.txt.utf-8     100%[===================>] 337.17K  1.41MB/s    in 0.2s    \n",
            "\n",
            "2024-03-06 23:38:59 (1.41 MB/s) - ‘67725.txt.utf-8’ saved [345264/345264]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"67724.txt.utf-8\",\"r\").read()\n",
        "text += open(\"67725.txt.utf-8\",\"r\").read()\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "len(paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4969"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "\"\"\"TODO: Ver como tá ficando os paragrafos\"\"\"\n",
        "\n",
        "len(cleaned_paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4892"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSRHqe3H4ZFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12603"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(r'\\w+', text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6594,\n",
              " 139,\n",
              " 4376,\n",
              " 19,\n",
              " 6595,\n",
              " 6,\n",
              " 44,\n",
              " 110,\n",
              " 269,\n",
              " 259,\n",
              " 2662,\n",
              " 10,\n",
              " 1064,\n",
              " 6596,\n",
              " 2,\n",
              " 186,\n",
              " 130,\n",
              " 280,\n",
              " 3,\n",
              " 2257,\n",
              " 6,\n",
              " 6597,\n",
              " 1,\n",
              " 2665]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(r'\\w+', sentence.lower())]\n",
        "\n",
        "encode_sentence(cleaned_paragraphs[20], vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 5 # 5 palavras de entrada. O target é a próxima palavra\n",
        "\"\"\"TODO: Preparar o dataset\"\"\""
      ],
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"TODO: divida o dataset em validação/treino com um proporção de 20/80 %. OBS, use random_state=18\"\"\""
      ],
      "metadata": {
        "id": "y1aetOpmDeQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95377"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\"\"\"TODO: implemente a classe do dataset\"\"\"\n",
        "\n",
        "train_data = MyDataset(...)\n",
        "val_data = MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "sample = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    \"\"\"TODO: implementar o modelo de linguagem\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "model = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [],
      "source": [
        "# sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGbJcT5KzJ_K"
      },
      "outputs": [],
      "source": [
        "output = model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um0lR4mNzJ_K",
        "outputId": "e6041da8-ca7f-4c9d-b28b-9e1250e820f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4842, 2163, 7516, 2652, 6373, 7429, 8003, 3759, 1768, 7740, 2595, 1859,\n",
              "        3189, 8049, 5727, 6132])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "output.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la-b-f8jzJ_L",
        "outputId": "f040cef4-e409-4d20-d335-a3133aaeb63c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   2,    3,    4,   37,    3,  215,   71,  411, 1263,  355,   87, 3653,\n",
              "         584,  980,    1,    7])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "a054092b-d801-4c60-eb75-85abfe57151d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "lr = \"\"\"TODO\"\"\"\"\n",
        "criterion = \"\"\"TODO CrossEntropy\"\"\"\"\n",
        "\n",
        "optimizer = \"\"\"TODO: AdamW ou outro\"\"\"\"\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\"\"\"TODO: Implemente o loop de treinamento. Em cada época, calcule e imprima a loss no dataset de validação\"\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação"
      ],
      "metadata": {
        "id": "PSXfwYISDoPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" TODO: calcule a perplexidade final no dataset de validação \"\"\""
      ],
      "metadata": {
        "id": "nXXO78GSDqPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [],
      "source": [
        "text = \"\"\n",
        "\n",
        "def generate_text(model, vocab, text, max_length):\n",
        "    \"\"\"TODO: implemente a função para gerar texto até atingir o max_length\"\"\"\n",
        "\n",
        "context = 5\n",
        "max_length= 10\n",
        "generate_text(text, max_length)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}