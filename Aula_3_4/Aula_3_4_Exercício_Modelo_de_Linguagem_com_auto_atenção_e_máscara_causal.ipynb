{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção e máscaras causais\n",
        "\n",
        "Seguimos na mesma linha de treinar um modelo de linguagem a partir dos textos do livro \"O Guarani\", de José de Alencar.\n",
        "\n",
        "Neste exercício, vamos treinar um modelo de linguagem com auto-atenção e com máscara causal. A máscara causal é necessária para que o modelo não tenha acesso a palavras futuras, que é a abordagem usada por grandes modelos de linguagem, como o GPT.\n",
        "\n",
        "Use a implementação matricial de auto-atenção da aula passada.\n",
        "\n",
        "### Modificações necessárias\n",
        "\n",
        "* Adicione a máscara causal na função `forward` da cabeça de auto-atenção.\n",
        "* Modifique o nosso dataloader para retornar inputs (uma lista de tokens de tamanho $n$), targets (uma lista de tokens de tamanho $n$ deslocada para a esquerda em 1 token). Exemplo `input = [1, 2, 3, 4]`, `target = [2, 3, 4, 5]` para a sequência `[1, 2, 3, 4, 5]` com `seq_len=4`, por exemplo (Ver slide 50).\n",
        "\n",
        "### Extra\n",
        "* MultiHeadAttention: modifique a cabeça de auto-atenção para ter múltiplas cabeças. Isso não é obrigatório, mas pode ser interessante para ver como o modelo se comporta.\n",
        "* Diagrama da geração: fazer diagrama que mostre os passos da geração de tokens (conforme slide 47).\n",
        "\n",
        "### Dicas\n",
        "\n",
        "* Use como base o vídeo do Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY. Observe que, no vídeo, ele primeiro implementa um modelo bi-grama, depois um modelo de linguagem com auto-atenção. O modelo de auto-atenção é implementado por volta do minuto 40, mas vale a pena assistir o vídeo todo.\n",
        "* Use esta implementação como base: https://colab.research.google.com/drive/1vFTg4MSXVJwNSzPjaCcvmqhxTP7gK7HA?usp=sharing. Observe como o modelo é organizado e como a máscara é implementada na classe MultiHeadAttention.\n",
        "* Use `context_size=9`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variáveis Globais e Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Global variables\n",
        "\n",
        "# Vocabulary\n",
        "vocab_size = 10000\n",
        "seq_len = 9\n",
        "pattern = r'\\w+|[,;.:!?\\']'\n",
        "\n",
        "# Training\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "lr = 0.0001\n",
        "\n",
        "# Model\n",
        "embedding_dim = 256\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# Colab environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if (IN_COLAB):\n",
        "    %pip install colorama\n",
        "\n",
        "    # Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    project_folder=\"/content/drive/MyDrive/Classes/IA024/Aula_2_3\"\n",
        "    os.chdir(project_folder)\n",
        "    !ls -la\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "# Check if download is necessary\n",
        "if not os.path.exists(\"67724.txt.utf-8\"):\n",
        "    print(\"Downloading Gutenberg texts\")\n",
        "\n",
        "    !wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "    !wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4969"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = open(\"67724.txt.utf-8\",\"r\").read()\n",
        "text += open(\"67725.txt.utf-8\",\"r\").read()\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg eBook of O Guarany: romance brazileiro, Vol. 1 (of 2)\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n"
          ]
        }
      ],
      "source": [
        "# Checking the text\n",
        "print(paragraphs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Porque é preciso.\n",
            "\n",
            "Voltou-se para ver se alguem estava alli que reparasse no que ia fazer, e deo com o italiano que a dous passos delle o olhava com um dos seus sorrisos sarcasticos.\n",
            "\n",
            " PAG. 292.--=Guanumby=.\n",
            "\n",
            "Um incidente veio atear a chamma que lastrava; Pery, apenas começou a romper o dia, via a alguma distancia do jardim o cadaver do Ruy Soeiro; e temendo que sua senhora acordando não presenciasse este triste espectaculo, tomou o corpo, e atravessando a esplanada, veio atira-lo no meio do pateo.\n",
            "\n",
            "--Tu me offendes, Pery! exclamou o fidalgo; a minha casa está aberta para todos, e sobretudo para ti que és amigo, e salvaste minha filha.\n",
            "\n",
            "Number of paragraphs: 4892\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4892"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "# Print 5 random paragraphs\n",
        "num_paragraphs = len(cleaned_paragraphs)\n",
        "for i in range(0,5):\n",
        "    idx = random.randrange(num_paragraphs)\n",
        "    print(f\"{cleaned_paragraphs[idx]}\\n\")\n",
        "\n",
        "print(\"Number of paragraphs: \" + str(num_paragraphs))\n",
        "\n",
        "len(cleaned_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12603"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(r'\\w+', text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever.\n",
            "[6594, 139, 4376, 19, 6595, 0, 6, 44, 110, 269, 259, 2662, 10, 1064, 6596, 0, 2, 186, 130, 280, 3, 2257, 6, 6597, 1, 2665, 0]\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(pattern, sentence.lower())]\n",
        "\n",
        "print(cleaned_paragraphs[20])\n",
        "print(encode_sentence(cleaned_paragraphs[20], vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Modificado para Máscara Causal\n",
        "\n",
        "Modifique o nosso dataloader para retornar inputs (uma lista de tokens de tamanho $n$), targets (uma lista de tokens de tamanho $n$ deslocada para a esquerda em 1 token). Exemplo `input = [1, 2, 3, 4]`, `target = [2, 3, 4, 5]` para a sequência `[1, 2, 3, 4, 5]` com `seq_len=4`, por exemplo (Ver slide 50)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CausalMaskDataset(Dataset):\n",
        "  def __init__(self, paragraphs, vocab, seq_len):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.vocab = vocab\n",
        "    self.seq_len = seq_len\n",
        "    self.tokens, self.targets = self.setup()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    tokens = torch.tensor(self.tokens[idx])\n",
        "    targets = torch.tensor(self.targets[idx])\n",
        "    return tokens, targets\n",
        "  \n",
        "  def setup(self):\n",
        "    tokens = []\n",
        "    targets = []\n",
        "    for paragraph in self.paragraphs:\n",
        "      encoded = encode_sentence(paragraph, self.vocab)\n",
        "      \n",
        "      # If paragraph is smaller than the sequence length, skip it.\n",
        "      if len(encoded) < self.seq_len + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(len(encoded) - self.seq_len):\n",
        "        tks = encoded[i:i+self.seq_len]\n",
        "        # Return targets with seq_len instead of a single one.\n",
        "        tgt = encoded[i+1:i+1+self.seq_len]\n",
        "        # Only add if there are no unknown tokens in both context and target.\n",
        "        bad_token = 0\n",
        "        if not (bad_token in tks or tgt == bad_token):\n",
        "          tokens.append(tks)\n",
        "          targets.append(tgt)\n",
        "    return tokens, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Verificação básica do Dataset modificado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input and Target Tensors:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([133,  47, 712, 144, 537, 324, 275,  35, 499]),\n",
              " tensor([ 47, 712, 144, 537, 324, 275,  35, 499,  47]))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tst_paragraphs = cleaned_paragraphs[:50]\n",
        "tst_seq_len = 9\n",
        "tst_dataset = CausalMaskDataset(tst_paragraphs, vocab, tst_seq_len)\n",
        "\n",
        "print(\"Input and Target Tensors:\")\n",
        "tst_dataset[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verificação do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y1aetOpmDeQd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 3913\n",
            "Validation samples: 979\n",
            "\n",
            "Training dataset samples: 15993\n",
            "Validation dataset samples: 4197\n"
          ]
        }
      ],
      "source": [
        "# Train/Validation split\n",
        "train_data, val_data = train_test_split(cleaned_paragraphs, test_size=0.2, random_state=18)\n",
        "\n",
        "train_dataset = CausalMaskDataset(train_data, vocab, seq_len)\n",
        "val_dataset = CausalMaskDataset(val_data, vocab, seq_len)\n",
        "\n",
        "# Counting all Samples\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print()\n",
        "print(f\"Training dataset samples: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[  20,   16,  392, 1076,    5,   63,  120,  104, 2154],\n",
            "        [6513,  298,  117,   40,    5, 2643,   23,    7, 6516],\n",
            "        [   1, 4073,    3,  782,    4, 1412,   22,  208,    4],\n",
            "        [1502,  118, 6118,   75,  489,    5,  203,   39, 1545],\n",
            "        [  10, 1139,    6,  856, 4116,   22,    2,    6,  196],\n",
            "        [   5,   21,  152,  151,   88,  193,    1, 1837,    4],\n",
            "        [ 933,  500,    2,    3,  115,    3, 1243,    6,   27],\n",
            "        [1909,   32,   12,  935,    2,  358,  299,   56, 1822],\n",
            "        [1339,   53,  197,   22, 3062,    4,   10, 1643, 4262],\n",
            "        [ 459,    1,  146,  665,    5, 7812,    4,   29,    2],\n",
            "        [   6,  467,  387,    1,  638,  177,   36,  935,  281],\n",
            "        [1821,  119, 1145, 4018,   53,  794,    4,  793,  228],\n",
            "        [ 499,  465, 1061,   35,  333,   47,   35,   67,   59],\n",
            "        [ 474,  806,   86,    7, 2420,    4, 1872,   75,  160],\n",
            "        [  71,  121,  648,   15,    2,   17,    9, 2464,   40],\n",
            "        [2651,  109,   35,  463,  375,   97, 1251,  413, 2652],\n",
            "        [ 252, 3244,    1, 2214, 5798,  483,  465,  324, 1718],\n",
            "        [ 365, 1519,   87,  107,    1,   92,    8,   80,   27],\n",
            "        [1037, 7561,   13,    1,  285, 1072,    5,    1,  285],\n",
            "        [3051,    4,  126,    7,  481,   22,  990,    5, 1872],\n",
            "        [   5,  555,    1,  126,   18, 4494,    1,    7,  659],\n",
            "        [2003, 5482, 4662,  278, 4907,   21,   10, 9128,    4],\n",
            "        [ 537,  324,  275,   35,  499,   47, 1250, 2651,  109],\n",
            "        [ 308,  338,  110,    7, 5675,    4,  150,   15,   74],\n",
            "        [   2,    9,  120,  610,    4,   46,  796,   10,  121],\n",
            "        [   5, 5436,    3,  236,   15,  103,  386,    3,  332],\n",
            "        [1807, 1964, 1636, 5468,   21,  703, 6477,    5, 1499],\n",
            "        [  70,  322,    7,   69, 1790,   53, 1908,    2, 2431],\n",
            "        [  12,   68, 6249,  966,    1,  549,    8,   16,  489],\n",
            "        [   5,  669,   19,  472,   11,  433,    4, 4932,    4],\n",
            "        [ 109, 5946, 3278,  279,   35, 3279, 5947,   47, 3280],\n",
            "        [4003,    6,    2,    1,  290,   12, 8723,   13,    7],\n",
            "        [2446, 1301,    4,   14,  168,    5, 1812,    1,   30],\n",
            "        [   5, 4101,    6,    1,  245,   72,    4,  152,  944],\n",
            "        [   6, 6177,    1,  153,  182,   15,  126,    1,  727],\n",
            "        [  31, 2475,   21,   10,  626,   40,  312,    8,    2],\n",
            "        [   9,   28,    3,  490,    4,  147,   14,  524, 4309],\n",
            "        [1126,   13,  228,   68,    2, 1879,   22,   16, 1174],\n",
            "        [   4,  393,    7,  130, 4578,    6,   30, 1828,    8],\n",
            "        [ 104,   55, 3086,   30,  292,  103,   62, 2541,  125],\n",
            "        [  85, 1155,  141,  431,  413,  333,   47,  144,  404],\n",
            "        [ 389,   30, 3798,   11,  204,   44,   19,  146,  682],\n",
            "        [ 916,    6,    2,   48,   63,  970,    4,   14,   76],\n",
            "        [ 219,    4,    2,   10, 7938, 2991,   38,   26, 2486],\n",
            "        [   8, 8757,    2, 3127,   32,    1,  136,    4,   17],\n",
            "        [8478,   11,  351, 8479,   22, 1277,   19, 5176,    2],\n",
            "        [   1,   61,   15,  147,    6,   87,   62,   19,  646],\n",
            "        [  55,    4, 4959,   30,   14,   92,   24, 3591,   21],\n",
            "        [  43, 5022, 1133,    3,   16, 8019, 1219,  550,    3],\n",
            "        [  93,   39,   49, 8044,    3,   72,    2,   10,  947],\n",
            "        [ 359,    5,  249,    8, 1413,  113,    7,  553, 3176],\n",
            "        [ 322, 5107,   23,    6,   10, 8182,    3,  428, 5108],\n",
            "        [   5,  495,    6,   15,    1,  142,   15,  701,    3],\n",
            "        [   6,  147, 2843,    4,  213,   99, 7326,    8, 2039],\n",
            "        [ 495,    6,  477,   22,  214,   11,   61,  103, 6101],\n",
            "        [ 804,    6,    4,    7, 1425,   32,  291,    2, 7642],\n",
            "        [4658,    2,   50,  858,   13,   12,   39,   49, 1204],\n",
            "        [  86,    4,  335,   15, 8150,  203, 2053,   22, 2119],\n",
            "        [   5,    1,   14,  167,   15,  910,   74,   12, 3538],\n",
            "        [  15,    1,  259,  398,   24,   27, 3115,   99,  383],\n",
            "        [ 692,    5,    7,  195,    4, 1380, 5414, 9642,  215],\n",
            "        [9220,  506,    1,   14,  380,  366,    8,    2,    1],\n",
            "        [1055, 1492,   20,   16, 1354,    1, 4184,  817, 1487],\n",
            "        [   3,  130, 1962, 1565,    1,  131,  151,  801, 6966]]), tensor([[  16,  392, 1076,    5,   63,  120,  104, 2154,    0],\n",
            "        [ 298,  117,   40,    5, 2643,   23,    7, 6516,  291],\n",
            "        [4073,    3,  782,    4, 1412,   22,  208,    4,   57],\n",
            "        [ 118, 6118,   75,  489,    5,  203,   39, 1545,  205],\n",
            "        [1139,    6,  856, 4116,   22,    2,    6,  196,    0],\n",
            "        [  21,  152,  151,   88,  193,    1, 1837,    4,   43],\n",
            "        [ 500,    2,    3,  115,    3, 1243,    6,   27,  745],\n",
            "        [  32,   12,  935,    2,  358,  299,   56, 1822,  301],\n",
            "        [  53,  197,   22, 3062,    4,   10, 1643, 4262,    0],\n",
            "        [   1,  146,  665,    5, 7812,    4,   29,    2, 2477],\n",
            "        [ 467,  387,    1,  638,  177,   36,  935,  281,  786],\n",
            "        [ 119, 1145, 4018,   53,  794,    4,  793,  228, 2530],\n",
            "        [ 465, 1061,   35,  333,   47,   35,   67,   59,  395],\n",
            "        [ 806,   86,    7, 2420,    4, 1872,   75,  160,    2],\n",
            "        [ 121,  648,   15,    2,   17,    9, 2464,   40,  254],\n",
            "        [ 109,   35,  463,  375,   97, 1251,  413, 2652,   47],\n",
            "        [3244,    1, 2214, 5798,  483,  465,  324, 1718,  141],\n",
            "        [1519,   87,  107,    1,   92,    8,   80,   27, 4413],\n",
            "        [7561,   13,    1,  285, 1072,    5,    1,  285, 1113],\n",
            "        [   4,  126,    7,  481,   22,  990,    5, 1872,    2],\n",
            "        [ 555,    1,  126,   18, 4494,    1,    7,  659,    4],\n",
            "        [5482, 4662,  278, 4907,   21,   10, 9128,    4, 1175],\n",
            "        [ 324,  275,   35,  499,   47, 1250, 2651,  109,   35],\n",
            "        [ 338,  110,    7, 5675,    4,  150,   15,   74,  327],\n",
            "        [   9,  120,  610,    4,   46,  796,   10,  121,    0],\n",
            "        [5436,    3,  236,   15,  103,  386,    3,  332,    4],\n",
            "        [1964, 1636, 5468,   21,  703, 6477,    5, 1499,    0],\n",
            "        [ 322,    7,   69, 1790,   53, 1908,    2, 2431,   32],\n",
            "        [  68, 6249,  966,    1,  549,    8,   16,  489,    0],\n",
            "        [ 669,   19,  472,   11,  433,    4, 4932,    4,  615],\n",
            "        [5946, 3278,  279,   35, 3279, 5947,   47, 3280,  775],\n",
            "        [   6,    2,    1,  290,   12, 8723,   13,    7,  223],\n",
            "        [1301,    4,   14,  168,    5, 1812,    1,   30,  474],\n",
            "        [4101,    6,    1,  245,   72,    4,  152,  944,    3],\n",
            "        [6177,    1,  153,  182,   15,  126,    1,  727, 3309],\n",
            "        [2475,   21,   10,  626,   40,  312,    8,    2,    1],\n",
            "        [  28,    3,  490,    4,  147,   14,  524, 4309,   26],\n",
            "        [  13,  228,   68,    2, 1879,   22,   16, 1174,   99],\n",
            "        [ 393,    7,  130, 4578,    6,   30, 1828,    8,  580],\n",
            "        [  55, 3086,   30,  292,  103,   62, 2541,  125,    1],\n",
            "        [1155,  141,  431,  413,  333,   47,  144,  404,  275],\n",
            "        [  30, 3798,   11,  204,   44,   19,  146,  682,    0],\n",
            "        [   6,    2,   48,   63,  970,    4,   14,   76,    5],\n",
            "        [   4,    2,   10, 7938, 2991,   38,   26, 2486,    0],\n",
            "        [8757,    2, 3127,   32,    1,  136,    4,   17,    1],\n",
            "        [  11,  351, 8479,   22, 1277,   19, 5176,    2,    3],\n",
            "        [  61,   15,  147,    6,   87,   62,   19,  646,    0],\n",
            "        [   4, 4959,   30,   14,   92,   24, 3591,   21,   10],\n",
            "        [5022, 1133,    3,   16, 8019, 1219,  550,    3,    2],\n",
            "        [  39,   49, 8044,    3,   72,    2,   10,  947, 8045],\n",
            "        [   5,  249,    8, 1413,  113,    7,  553, 3176,    0],\n",
            "        [5107,   23,    6,   10, 8182,    3,  428, 5108,    0],\n",
            "        [ 495,    6,   15,    1,  142,   15,  701,    3,    2],\n",
            "        [ 147, 2843,    4,  213,   99, 7326,    8, 2039,    0],\n",
            "        [   6,  477,   22,  214,   11,   61,  103, 6101,   12],\n",
            "        [   6,    4,    7, 1425,   32,  291,    2, 7642,  126],\n",
            "        [   2,   50,  858,   13,   12,   39,   49, 1204,    0],\n",
            "        [   4,  335,   15, 8150,  203, 2053,   22, 2119,   11],\n",
            "        [   1,   14,  167,   15,  910,   74,   12, 3538,    0],\n",
            "        [   1,  259,  398,   24,   27, 3115,   99,  383,    2],\n",
            "        [   5,    7,  195,    4, 1380, 5414, 9642,  215,   18],\n",
            "        [ 506,    1,   14,  380,  366,    8,    2,    1,  161],\n",
            "        [1492,   20,   16, 1354,    1, 4184,  817, 1487, 4185],\n",
            "        [ 130, 1962, 1565,    1,  131,  151,  801, 6966,    8]])]\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "sample = next(iter(train_loader))\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "### Implementação do Modelo com Máscara Causal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Código baseado no tutorial do Andrej Karpathy https://github.com/karpathy/ng-video-lecture\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size, seq_len):\n",
        "        super(Head, self).__init__()\n",
        "        self.key = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.query = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.value = nn.Linear(embedding_dim, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "\n",
        "        return out\n",
        "    \n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 4 * embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * embedding_dim, embedding_dim),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "class KarpathyModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, seq_len):\n",
        "      super(KarpathyModel, self).__init__()\n",
        "    \n",
        "      # Embedding\n",
        "      self.embedding = nn.Embedding(vocab_size+1, embedding_dim)\n",
        "      # Positional Embedding (simpler?)\n",
        "      self.positional_embedding = nn.Embedding(seq_len, embedding_dim)\n",
        "      # Single Head Attention\n",
        "      self.attention = Head(embedding_dim, seq_len)\n",
        "      # Projection\n",
        "      self.WO = nn.Linear(embedding_dim, embedding_dim)\n",
        "      # Linear Layers\n",
        "      self.ffwd = FeedForward(embedding_dim)\n",
        "\n",
        "      # Normalization Layers\n",
        "      self.ln_attention = nn.LayerNorm(embedding_dim)\n",
        "      self.ln_ffwd = nn.LayerNorm(embedding_dim)\n",
        "      self.ln_f = nn.LayerNorm(embedding_dim) # final layer norm\n",
        "\n",
        "      self.lm_head = nn.Linear(embedding_dim, vocab_size+1)\n",
        "      self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input):\n",
        "      B,T = input.shape\n",
        "      # idx and targets are both (B,T) tensor of integers\n",
        "      embedding_input = self.embedding(input) # (B,T,C)\n",
        "      positions = self.positional_embedding(torch.arange(T, device=device)) # (T,C)\n",
        "      x = embedding_input + positions # (B,T,C)\n",
        "      # Auto Atenção\n",
        "      attention = self.ln_attention(x)\n",
        "      attention = self.attention(attention)\n",
        "      attention = self.WO(attention)\n",
        "      attention = self.dropout(attention)\n",
        "      x = x + attention\n",
        "      # MLP\n",
        "      ffwd = self.ln_ffwd(x)\n",
        "      ffwd = self.ffwd(ffwd)\n",
        "\n",
        "      x = x + ffwd\n",
        "      # Camada de saida\n",
        "      x = self.ln_f(x)\n",
        "      logits = self.lm_head(x)\n",
        "\n",
        "      return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=10, seq_len=9):\n",
        "      # idx is (B, T) array of indices in the current context\n",
        "      for _ in range(max_new_tokens):\n",
        "          # crop idx to the last block_size tokens\n",
        "          idx_cond = idx[:, -seq_len:]\n",
        "          # get the predictions\n",
        "          logits = self(idx_cond)\n",
        "          # focus only on the last time step\n",
        "          logits = logits[:, -1, :] # becomes (B, C)\n",
        "          # apply softmax to get probabilities\n",
        "          probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "          #Extraido do Ramon Simoes --> Excluir o token <unk> (codificado como 0) atribuindo probabilidade zero\n",
        "          probs[:,0] = 0.0\n",
        "          probs = probs / probs.sum(dim=-1, keepdim=True)\n",
        "          # sample from the distribution\n",
        "          idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "          # append sampled index to the running sequence\n",
        "          idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "      return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Treinamento e Avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funções de Treinamento e Avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Contagem de Parâmetros do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'The model has a total of {total_params:,} parameters.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Avaliação Inicial Pré-Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "def initial_eval(model, criterion, device):\n",
        "    # Initial Perplexity and Loss\n",
        "    # Before training\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            loss = criterion(logits, targets)\n",
        "            \n",
        "    loss /= len(train_loader)\n",
        "\n",
        "    perp = torch.exp(loss)\n",
        "\n",
        "    print(f'Initial Loss: {loss:.4f}')\n",
        "    print(f'Initial Perplexity: {perp:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Função de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, device):\n",
        "      # Training Loop\n",
        "      model.train()\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "            epoch_start = time.time()\n",
        "            # Metrics\n",
        "            epoch_loss = 0\n",
        "            epoch_correct = 0\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for inputs, targets in train_loader:\n",
        "                  inputs = inputs.to(device)  # Move input data to the device\n",
        "                  targets = targets.to(device)\n",
        "\n",
        "                  # Forward pass\n",
        "                  logits = model(inputs)\n",
        "                  B, T, C = logits.shape\n",
        "                  logits = logits.view(B*T, C)\n",
        "                  targets = targets.view(B*T)\n",
        "                  loss = criterion(logits, targets)\n",
        "\n",
        "                  # Backward pass and optimization\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "                  # Loss\n",
        "                  epoch_loss += loss.item()\n",
        "\n",
        "                  # Predicted\n",
        "                  _, predicted = torch.max(logits, 1)\n",
        "                  epoch_correct += (predicted == targets).sum().item()\n",
        "                  epoch_samples += targets.size(0)\n",
        "\n",
        "            # Calculate average loss and accuracy for epoch\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            acc = epoch_correct / epoch_samples\n",
        "\n",
        "            # Perplexity\n",
        "            perp = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "            epoch_end = time.time()\n",
        "            epoch_time = epoch_end - epoch_start\n",
        "            # Print epoch statistics\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Time:{epoch_time:.2f}, Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%, Perplexity: {perp:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Função de Avaliação do Modelo Treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval(model, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    loss_sum = 0\n",
        "    total_sum = 0\n",
        "    correct_sum = 0\n",
        "    eval_round = 0\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = criterion(logits, targets)     \n",
        "\n",
        "            loss_sum += loss\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "\n",
        "            total_sum += targets.size(0)\n",
        "            correct_sum += (predicted == targets).sum().item()\n",
        "            eval_round += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = 100 * correct_sum / total_sum\n",
        "\n",
        "    # Calculate average perplexity\n",
        "    average_loss = loss_sum / len(val_loader)\n",
        "    average_perplexity = torch.exp(average_loss)\n",
        "\n",
        "    print(f'Test Accuracy: {acc:.2f}%')\n",
        "    print(f'Average Loss: {average_loss:.2f}')\n",
        "    print(f'Average Perplexity: {average_perplexity:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Avaliação Inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KarpathyModel(\n",
            "  (embedding): Embedding(10001, 256)\n",
            "  (positional_embedding): Embedding(9, 256)\n",
            "  (attention): Head(\n",
            "    (key): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (query): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (value): Linear(in_features=256, out_features=256, bias=False)\n",
            "    (dropout): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (WO): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (ffwd): FeedForward(\n",
            "    (net): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "      (3): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (ln_attention): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (ln_ffwd): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=256, out_features=10001, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model_attn = KarpathyModel(vocab_size, embedding_dim, seq_len)\n",
        "print(model_attn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model with Self Attention:\n",
            "\n",
            "The model has a total of 5,922,321 parameters.\n",
            "\n",
            "Initial Evaluation\n",
            "\n",
            "Initial Loss: 0.0375\n",
            "Initial Perplexity: 1.0382\n"
          ]
        }
      ],
      "source": [
        "print(\"Model with Self Attention:\")\n",
        "print()\n",
        "count_parameters(model_attn)\n",
        "\n",
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model_attn.to(device)\n",
        "\n",
        "print()\n",
        "print(\"Initial Evaluation\")\n",
        "print()\n",
        "initial_eval(model_attn, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "### Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model with Self Attention and Causal Mask:\n",
            "\n",
            "\n",
            "Training Start\n",
            "\n",
            "Epoch [1/100], Time:2.99, Loss: 7.3040, Accuracy: 0.06%, Perplexity: 1486.1747\n",
            "Epoch [2/100], Time:2.87, Loss: 6.4204, Accuracy: 0.08%, Perplexity: 614.2295\n",
            "Epoch [3/100], Time:2.88, Loss: 5.9556, Accuracy: 0.11%, Perplexity: 385.9193\n",
            "Epoch [4/100], Time:3.03, Loss: 5.4894, Accuracy: 0.13%, Perplexity: 242.1127\n",
            "Epoch [5/100], Time:3.10, Loss: 5.1247, Accuracy: 0.15%, Perplexity: 168.1209\n",
            "Epoch [6/100], Time:3.53, Loss: 4.8321, Accuracy: 0.17%, Perplexity: 125.4745\n",
            "Epoch [7/100], Time:3.67, Loss: 4.5867, Accuracy: 0.18%, Perplexity: 98.1659\n",
            "Epoch [8/100], Time:3.48, Loss: 4.3737, Accuracy: 0.19%, Perplexity: 79.3368\n",
            "Epoch [9/100], Time:3.49, Loss: 4.1870, Accuracy: 0.21%, Perplexity: 65.8251\n",
            "Epoch [10/100], Time:3.47, Loss: 4.0224, Accuracy: 0.22%, Perplexity: 55.8356\n",
            "Epoch [11/100], Time:3.45, Loss: 3.8733, Accuracy: 0.24%, Perplexity: 48.1031\n",
            "Epoch [12/100], Time:3.50, Loss: 3.7383, Accuracy: 0.25%, Perplexity: 42.0285\n",
            "Epoch [13/100], Time:3.37, Loss: 3.6133, Accuracy: 0.27%, Perplexity: 37.0886\n",
            "Epoch [14/100], Time:3.46, Loss: 3.4984, Accuracy: 0.28%, Perplexity: 33.0640\n",
            "Epoch [15/100], Time:3.70, Loss: 3.3936, Accuracy: 0.30%, Perplexity: 29.7733\n",
            "Epoch [16/100], Time:3.57, Loss: 3.2891, Accuracy: 0.31%, Perplexity: 26.8192\n",
            "Epoch [17/100], Time:3.54, Loss: 3.1916, Accuracy: 0.32%, Perplexity: 24.3266\n",
            "Epoch [18/100], Time:3.37, Loss: 3.1012, Accuracy: 0.34%, Perplexity: 22.2238\n",
            "Epoch [19/100], Time:3.37, Loss: 3.0110, Accuracy: 0.35%, Perplexity: 20.3079\n",
            "Epoch [20/100], Time:3.64, Loss: 2.9259, Accuracy: 0.37%, Perplexity: 18.6516\n",
            "Epoch [21/100], Time:3.67, Loss: 2.8430, Accuracy: 0.38%, Perplexity: 17.1668\n",
            "Epoch [22/100], Time:3.38, Loss: 2.7644, Accuracy: 0.39%, Perplexity: 15.8689\n",
            "Epoch [23/100], Time:3.38, Loss: 2.6895, Accuracy: 0.41%, Perplexity: 14.7247\n",
            "Epoch [24/100], Time:3.35, Loss: 2.6175, Accuracy: 0.42%, Perplexity: 13.7015\n",
            "Epoch [25/100], Time:3.41, Loss: 2.5479, Accuracy: 0.44%, Perplexity: 12.7803\n",
            "Epoch [26/100], Time:3.41, Loss: 2.4859, Accuracy: 0.45%, Perplexity: 12.0119\n",
            "Epoch [27/100], Time:3.39, Loss: 2.4212, Accuracy: 0.46%, Perplexity: 11.2590\n",
            "Epoch [28/100], Time:3.35, Loss: 2.3602, Accuracy: 0.47%, Perplexity: 10.5934\n",
            "Epoch [29/100], Time:3.38, Loss: 2.3079, Accuracy: 0.48%, Perplexity: 10.0537\n",
            "Epoch [30/100], Time:3.56, Loss: 2.2472, Accuracy: 0.49%, Perplexity: 9.4612\n",
            "Epoch [31/100], Time:3.46, Loss: 2.2059, Accuracy: 0.50%, Perplexity: 9.0786\n",
            "Epoch [32/100], Time:3.52, Loss: 2.1575, Accuracy: 0.51%, Perplexity: 8.6499\n",
            "Epoch [33/100], Time:3.44, Loss: 2.1095, Accuracy: 0.52%, Perplexity: 8.2438\n",
            "Epoch [34/100], Time:3.45, Loss: 2.0676, Accuracy: 0.53%, Perplexity: 7.9057\n",
            "Epoch [35/100], Time:3.53, Loss: 2.0241, Accuracy: 0.53%, Perplexity: 7.5696\n",
            "Epoch [36/100], Time:3.48, Loss: 1.9903, Accuracy: 0.54%, Perplexity: 7.3179\n",
            "Epoch [37/100], Time:3.40, Loss: 1.9478, Accuracy: 0.55%, Perplexity: 7.0131\n",
            "Epoch [38/100], Time:3.35, Loss: 1.9132, Accuracy: 0.56%, Perplexity: 6.7745\n",
            "Epoch [39/100], Time:3.30, Loss: 1.8792, Accuracy: 0.56%, Perplexity: 6.5483\n",
            "Epoch [40/100], Time:3.34, Loss: 1.8501, Accuracy: 0.57%, Perplexity: 6.3602\n",
            "Epoch [41/100], Time:3.29, Loss: 1.8146, Accuracy: 0.58%, Perplexity: 6.1386\n",
            "Epoch [42/100], Time:3.28, Loss: 1.7869, Accuracy: 0.58%, Perplexity: 5.9708\n",
            "Epoch [43/100], Time:3.24, Loss: 1.7609, Accuracy: 0.59%, Perplexity: 5.8175\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[17], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Loss\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\fabio.grassiotto\\Research\\venv\\ml\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\fabio.grassiotto\\Research\\venv\\ml\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Model with Self Attention and Causal Mask:\")\n",
        "print()\n",
        "\n",
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model_attn.parameters(), lr)\n",
        "\n",
        "model_attn.to(device)\n",
        "\n",
        "print()\n",
        "print(\"Training Start\")\n",
        "print()\n",
        "train(model_attn, criterion, optimizer, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "### Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\n",
        "print(\"Evaluation Start\")\n",
        "print()\n",
        "eval(model_attn, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple decoder for tokens into a sentence\n",
        "def decode(tokens):\n",
        "  words = []\n",
        "  for key, value in vocab.items():\n",
        "    if value in tokens:\n",
        "      words.append(key)\n",
        "  \n",
        "  sentence = ' '.join(words)\n",
        "  sentence = sentence.capitalize() + '.'\n",
        "  return sentence\n",
        "\n",
        "seq = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "tokens = model_attn.generate(seq, 20, seq_len)[0].tolist()\n",
        "print(decode(tokens))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA024",
      "language": "python",
      "name": "ia024"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
