{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4HuVIYGgXg"
      },
      "source": [
        "# Respostas das Questões do Processo Seletivo Aluno Especial IA-024 1S2024 FEEC-UNICAMP\n",
        "\n",
        "## Aluno: Fabio Grassiotto\n",
        "## RA: 890441\n",
        "\n",
        "Link para o notebook com a implementação:  \n",
        "https://colab.research.google.com/drive/1xtikSpHPHJylcKZA_XAXJv9mSlV0cLS9?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqDQ0NVV2FZh"
      },
      "source": [
        "#### Seção I\n",
        "\n",
        "##### I.1. Na célula de calcular o vocabulário, aproveite o laço sobre IMDB de treinamento e utilize um segundo contador para calcular o número de amostras positivas e amostras negativas. Calcule também o comprimento médio do texto em número de palavras dos textos das amostras.\n",
        "\n",
        "Código implementado na seção I do notebook. Seguem os resultados obtidos:\n",
        "\n",
        "Amostras positivas, negativas e totais:\n",
        "Counter({'total': 25000, 'pos': 12500, 'neg': 12500})\n",
        "\n",
        "Comprimento médio do texto em palavras\n",
        "270.68748"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVMWC51b2FZh"
      },
      "source": [
        "##### I.2 Mostre as cinco palavras mais frequentes do vocabulário e as cinco palavras menos frequentes.\n",
        "\n",
        "(Utilizando o Tokenizador)\n",
        "\n",
        "Cinco palavras mais frequentes:\n",
        "['the', '.', ',', 'and', 'a']\n",
        "\n",
        "Cinco palavras menos frequentes:\n",
        "['voicing', 'hazard', 'lynda', 'gft', 'watergate']\n",
        "\n",
        "##### Qual é o código do token que está sendo utilizado quando a palavra não está no vocabulário?\n",
        "\n",
        "Na função de dicionário dict.get() o segundo parâmetro indica o valor default caso a palavra não seja encontrada no dicionário. Nesse caso o código do token usado é o número zero.\n",
        "\n",
        "##### Número de tokens que não estão no vocabulário na base de treinamento:\n",
        "174226\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdrwXXrY2FZh"
      },
      "source": [
        "#### I.3.a) Qual é a razão pela qual o modelo preditivo conseguiu acertar 100% das amostras de teste do dataset selecionado com apenas as primeiras 200 amostras?\n",
        "\n",
        "Ao reduzirmos a base de treinamento para apenas 200 amostras, a base se tornou totalmente desbalanceada. Como pudemos verificar, temos 200 amostras classificadas como negativas e nenhuma como positiva.\n",
        "Portanto a taxa de acurácia calculada sobre a classificação da base de testes depende unicamente da percentagem de amostras positivas ou negativas nesta base.\n",
        "\n",
        "#### I.3.b) Modifique a forma de selecionar 200 amostras do dataset, porém garantindo que ele continue balanceado, isto é, aproximadamente 100 amostras positivas e 100 amostras negativas.\n",
        "\n",
        "Para obtermos um dataset balanceado, usaremos uma função que seleciona amostras do dataset de acordo com a classificação e cria um dataset com a quantidade de amostras de cada classificação desejada conforme abaixo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BreW95PM2FZi"
      },
      "source": [
        "#### Seção II\n",
        "\n",
        "##### II.1.a) Investigue o dataset criado na linha 24. Faça um código que aplique um laço sobre o dataset train_data e calcule novamente quantas amostras positivas e negativas do dataset.\n",
        "\n",
        "Seção do código implementado:\n",
        "````\n",
        "counter_lbl = Counter({\"pos\": 0, \"neg\": 0, \"total\": 0})\n",
        "words_encoded = 0\n",
        "for (oneHot, sentiment) in train_data:\n",
        "\n",
        "    words = oneHot.tolist()\n",
        "    label = sentiment.item()\n",
        "\n",
        "    # Número de amostras positivas e negativas\n",
        "    if (label == 1):\n",
        "      counter_lbl['neg'] += 1\n",
        "    else:\n",
        "      counter_lbl['pos'] += 1\n",
        "    counter_lbl['total'] += 1\n",
        "\n",
        "    hot_encoded = sum(words[i] for i in range(len(words)) if words[i] != 0)\n",
        "    words_encoded +=  hot_encoded\n",
        "\n",
        "avg_words_enc = words_encoded / counter_lbl['total']\n",
        "````\n",
        "##### II.1.b) Calcule também o número médio de palavras codificadas em cada vetor one-hot.\n",
        "\n",
        "Quantidade média de palavras codificadas em cada vetor one-hot\n",
        "139.59268\n",
        "\n",
        "#### Compare este valor com o comprimento médio de cada texto (contado em palavras), conforme calculado no exercício I.1.c. e explique a diferença.\n",
        "\n",
        "No exercício I.1.c, o comprimento médio do texto em palavras depois de passar pelo tokenizador foi de cerca de 270 palavras. Essa diferença do vetor One-Hot se deve ao fato que o vetor one-hot só codifica as palavras que foram identificadas no dicionário, enquanto que o comprimento médio considera todas as palavras das sentenças. Ou seja, palavras que não foram codificadas no dicionário serão representadas por zeros.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr-z_bgy2FZi"
      },
      "source": [
        "##### II.2.a) Medição dos tempos de loop\n",
        "\n",
        "Notamos que o tempo do passo do forward leva mais tempo que o passo de backward, conforme os dados obtidos abaixo para a primeira época do treinamento.\n",
        "Também notamos que a maior parte to tempo do loop de forward é gasto com a transferência dos dados da CPU para a GPU (97% no primeiro loop).\n",
        "\n",
        "```\n",
        "Loop # 1\n",
        "Tempo de loop =  0.048320770263671875\n",
        "Forward pass =  0.047322750091552734\n",
        "Gpu copy = 97.88851606662435 %\n",
        "Model processing = 2.1114839333756574 %\n",
        "Backward pass =  0.0009980201721191406\n",
        "\n",
        "Loop # 2\n",
        "Tempo de loop =  0.007141590118408203\n",
        "Forward pass =  0.005140781402587891\n",
        "Gpu copy = 80.50737408403673 %\n",
        "Model processing = 19.49262591596327 %\n",
        "Backward pass =  0.0020008087158203125\n",
        "```\n",
        "##### II.2.b) Trecho que precisa ser otimizado. (Esse é um problema mais difícil)\n",
        "\n",
        "Para otimizarmos o loop, o carregamento dos dados em GPU pode ser realizado pelo Dataloader fora do loop de treinamento, para tanto alterando o método __init__() da classe IMDBDataset.\n",
        "```\n",
        "def __init__(self, split, vocab):\n",
        "    #self.data = list(IMDB(split=split))[:n_samples]\n",
        "    self.data = list(balanced_dataset(IMDB(split=split), n_samples))        \n",
        "    self.vocab = vocab\n",
        "```\n",
        "##### II.2.c) Otimize o código e explique aqui.\n",
        "Substituimos então com a nova implementação, onde o dataset inteiro é pré-processado, codificado em forma One-Hot (uma vez que tensores não suportam strings) e movido para a GPU antes do processo de treinamento:\n",
        "````\n",
        "def __init__(self, split, vocab):\n",
        "    \n",
        "    # II.2.b) Trecho que precisa ser otimizado. (Esse é um problema mais difícil)\n",
        "    self.data = list(balanced_dataset(IMDB(split='train'), n_samples))\n",
        "\n",
        "    if preload_to_gpu:          \n",
        "        labels = [x[0] for x in self.data]\n",
        "        lines = [x[1] for x in self.data]\n",
        "\n",
        "        # One-Hot Encoding\n",
        "        self.labels_enc = []\n",
        "        for l in labels:\n",
        "        l = 1 if l == 1 else 0\n",
        "        self.labels_enc.append(l)\n",
        "        self.labels_enc = torch.tensor(self.labels_enc)\n",
        "        self.labels_enc = self.labels_enc.to(device)\n",
        "\n",
        "        self.lines_enc = []\n",
        "        for l in lines:\n",
        "        X = torch.zeros(len(vocab) + 1)\n",
        "        for word in encode_sentence(l, vocab):\n",
        "            X[word] = 1\n",
        "        self.lines_enc.append(X)\n",
        "        self.lines_enc = [tensor.to(device) for tensor in self.lines_enc]\n",
        "\n",
        "    self.vocab = vocab\n",
        "````\n",
        "##### Comparação do tempo de treinamento com a otimização (GPU RTX2060 local):\n",
        "Sem pre-load em GPU:\n",
        "````\n",
        "Epoch [1/5],             Loss: 0.6911,             Elapsed Time: 61.36 sec\n",
        "Epoch [2/5],             Loss: 0.6929,             Elapsed Time: 58.69 sec\n",
        "Epoch [3/5],             Loss: 0.6984,             Elapsed Time: 58.95 sec\n",
        "Epoch [4/5],             Loss: 0.6792,             Elapsed Time: 58.60 sec\n",
        "Epoch [5/5],             Loss: 0.6874,             Elapsed Time: 58.59 sec\n",
        "````\n",
        "Com pre-load em GPU (RTX2060)\n",
        "````\n",
        "Epoch [1/5],             Loss: 0.6896,             Elapsed Time: 3.81 sec\n",
        "Epoch [2/5],             Loss: 0.6925,             Elapsed Time: 0.58 sec\n",
        "Epoch [3/5],             Loss: 0.6933,             Elapsed Time: 0.64 sec\n",
        "Epoch [4/5],             Loss: 0.6890,             Elapsed Time: 0.58 sec\n",
        "Epoch [5/5],             Loss: 0.6904,             Elapsed Time: 0.57 sec\n",
        "````\n",
        "Notamos, no entanto, que o uso de mémória na GPU se torna muito maior, conforme pode ser visualizado abaixo (5Gb/6Gb total):\n",
        "````\n",
        "[venv:ml] $ nvidia-smi\n",
        "Mon Feb 12 08:23:42 2024\n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 516.94       Driver Version: 516.94       CUDA Version: 11.7     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|                               |                      |               MIG M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
        "| N/A   76C    P8    12W /  N/A |   5035MiB /  6144MiB |      1%      Default |\n",
        "|                               |                      |                  N/A |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWuAbn_2E1XD"
      },
      "source": [
        "##### II.3 Faça a melhor escolha do LR, analisando o valor da acurácia no conjunto de teste, utilizando para cada valor de LR, a acurácia obtida. Faça um gráfico de Acurácia vs LR e escolha o LR que forneça a maior acurácia possível."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "594pDvrHE1XD",
        "outputId": "a5475fc4-b7b5-4c1e-c0da-e4e1cf1290b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR =  0.0001\n",
            "Epoch [1/5],               Loss: 0.6939,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5000 \n",
            "Epoch [2/5],               Loss: 0.6937,               Elapsed Time: 0.46 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5003 \n",
            "Epoch [3/5],               Loss: 0.6941,               Elapsed Time: 0.47 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5002 \n",
            "Epoch [4/5],               Loss: 0.6942,               Elapsed Time: 0.49 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5003 \n",
            "Epoch [5/5],               Loss: 0.6954,               Elapsed Time: 0.51 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5002 \n",
            "Test Accuracy: 52.412%\n",
            "\n",
            "LR =  0.001\n",
            "Epoch [1/5],               Loss: 0.6956,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.4998 \n",
            "Epoch [2/5],               Loss: 0.6914,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5000 \n",
            "Epoch [3/5],               Loss: 0.6853,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5003 \n",
            "Epoch [4/5],               Loss: 0.6860,               Elapsed Time: 0.36 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.4999 \n",
            "Epoch [5/5],               Loss: 0.7005,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.4990 \n",
            "Test Accuracy: 54.152%\n",
            "\n",
            "LR =  0.01\n",
            "Epoch [1/5],               Loss: 0.6765,               Elapsed Time: 0.36 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5000 \n",
            "Epoch [2/5],               Loss: 0.6476,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.4998 \n",
            "Epoch [3/5],               Loss: 0.5837,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5001 \n",
            "Epoch [4/5],               Loss: 0.4483,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5000 \n",
            "Epoch [5/5],               Loss: 0.5591,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5001 \n",
            "Test Accuracy: 82.02%\n",
            "\n",
            "LR =  0.1\n",
            "Epoch [1/5],               Loss: 0.3548,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5001 \n",
            "Epoch [2/5],               Loss: 0.2222,               Elapsed Time: 0.36 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5007 \n",
            "Epoch [3/5],               Loss: 0.2340,               Elapsed Time: 0.36 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5002 \n",
            "Epoch [4/5],               Loss: 0.3581,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5002 \n",
            "Epoch [5/5],               Loss: 0.3277,               Elapsed Time: 0.37 sec,               Loader Iterations: 196,               Spls lst batch: 40 ,               R avg: 0.5004 \n",
            "Test Accuracy: 88.212%\n",
            "\n",
            "[0.0001, 0.001, 0.01, 0.1]\n",
            "[52.412, 54.152, 82.02, 88.212]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lr_list = [0.0001, 0.001, 0.01, 0.1]\n",
        "acc_list = []\n",
        "\n",
        "for lr in lr_list:\n",
        "    print(\"LR = \", lr)\n",
        "    model = OneHotMLP(vocab_size) # to reset weights\n",
        "    train_mdl(model, lr)\n",
        "    acc_list.append(eval_mdl(model))\n",
        "    print()\n",
        "\n",
        "print(lr_list)\n",
        "print(acc_list)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc51vZ3pE1XD"
      },
      "source": [
        "##### II.3.a) Gráfico Acurácia vs LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "tx1o-KJLE1XD",
        "outputId": "537448bb-3b90-4c55-9f5e-cba369cdaee6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.lineplot(x=lr_list, y=acc_list)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs Learning Rate\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjsz_MbpE1XD"
      },
      "source": [
        "##### II.3.b) Valor ótimo do LR\n",
        "\n",
        "Notamos que o valor ótimo para a Learning Rate foi de cerca de 0.1, com crescimento exponencial ao aumentá-la. Valores acima deste são grandes demais e não levam à otimização do modelo.\n",
        "\n",
        "##### II.3.c) Mostre a equação utilizada no gradiente descendente e qual é o papel do LR no ajuste dos parâmetros (weights) do modelo da rede neural.\n",
        "\n",
        "No processo de otimização de uma função, a fórmula utilizada para a estimativa do próximo valor da função é dada por:\n",
        "\n",
        "````\n",
        "valor atualizado = valor anterior - learning rate*gradiente\n",
        "`````\n",
        "\n",
        "Portanto o papel da LR é definir qual é o *tamanho* do passo a ser utilizado no processo de atualização."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxy2iwk0E1XD"
      },
      "source": [
        "##### II.4 Melhores a forma de tokenizar, isto é, pré-processar o dataset de modo que a codificação seja indiferente das palavras serem escritas com maiúsculas ou minúsculas e sejam pouco influenciadas pelas pontuações.\n",
        "##### II.4.a) Mostre os trechos modificados para este novo tokenizador, tanto na seção I - Vocabulário, como na seção II - Dataset.\n",
        "\n",
        "Na seção I - Vocabulário:\n",
        "\n",
        "````\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "for (label, line) in list(IMDB(split='train'))[:n_samples]:\n",
        "    if (use_tokenizer):\n",
        "      tokenizer = get_tokenizer('basic_english')\n",
        "      # tokenize the sentence\n",
        "      line = tokenizer(line)\n",
        "    counter.update(line.split())\n",
        "\n",
        "    # Número de amostras positivas e negativas\n",
        "    if (label == 1):\n",
        "      counter_lbl['neg'] += 1\n",
        "    else:\n",
        "      counter_lbl['pos'] += 1\n",
        "    counter_lbl['total'] += 1\n",
        "\n",
        "    # Comprimento médio do texto das reviews em palavras\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "    # tokenize the sentence\n",
        "    tokens = tokenizer(line)\n",
        "\n",
        "    # count the number of words\n",
        "    total_review_len += len(tokens)\n",
        "\n",
        "````\n",
        "\n",
        "Na Seção II - Dataset:\n",
        "São apenas necessárias alterações no encoder da sentença, conforme abaixo.\n",
        "\n",
        "````\n",
        "def encode_sentence(sentence, vocab, use_tokenizer):\n",
        "    if (use_tokenizer):\n",
        "       sentence = tokenizer(sentence)\n",
        "       return [vocab.get(word, 0) for word in sentence]\n",
        "    else:\n",
        "      return [vocab.get(word, 0) for word in sentence.split()] # 0 for OOV\n",
        "````\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Uxv3COE1XE"
      },
      "source": [
        "##### II.4.b) Recalcule novamente os valores do exercício I.2.c - número de tokens unknown, e apresente uma tabela comparando os novos valores com os valores obtidos com o tokenizador original e justifique os resultados obtidos.\n",
        "\n",
        "Sem o tokenizador:\n",
        "\n",
        "566141\n",
        "\n",
        "Com o tokenizador:\n",
        "\n",
        "174226\n",
        "\n",
        "Estes valores se justificam pelo fato que o tokenizador altera as palavras das sentenças, mantendo apenas radicais, de forma que menos palavras não serão encontradas na base do vocabulário.\n",
        "\n",
        "\n",
        "##### II.4.c) Execute agora no notebook inteiro com o novo tokenizador e veja o novo valor da acurácia obtido com a melhoria do tokenizador.\n",
        "\n",
        "Sem o tokenizador:\n",
        "\n",
        "Test Accuracy: 73.45% (Para LR = 0.1)\n",
        "\n",
        "Com o tokenizador\n",
        "\n",
        "Test Accuracy: 88.47% (Para LR = 0.1)\n",
        "\n",
        "O aumento da acurácia é justificado pelo fato que menos palavras de cada sentença não serão reconhecidas (OneHot encoding não terá tantos valores zerados)\n",
        "\n",
        "##### Os dados obtidos estão resumidos na tabela abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P0mX9hVE1XE",
        "outputId": "81ae1a28-723f-40d3-a7e9-5d788d98bd75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uso do Tokenizador      Tokens Unknown  Test Accuracy\n",
            "--------------------  ----------------  ---------------\n",
            "Sem Tokenizador                 566141  73.45%\n",
            "Com Tokenizador                 174226  88.47%\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    ['Sem Tokenizador', 566141, '73.45%'],\n",
        "    ['Com Tokenizador', 174226, '88.47%'],\n",
        "]\n",
        "\n",
        "# Headers\n",
        "headers = ['Uso do Tokenizador', 'Tokens Unknown', 'Test Accuracy']\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(data, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNc6iW_iE1XE"
      },
      "source": [
        "#### Seção III\n",
        "\n",
        "##### Vamos estudar agora o Data Loader da seção III do notebook. Em primeiro lugar anote a acurácia do notebook com as melhorias de eficiência de rodar em GPU, com ajustes de LR e do tokenizador. Em seguida mude o parâmetro shuffle na construção do objeto train_loader para False e execute novamente o notebook por completo e meça novamente a acurácia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv0fHI8rE1XE",
        "outputId": "e9155b83-dd0b-4ff6-b108-96132d698b2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shuffle dos dados de Treinamento    Test Accuracy\n",
            "----------------------------------  ---------------\n",
            "Com Shuffle                         88.47%\n",
            "Sem Shuffle                         50.00%\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    ['Com Shuffle', '88.47%'],\n",
        "    ['Sem Shuffle', '50.00%']\n",
        "]\n",
        "\n",
        "# Headers\n",
        "headers = ['Shuffle dos dados de Treinamento', 'Test Accuracy']\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(data, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS6Bj4jV2FZl"
      },
      "source": [
        "##### III.1.a) Explique as duas principais vantagens do uso de batch no treinamento de redes neurais.\n",
        "\n",
        "O uso de lotes em treinamento é importante por causa do aumento da eficiência computacional e para aumentar a estabilidade do gradiente.\n",
        "A eficiência computacional é aumentada pois com lotes maiores a paralelização do processamento em GPUs é mais bem aproveitada, enquanto que a estabilidade do gradiente é aumentada durante o treinamento pois em cada iteração, o gradiente é calculado com base na função de perda para o lote inteiro reduzindo a variabilidade do gradiente em comparação com o cálculo individual para cada exemplo.\n",
        "\n",
        "##### III.1.b) Explique por que é importante fazer o embaralhamento das amostras do batch em cada nova época.\n",
        "\n",
        "O embarelhamento das amostras de batch do treinamento é essencial para aumentar a generabilidade do modelo. As razões para tanto são:\n",
        "\n",
        "* Redução do viés das amostras ordenadas do início ao fim do dataset.\n",
        "* Estabilização do gradiente (redução da oscilação causada por amostras ordenadas).\n",
        "* Melhoria da convergência, pois amostras agrupadas de uma classe dificultam o processo de aprendizado da rede neural.\n",
        "\n",
        "Em geral o embaralhamento de amostras de treinamento é um processo usual para a redução da generalização e a obtenção de um modelo de melhores características.\n",
        "\n",
        "\n",
        "##### III.1.c) Se você alterar o shuffle=False no instanciamento do objeto test_loader, por que o cálculo da acurácia não se altera?\n",
        "\n",
        "A acurácia não se altera pois em tempo de inferência (ou seja, fase de teste) os pesos do modelo não são alterados mais. Portanto, a base de testes é usada apenas para verificar a capacidade de generalização do modelo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY_VsNaq2FZl"
      },
      "source": [
        "##### III.2.a) Faça um laço no objeto train_loader e meça quantas iterações o Loader tem. Mostre o código para calcular essas iterações. Explique o valor encontrado.\n",
        "\n",
        "Modificações no código de treinamento (função train_mdl()) acima:\n",
        "````\n",
        "  for epoch in range(num_epochs):\n",
        "      start_time = time.time()  # Start time of the epoch\n",
        "      model.train()\n",
        "\n",
        "      loop_count = 0\n",
        "\n",
        "      train_loader_iterations = 0    \n",
        "      for inputs, labels in train_loader:\n",
        "          train_loader_iterations += 1\n",
        "````\n",
        "Número de interações por época:\n",
        "````\n",
        "Epoch [1/5],               Loss: 0.3918,               Elapsed Time: 6.79 sec,               Loader Iterations: 196\n",
        "Epoch [2/5],               Loss: 0.3028,               Elapsed Time: 0.56 sec,               Loader Iterations: 196\n",
        "Epoch [3/5],               Loss: 0.1997,               Elapsed Time: 0.57 sec,               Loader Iterations: 196\n",
        "Epoch [4/5],               Loss: 0.1883,               Elapsed Time: 0.57 sec,               Loader Iterations: 196\n",
        "Epoch [5/5],               Loss: 0.3806,               Elapsed Time: 0.56 sec,               Loader Iterations: 196\n",
        "\n",
        "````\n",
        "\n",
        "##### III.2.b) Imprima o número de amostras do último batch do train_loader e justifique o valor encontrado? Ele pode ser menor que o batch_size?\n",
        "\n",
        "````\n",
        "Number of samples in last batch: 40\n",
        "````\n",
        "O valor encontrado é menor que o tamanho do batch size (nesse caso, 128) pois esta é a quantidade de amostras restantes nas base. Como temos 196 iterações, o total de amostras nos primeiros 195 ciclos totaliza 24.960. Portanto, o último batch tem um total de 25.000 (tamanho da base) - 24.960 = 40.\n",
        "\n",
        "##### III.2.c) Calcule R, a relação do número de amostras positivas sobre o número de amostras no batch e no final encontre o valor médio de R, para ver se o data loader está entregando batches balanceados. Desta vez, em vez de fazer um laço explícito, utilize list comprehension para criar uma lista contendo a relação R de cada amostra no batch. No final, calcule a média dos elementos da lista para fornecer a resposta final.\n",
        "\n",
        "Médias de R por época:\n",
        "````\n",
        "R avg: 0.4999\n",
        "R avg: 0.5004\n",
        "R avg: 0.5003\n",
        "R avg: 0.4999\n",
        "R avg: 0.5000\n",
        "````\n",
        "\n",
        "##### III.2.d) Mostre a estrutura de um dos batches. Cada batch foi criado no método __getitem__ do Dataset, linha 20. É formado por uma tupla com o primeiro elemento sendo a codificação one-hot do texto e o segundo elemento o label esperado, indicando positivo ou negativo. Mostre o shape (linhas e colunas) e o tipo de dado (float ou integer), tanto da entrada da rede como do label esperado. Desta vez selecione um elemento do batch do train_loader utilizando as funções next e iter: batch = next(iter(train_loader))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj1WP5H42FZl",
        "outputId": "f9b2252a-eeef-4a2e-ae2c-d8d48ecfea19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dado de entrada:\n",
            "torch.Size([20001])\n",
            "torch.float32\n",
            "\n",
            "Label:\n",
            "torch.Size([1])\n",
            "torch.int64\n"
          ]
        }
      ],
      "source": [
        "my_loader = DataLoader(train_data, batch_size=1)\n",
        "batch = next(iter(my_loader))\n",
        "\n",
        "# Dado de entrada\n",
        "entrada = batch[0].tolist()\n",
        "dado_entrada = (batch[0])[0]\n",
        "\n",
        "print(\"Dado de entrada:\")\n",
        "print(dado_entrada.size())\n",
        "print(dado_entrada.dtype)\n",
        "print()\n",
        "\n",
        "# Label\n",
        "print(\"Label:\")\n",
        "lbl = batch[1]\n",
        "print(lbl.size())\n",
        "print(lbl.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV3MDCCG2FZm"
      },
      "source": [
        "##### III.3.a) Verifique a influência do batch size na acurácia final do modelo. Experimente usar um batch size de 1 amostra apenas e outro com mais de 128 e comente sobre os resultados.\n",
        "\n",
        "Notei que o cálculo de perda da linha criterion() gera um erro com batch size = 1, então usei batch size = 2 para este exercício."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6btw7Mf2FZm",
        "outputId": "3b7700b2-0df6-49ab-e6cf-fd0e75795095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5],               Loss: 0.0217,               Elapsed Time: 17.09 sec,               Loader Iterations: 12500,               Spls lst batch: 2 ,               R avg: 0.5000 \n",
            "Epoch [2/5],               Loss: 0.5510,               Elapsed Time: 16.61 sec,               Loader Iterations: 12500,               Spls lst batch: 2 ,               R avg: 0.5000 \n",
            "Epoch [3/5],               Loss: 0.3738,               Elapsed Time: 16.82 sec,               Loader Iterations: 12500,               Spls lst batch: 2 ,               R avg: 0.5000 \n",
            "Epoch [4/5],               Loss: 0.0002,               Elapsed Time: 17.54 sec,               Loader Iterations: 12500,               Spls lst batch: 2 ,               R avg: 0.5000 \n",
            "Epoch [5/5],               Loss: 0.0039,               Elapsed Time: 17.01 sec,               Loader Iterations: 12500,               Spls lst batch: 2 ,               R avg: 0.5000 \n",
            "Test Accuracy: 86.568%\n",
            "\n",
            "Epoch [1/5],               Loss: 0.4962,               Elapsed Time: 0.35 sec,               Loader Iterations: 98,               Spls lst batch: 168 ,               R avg: 0.4999 \n",
            "Epoch [2/5],               Loss: 0.4107,               Elapsed Time: 0.35 sec,               Loader Iterations: 98,               Spls lst batch: 168 ,               R avg: 0.5003 \n",
            "Epoch [3/5],               Loss: 0.3138,               Elapsed Time: 0.38 sec,               Loader Iterations: 98,               Spls lst batch: 168 ,               R avg: 0.5000 \n",
            "Epoch [4/5],               Loss: 0.3772,               Elapsed Time: 0.36 sec,               Loader Iterations: 98,               Spls lst batch: 168 ,               R avg: 0.4999 \n",
            "Epoch [5/5],               Loss: 0.2645,               Elapsed Time: 0.38 sec,               Loader Iterations: 98,               Spls lst batch: 168 ,               R avg: 0.5002 \n",
            "Test Accuracy: 87.728%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Acurácia com batch = 2\n",
        "train_loader = DataLoader(train_data, batch_size=2, shuffle=train_shuffle)\n",
        "model = OneHotMLP(vocab_size) # to reset weights\n",
        "train_mdl(model, best_LR)\n",
        "eval_mdl(model)\n",
        "print()\n",
        "\n",
        "# Acurácia com batch = 256\n",
        "train_loader = DataLoader(train_data, batch_size=256, shuffle=train_shuffle)\n",
        "model = OneHotMLP(vocab_size) # to reset weights\n",
        "train_mdl(model, best_LR)\n",
        "eval_mdl(model)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSj_23Ar2FZn"
      },
      "source": [
        "Pudemos verificar que o batch size muito reduzido aumenta em muito a acurácia, mas em contrapartida aumenta muito a complexidade computacional. O ganho da acurácia pode ser explicado pela melhoria na generalização. Nesse caso os pesos do modelo são atualizados depois da análise de cada amostra de forma independente. O aumento do batch size de 128 para 256 não trouxe ganhos na acurácia.\n",
        "Portanto, para datasets pequenos como o caso deste exercício, uma redução do tamanho do batch pode ser benéfico desde que o custo computacional não seja excessivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jy0NBkx2FZn"
      },
      "source": [
        "#### Seção IV\n",
        "\n",
        "##### IV.1.a) Faça a predição do modelo utilizando um batch do train_loader: extraia um batch do train_loader, chame de (input, target), onde input é a entrada da rede e target é o label esperado. Como a rede está com seus parâmetros (weights) aleatórios, o logito de saída da rede será um valor aleatório, porém a chamada irá executar sem erros:\n",
        "\n",
        "logit = model( input)\n",
        "\n",
        "aplique a função sigmoidal ao logito para convertê-lo numa probabilidade de valor entre 0 e 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfMYPTK_2FZn",
        "outputId": "d1746533-f216-43ec-eb84-10df770827df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilidade: 47.73 %\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "new_loader = DataLoader(train_data, batch_size=128, shuffle=train_shuffle)\n",
        "model = OneHotMLP(vocab_size).to(device)\n",
        "\n",
        "input, target = next(iter(new_loader))\n",
        "logit = model(input)\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "probability = sigmoid(logit[0])*100\n",
        "\n",
        "# Cálculo da probabilidade para a primeira amostra\n",
        "print(f'Probabilidade: {probability.item():.2f} %')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWzzNgym2FZn"
      },
      "source": [
        "##### IV.1.b) Agora, treine a rede executando o notebook todo e verifique se a acurácia está alta. Agora repita o exercício anterior, porém agora, compare o valor da probabilidade encontrada com o target esperado e verifique se ele acertou. Você pode considerar que se a probabilidade for maior que 0.5, pode-se dar o label 1 e se for menor que 0.5, o label 0. Observe isso que é feito na linha 11 da seção VI - Avaliação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4NIsB7t2FZn",
        "outputId": "e7f7957b-79f0-438f-eadc-c0dc38cf04c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predição =  0.0\n",
            "Target Esperado =  1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "new_loader = DataLoader(train_data, batch_size=128, shuffle=train_shuffle)\n",
        "model = OneHotMLP(vocab_size).to(device)\n",
        "\n",
        "input, target = next(iter(new_loader))\n",
        "logit = model(input).cpu()\n",
        "\n",
        "predicted = torch.round(torch.sigmoid(logit.squeeze()))\n",
        "\n",
        "print(\"Predição = \", predicted[0].item())\n",
        "print(\"Target Esperado = \", target[0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gmBPP3k2FZo"
      },
      "source": [
        "Se você der um print no modelo: print(model), você obterá:  \n",
        "````\n",
        "OneHotMLP(\n",
        "  (fc1): Linear(in_features=20001, out_features=200, bias=True)  \n",
        "  (fc2): Linear(in_features=200, out_features=1, bias=True)  \n",
        "  (relu): ReLU()\n",
        ")\n",
        "````\n",
        "Os pesos da primeira camada podem ser visualizados com model.fc1.weight e o elemento constante (bias) pode ser visualizado com model.fc1.bias  \n",
        "Calcule o número de parâmetros do modelo, preenchendo a seguinte tabela (utilize shape para verificar a estrutura de cada parâmetro do modelo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqBNK4Nq2FZo",
        "outputId": "9df729cb-a705-47a7-8de7-7ec9d8f32ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Parameters:\n",
            "FC1 weights dimensions:  [200, 20001]\n",
            "FC2 weights dimensions:  [1, 200]\n",
            "FC1 bias dimensions:  [200]\n",
            "FC2 bias dimensions:  [1]\n",
            "\n",
            "layer    fc1            fc2           total\n",
            "-------  -------  ----  ------  ----  -------\n",
            "         weight   bias  weight  bias\n",
            "size     4000200  200   200     1\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "model = OneHotMLP(vocab_size)\n",
        "\n",
        "w_fc1 = model.fc1.weight.size()\n",
        "b_fc1 = model.fc1.bias.size()\n",
        "w_fc2 = model.fc2.weight.size()\n",
        "b_fc2 = model.fc2.bias.size()\n",
        "\n",
        "print(\"Model Parameters:\")\n",
        "print(\"FC1 weights dimensions: \", list(w_fc1))\n",
        "print(\"FC2 weights dimensions: \", list (w_fc2))\n",
        "print(\"FC1 bias dimensions: \", list(b_fc1))\n",
        "print(\"FC2 bias dimensions: \", list (b_fc2))\n",
        "print()\n",
        "\n",
        "# Table data\n",
        "data = [\n",
        "    ['', 'weight', 'bias', 'weight', 'bias', ''],\n",
        "    ['size', w_fc1[0]*w_fc1[1], b_fc1[0], w_fc2[0]*w_fc2[1], b_fc2[0], '']\n",
        "]\n",
        "\n",
        "# Headers\n",
        "headers = ['layer', 'fc1', '', 'fc2', '', 'total']\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(data, headers=headers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_BZEYIV2FZo"
      },
      "source": [
        "#### Seção V\n",
        "\n",
        "##### V.1.a) Qual é o valor teórico da Loss quando o modelo não está treinado, mas apenas inicializado? Isto é, a probabilidade predita tanto para a classe 0 como para a classe 1, é sempre 0,5 ? Justifique. Atenção: na equação da Entropia Cruzada utilize o logaritmo natural.\n",
        "\n",
        "Utilizando a equação da entropia cruzada, podemos obter o valor teórico da perda:\n",
        "\n",
        "Loss = -1/n SumN (yi * ln(y^i) + (1-yi)ln(1-y^i))\n",
        "\n",
        "com y^i = 0.5, temos: ln(y^i) = ln(1-y^i) = ln(0.5) = -0.69314\n",
        "\n",
        "Loss = -1/n SumN(yi * (-0.69314) + (1-yi) * (-0.69314))\n",
        "Loss = -1/n SumN(-0.69314yi - 0.69314 + 0.69314yi)\n",
        "\n",
        "cancelando ambos termos em yi ->\n",
        "\n",
        "Loss = -1/n SumN(-0.69314) e portanto Loss = 0.69314 para o modelo inicializado independente do número de amostras N.\n",
        "\n",
        "No entanto, para um modelo não inicializado, o valor da perda depende do valor dos pesos da rede neural não inicializada, que pode variar e não ser o mesmo que o valor teórico.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL2ByJ1r2FZo"
      },
      "source": [
        "##### V.1.b) Utilize as amostras do primeiro batch: (input,target) = next(iter(train_loader)) e calcule o valor da Loss utilizando a equação fornecida anteriormente utilizando o pytorch. Verifique se este valor confere com o valor teórico do exercício anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CcsKJ_m2FZo",
        "outputId": "2df287da-f1cd-43af-a3fa-bbcd22ade4ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6917, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "new_loader = DataLoader(train_data, batch_size=128, shuffle=train_shuffle)\n",
        "model = OneHotMLP(vocab_size).to(device)\n",
        "\n",
        "input, target = next(iter(new_loader))\n",
        "logit = model(input)\n",
        "prob = torch.sigmoid(logit)\n",
        "\n",
        "# Calculo numérico da perda\n",
        "loss = - torch.sum(torch.mul(target, torch.log(prob).t()) + torch.mul(1-target, torch.log(1-prob).t())) / prob.shape[0]\n",
        "print(loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQuvA4Nn2FZp"
      },
      "source": [
        "Notamos que para um batch acima o valor da perda calculada não é a mesma da perda teórica, mas é muito próxima devido aos valores dos pesos não inicializados da rede neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6iCO6rn2FZp"
      },
      "source": [
        "##### V.1.c) O pytorch possui várias funções que facilitam o cálculo da Loss pela Entropia Cruzada. Utilize a classe nn.BCELoss (Binary Cross Entropy Loss). Você primeiro deve instanciar uma função da classe nn.BCELoss. Esta função instanciada recebe dois parâmetros (probs , targets) e retorna a Loss. Use a busca do Google para ver a documentação do BCELoss do pytorch. Calcule então a função de Loss da entropia cruzada, porém usando agora a função instanciada pelo BCELoss e confira se o resultado é exatamente o mesmo obtido no exercício anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kne3fSDl2FZp",
        "outputId": "cbf00fc9-b02d-4422-9ad8-aac2f859cb46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6917, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "loss = loss_fn(prob.squeeze(), target.float())\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQHxVvdg2FZp"
      },
      "source": [
        "Notamos que o valor foi o mesmo que o obtido acima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy8A-4W-2FZq"
      },
      "source": [
        "##### V.1.d) Repita o mesmo exercício, porém agora usando a classe nn.BCEWithLogitsLoss, que é a opção utilizada no notebook. O resultado da Loss deve igualar aos resultados anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h4Y_zc52FZq",
        "outputId": "961bc5c2-b929-42f3-dcbe-a880fd2decad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6917, device='cuda:0',\n",
            "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "loss = loss_fn(logit.squeeze(), target.float())\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSduI85Q2FZq"
      },
      "source": [
        "Novamente chegamos ao mesmo valor calculado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI2HO_Pc2FZr"
      },
      "source": [
        "##### V.2.a) Modifique a célula do laço de treinamento de modo que a primeira Loss a ser impressa seja a Loss com o modelo inicializado (isto é, sem nenhum treinamento), fornecendo a Loss esperada conforme os exercícios feitos anteriormente. Observe que desta forma, fica fácil verificar se o seu modelo está correto e a Loss está sendo calculada corretamente. Atenção: Mantenha esse código da impressão do valor da Loss inicial, antes do treinamento, nesta célula, pois ela é sempre útil para verificar se não tem nada errado, antes de começar o treinamento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBc0zeWF2FZr",
        "outputId": "97aec529-b9cb-46d5-a1a2-8377a26e6e5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss before training: 0.6928\n",
            "\n",
            "Epoch [1/5],               Loss: 0.4690\n",
            "Epoch [2/5],               Loss: 0.3171\n",
            "Epoch [3/5],               Loss: 0.3201\n",
            "Epoch [4/5],               Loss: 0.3079\n",
            "Epoch [5/5],               Loss: 0.3412\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Medição da perda\n",
        "def train_first_loss(model, lr):\n",
        "\n",
        "  model = model.to(device)\n",
        "  # Define loss and optimizer\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr)\n",
        "\n",
        "  # Training loop\n",
        "  num_epochs = 5\n",
        "  # First loss calculation\n",
        "  is_first_loss = True\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      start_time = time.time()\n",
        "      model.train()\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "\n",
        "          if not preload_to_gpu:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs.squeeze(), labels.float())\n",
        "\n",
        "          if is_first_loss:\n",
        "            print(f'Loss before training: {loss.item():.4f}')\n",
        "            is_first_loss = False\n",
        "            print()\n",
        "\n",
        "          # Backward and optimize\n",
        "          backward_start = time.time()\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
        "              Loss: {loss.item():.4f}')\n",
        "  print()\n",
        "\n",
        "model = OneHotMLP(vocab_size)\n",
        "train_first_loss(model, best_LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC_afFkI2FZr"
      },
      "source": [
        "Notamos que o primeiro valor calculado da perda se manteve o mesmo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuxwlsvL2FZr"
      },
      "source": [
        "##### V.2.b) Execute a célula de treinamento por uma segunda vez e observe que a Loss continua diminuindo e o modelo está continuando a ser treinado. O que é necessário fazer para que o treinamento comece novamente do modelo aleatório? Qual(is) célula(s) é(são) preciso executar antes de executar o laço de treinamento novamente?\n",
        "\n",
        "Para que o treinamento inicie novamente, os pesos devem ser resetados a seus valores iniciais.\n",
        "Uma maneira de fazer isso é criando uma função que resete os parãmetros de cada camada, por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4LM6j5Ht2FZs"
      },
      "outputs": [],
      "source": [
        "def reset_weights(model):\n",
        "  for module in model.modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "      module.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlDc42iD2FZs"
      },
      "source": [
        "E adicionar ao loop de treinamento acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YCjHJX0L2FZs"
      },
      "outputs": [],
      "source": [
        "# Medição da perda\n",
        "def train_first_loss_reset(model, lr):\n",
        "\n",
        "  model = model.to(device)\n",
        "  reset_weights(model)\n",
        "  # Define loss and optimizer\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr)\n",
        "\n",
        "  # Training loop\n",
        "  num_epochs = 5\n",
        "  # First loss calculation\n",
        "  is_first_loss = True\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      start_time = time.time()\n",
        "      model.train()\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "\n",
        "          if not preload_to_gpu:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs.squeeze(), labels.float())\n",
        "\n",
        "          if is_first_loss:\n",
        "            print(f'Loss before training: {loss.item():.4f}')\n",
        "            is_first_loss = False\n",
        "            print()\n",
        "\n",
        "          # Backward and optimize\n",
        "          backward_start = time.time()\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
        "              Loss: {loss.item():.4f}')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3qDZo3J2FZs"
      },
      "source": [
        "Sem o reset de parâmetros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjUXeM3U2FZs",
        "outputId": "40a50489-cf90-4ced-b964-2fe7ae8947da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss before training: 0.6927\n",
            "\n",
            "Epoch [1/5],               Loss: 0.4922\n",
            "Epoch [2/5],               Loss: 0.3288\n",
            "Epoch [3/5],               Loss: 0.3500\n",
            "Epoch [4/5],               Loss: 0.2923\n",
            "Epoch [5/5],               Loss: 0.3409\n",
            "\n",
            "Loss before training: 0.2243\n",
            "\n",
            "Epoch [1/5],               Loss: 0.3253\n",
            "Epoch [2/5],               Loss: 0.2242\n",
            "Epoch [3/5],               Loss: 0.2936\n",
            "Epoch [4/5],               Loss: 0.2388\n",
            "Epoch [5/5],               Loss: 0.1457\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = OneHotMLP(vocab_size)\n",
        "train_first_loss(model, best_LR)\n",
        "train_first_loss(model, best_LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8cEU7Z_2FZt"
      },
      "source": [
        "Com reset de parâmetros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4S-eXBs2FZt",
        "outputId": "3fd581b9-8ac2-41e7-e213-5724b7a73306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss before training: 0.6957\n",
            "\n",
            "Epoch [1/5],               Loss: 0.4929\n",
            "Epoch [2/5],               Loss: 0.3588\n",
            "Epoch [3/5],               Loss: 0.2829\n",
            "Epoch [4/5],               Loss: 0.2523\n",
            "Epoch [5/5],               Loss: 0.2483\n",
            "\n",
            "Loss before training: 0.6972\n",
            "\n",
            "Epoch [1/5],               Loss: 0.4644\n",
            "Epoch [2/5],               Loss: 0.3141\n",
            "Epoch [3/5],               Loss: 0.3423\n",
            "Epoch [4/5],               Loss: 0.3514\n",
            "Epoch [5/5],               Loss: 0.3239\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = OneHotMLP(vocab_size)\n",
        "train_first_loss_reset(model, best_LR)\n",
        "train_first_loss_reset(model, best_LR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAvSd6EZ2FZt"
      },
      "source": [
        "##### V.3.a) Repita o exercício V.1.a) porém agora utilizando a equação acima.\n",
        "\n",
        "C - número de classes  \n",
        "N - número de amostras no batch\n",
        "\n",
        "\n",
        "H(y,y^) = -(1/N) sumN (sumC yij*log(yi,c))\n",
        "\n",
        "Neste caso, temos 2 classes (positiva e negativa, e portanto y^ij = 50%)\n",
        "\n",
        "daí log(y^ij) = log(0.5) = -0.69314.\n",
        "\n",
        "Com duas classes:\n",
        "\n",
        "sumC(yij*log(y^i,c)) = 2*(yij*(-0.69314)) = -1.38628*yij\n",
        "\n",
        "H(y,y) = -1/(N)* sumC(-1.38628*yij)\n",
        "\n",
        "Se temos duas classes, podemos assumir que metade são da classe 0 e metade da classe 1, e portanto\n",
        "\n",
        "sumC(yij) = N/2*(-1.38628)\n",
        "\n",
        "daí a perda seria dada por\n",
        "\n",
        "H(y,y^) = -1/(N)*(N/2)*(-1.28628) = 1.38628/2 = 0.69314\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJniQg--2FZu"
      },
      "source": [
        "##### V.3.b) Modifique a camada de saída da rede para 2 logitos e utilize a função Softmax para converter os logitos em probabilidades. Repita o exercício V.1.b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8WgjtHAi2FZu"
      },
      "outputs": [],
      "source": [
        "class OneHotMLP_2logits(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(OneHotMLP_2logits, self).__init__()\n",
        "        self.fc1 = nn.Linear(vocab_size+1, 200)\n",
        "        self.fc2 = nn.Linear(200, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.fc1(x.float())\n",
        "        o = self.relu(o)\n",
        "        o = self.fc2(o)\n",
        "        return self.softmax(o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKwzqK7C2FZu",
        "outputId": "6198b7c2-f9af-409f-86c2-57cb6b0c0d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6941, device='cuda:0', grad_fn=<NegBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "new_loader = DataLoader(train_data, batch_size=128, shuffle=train_shuffle)\n",
        "model = OneHotMLP_2logits(vocab_size).to(device)\n",
        "\n",
        "input, target = next(iter(new_loader))\n",
        "probs_2logits = model(input)\n",
        "\n",
        "# Calculo numérico da perda\n",
        "log_probs = F.log_softmax(probs_2logits, dim=1)\n",
        "log_probs_correct_class = torch.gather(log_probs, 1, target.unsqueeze(1)).squeeze(1)\n",
        "loss = -log_probs_correct_class.mean()\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjEzJPRg2FZu"
      },
      "source": [
        "##### V.3.c) Utilize agora a função nn.CrossEntropyLoss para calcular a Loss e verifique se os resultados são os mesmos que anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUREaFDt2FZv",
        "outputId": "d7a812e0-7e53-4510-8c6a-c09dce79b6b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "loss = loss_fn(probs_2logits.squeeze(), target)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCfPtlam2FZv"
      },
      "source": [
        "Notamos que o valor foi o mesmo que o obtido acima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSnbKi-o2FZv"
      },
      "source": [
        "\n",
        "##### V.3.d) Modifique as seções V e VI para que o notebook funcione com a saída da rede com 2 logitos. Há necessidade de alterar o laço de treinamento e o laço de cálculo da acurácia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go5vMaim2FZv",
        "outputId": "f9e39eec-795c-4b95-c8c9-feb70728d18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5],               Loss: 0.5940\n",
            "Epoch [2/5],               Loss: 0.5067\n",
            "Epoch [3/5],               Loss: 0.4641\n",
            "Epoch [4/5],               Loss: 0.4820\n",
            "Epoch [5/5],               Loss: 0.4484\n",
            "\n",
            "Test Accuracy: 86.74%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "86.74"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Treinamento e inferência multi-classe\n",
        "\n",
        "def train_two_logits(model, lr):\n",
        "\n",
        "  model = model.to(device)\n",
        "  reset_weights(model)\n",
        "  # Define loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr)\n",
        "\n",
        "  # Training loop\n",
        "  num_epochs = 5\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "\n",
        "          if not preload_to_gpu:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs.squeeze(), labels)\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
        "              Loss: {loss.item():.4f}')\n",
        "  print()\n",
        "\n",
        "def eval_two_logits(model):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        acc = 100* correct/total\n",
        "        print(f'Test Accuracy: {acc}%')\n",
        "    return acc\n",
        "\n",
        "\n",
        "model = OneHotMLP_2logits(vocab_size)\n",
        "train_two_logits(model, best_LR)\n",
        "eval_two_logits(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIfL__5A2FZw"
      },
      "source": [
        "#### Seção VI\n",
        "\n",
        "##### VI.1.a) Calcule o número de amostras que está sendo considerado na seção de avaliação.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB8bpSx52FZw",
        "outputId": "ff401718-831b-45c2-e023-43a11d5672c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25000\n"
          ]
        }
      ],
      "source": [
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3csQiSm2FZw"
      },
      "source": [
        "##### VI.1.b) Explique o que faz os comandos model.eval()e with torch.no_grad().\n",
        "\n",
        "O comando model.eval() informa para o Pytorch que estamos em modo de inferência, o que faz com que algumas camadas dos modelos (como camadas de dropout) sejam desabilitadas.\n",
        "\n",
        "O loop **with torch.no_grad()** informa o Pytorch para não calcular gradientes relacionados a um tensor. Assim, loops onde o gradiente precisa ser preservado utilizam essa configuração.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXTpry3K2FZw"
      },
      "source": [
        "##### VI.1.c) Existe uma forma mais simples de calcular a classe predita na linha 11, sem a necessidade de usar a função torch.sigmoid?\n",
        "\n",
        "Torch.sigmoid() é uma função de ativação, para transformar uma entrada numérica em um número entre zero e um. Uma maneira muito simples de fazer a mesma coisa é dividir a entrada pelo valor máximo da entrada observada, além de, claro, utilizar outras funções de ativação alternativas (ReLU, etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3rtQMJu2FZx"
      },
      "source": [
        "##### VI.2.a) Utilizando a resposta do exercício V.1.a, que é a Loss teórica de um modelo aleatório de 2 classes, qual é o valor da perplexidade?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWePwvbf2FZx",
        "outputId": "2e956315-0278-4e4c-8bb8-140fff950ae3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5000)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.exp(torch.tensor(-0.69314))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KokUiD3P2FZx"
      },
      "source": [
        "A perplexidade neste caso nos retorna a probabilidade de distribuição das classes de 50%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guL4916P2FZx"
      },
      "source": [
        "##### VI.2.b) E se o modelo agora fosse para classificar a amostra em N classes, qual seria o valor da perplexidade para o caso aleatório?\n",
        "\n",
        "Para N classes, a perplexidade seria dada por 1/N.\n",
        "\n",
        "##### VI.2.c) Qual é o valor da perplexidade quando o modelo acerta todas as classes com 100% de probabilidade?\n",
        "\n",
        "Quando um modelo acerta 100% das previsões, a perplexidade é 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7fSc0CY2FZx"
      },
      "source": [
        "##### VI.3.a) Modifique o código da seção VI - Avaliação, para que além de calcular a acurácia, calcule também a perplexidade. lembrar que PPL = torch.exp(CE). Assim, será necessário calcular a entropia cruzada, como feito no laço de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdLQvZTn2FZy",
        "outputId": "79d31322-b406-4963-ae8d-c8220af5d29b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 86.74%                 Test Perplexity: 1.5994813442230225\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "86.74"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def eval_with_perplexity(model):\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #total_loss = 0\n",
        "    #total_labels = 0\n",
        "    perplexity = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            perplexity = torch.exp(loss)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        acc = 100* correct/total\n",
        "        print(f'Test Accuracy: {acc}% \\\n",
        "                Test Perplexity: {perplexity}')\n",
        "    return acc\n",
        "\n",
        "eval_with_perplexity(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6fgfDBG2FZy"
      },
      "source": [
        "##### VI.4.a) Modifique o laço de treinamento para incorporar também o cálculo da avaliação ao final de cada época. Aproveite para reportar também a perplexidade, tanto do treinamento como da avaliação (observe que será mais fácil de interpretar). Essa é a forma usual de se fazer o treinamento, monitorando se o modelo não entra em overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUnp2xY12FZy",
        "outputId": "58a183c3-6648-42e1-cb6d-43bb130a4b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 79.372%                 Test Perplexity: 1.8890479803085327\n",
            "Epoch [1/5],               Loss: 0.6000,               Train Perplexity: 1.822061538696289\n",
            "Test Accuracy: 83.416%                 Test Perplexity: 1.7140659093856812\n",
            "Epoch [2/5],               Loss: 0.5073,               Train Perplexity: 1.6608058214187622\n",
            "Test Accuracy: 85.052%                 Test Perplexity: 1.6764986515045166\n",
            "Epoch [3/5],               Loss: 0.4443,               Train Perplexity: 1.559381365776062\n",
            "Test Accuracy: 86.024%                 Test Perplexity: 1.583828091621399\n",
            "Epoch [4/5],               Loss: 0.4751,               Train Perplexity: 1.6081184148788452\n",
            "Test Accuracy: 86.616%                 Test Perplexity: 1.6186288595199585\n",
            "Epoch [5/5],               Loss: 0.4688,               Train Perplexity: 1.5980067253112793\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def train_and_eval(model, lr, epochs):\n",
        "\n",
        "  model = model.to(device)\n",
        "  reset_weights(model)\n",
        "  # Define loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr)\n",
        "\n",
        "  perplexity = 0\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "\n",
        "          if not preload_to_gpu:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs.squeeze(), labels)\n",
        "          perplexity = torch.exp(loss)\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      eval_with_perplexity(model)\n",
        "      model.train()\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{epochs}], \\\n",
        "              Loss: {loss.item():.4f}, \\\n",
        "              Train Perplexity: {perplexity}')\n",
        "  print()\n",
        "\n",
        "model = OneHotMLP_2logits(vocab_size)\n",
        "train_and_eval(model, best_LR, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpXeI-kr2FZy"
      },
      "source": [
        "##### Por fim, como o dataset tem muitas amostras, ele é demorado de entrar em overfitting. Para ficar mais evidente, diminua novamente o número de amostras do dataset de treino de 25 mil para 1 mil amostras e aumente o número de épocas para ilustrar o caso do overfitting, em que a perplexidade de treinamento continua caindo, porém a perplexidade no conjunto de teste começa a aumentar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmx_vi6t2FZz",
        "outputId": "3bdff538-4537-4728-a0e9-558a2a12daca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 100.0%                 Test Perplexity: 1.4652374982833862\n",
            "Epoch [1/100],               Loss: 0.3981,               Train Perplexity: 1.4890598058700562\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.402090311050415\n",
            "Epoch [2/100],               Loss: 0.3379,               Train Perplexity: 1.4019744396209717\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3873093128204346\n",
            "Epoch [3/100],               Loss: 0.3279,               Train Perplexity: 1.3881028890609741\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3811591863632202\n",
            "Epoch [4/100],               Loss: 0.3222,               Train Perplexity: 1.3801277875900269\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3778650760650635\n",
            "Epoch [5/100],               Loss: 0.3193,               Train Perplexity: 1.376153588294983\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3758258819580078\n",
            "Epoch [6/100],               Loss: 0.3194,               Train Perplexity: 1.376289963722229\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3744561672210693\n",
            "Epoch [7/100],               Loss: 0.3184,               Train Perplexity: 1.3749326467514038\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.373476505279541\n",
            "Epoch [8/100],               Loss: 0.3167,               Train Perplexity: 1.3725703954696655\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.372735619544983\n",
            "Epoch [9/100],               Loss: 0.3178,               Train Perplexity: 1.374079704284668\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3721671104431152\n",
            "Epoch [10/100],               Loss: 0.3151,               Train Perplexity: 1.370390772819519\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3717094659805298\n",
            "Epoch [11/100],               Loss: 0.3163,               Train Perplexity: 1.3721094131469727\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.371337890625\n",
            "Epoch [12/100],               Loss: 0.3155,               Train Perplexity: 1.37095308303833\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3710299730300903\n",
            "Epoch [13/100],               Loss: 0.3152,               Train Perplexity: 1.3705111742019653\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3707679510116577\n",
            "Epoch [14/100],               Loss: 0.3165,               Train Perplexity: 1.372287631034851\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3705463409423828\n",
            "Epoch [15/100],               Loss: 0.3147,               Train Perplexity: 1.3699136972427368\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3703547716140747\n",
            "Epoch [16/100],               Loss: 0.3148,               Train Perplexity: 1.3700511455535889\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.37018620967865\n",
            "Epoch [17/100],               Loss: 0.3158,               Train Perplexity: 1.371317744255066\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3700395822525024\n",
            "Epoch [18/100],               Loss: 0.3147,               Train Perplexity: 1.3699020147323608\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3699101209640503\n",
            "Epoch [19/100],               Loss: 0.3144,               Train Perplexity: 1.3694963455200195\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3697941303253174\n",
            "Epoch [20/100],               Loss: 0.3145,               Train Perplexity: 1.3696213960647583\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3696900606155396\n",
            "Epoch [21/100],               Loss: 0.3147,               Train Perplexity: 1.3698198795318604\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3695961236953735\n",
            "Epoch [22/100],               Loss: 0.3146,               Train Perplexity: 1.3697214126586914\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3695107698440552\n",
            "Epoch [23/100],               Loss: 0.3147,               Train Perplexity: 1.3699082136154175\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3694332838058472\n",
            "Epoch [24/100],               Loss: 0.3147,               Train Perplexity: 1.3698039054870605\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3693631887435913\n",
            "Epoch [25/100],               Loss: 0.3140,               Train Perplexity: 1.368842363357544\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3692984580993652\n",
            "Epoch [26/100],               Loss: 0.3144,               Train Perplexity: 1.3694206476211548\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3692387342453003\n",
            "Epoch [27/100],               Loss: 0.3146,               Train Perplexity: 1.3697483539581299\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3691834211349487\n",
            "Epoch [28/100],               Loss: 0.3147,               Train Perplexity: 1.3698594570159912\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3691322803497314\n",
            "Epoch [29/100],               Loss: 0.3146,               Train Perplexity: 1.369748592376709\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3690849542617798\n",
            "Epoch [30/100],               Loss: 0.3146,               Train Perplexity: 1.369724988937378\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3690409660339355\n",
            "Epoch [31/100],               Loss: 0.3139,               Train Perplexity: 1.3687975406646729\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368999719619751\n",
            "Epoch [32/100],               Loss: 0.3142,               Train Perplexity: 1.3691222667694092\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368961215019226\n",
            "Epoch [33/100],               Loss: 0.3141,               Train Perplexity: 1.3689675331115723\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3689253330230713\n",
            "Epoch [34/100],               Loss: 0.3143,               Train Perplexity: 1.3693097829818726\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368891716003418\n",
            "Epoch [35/100],               Loss: 0.3137,               Train Perplexity: 1.368466854095459\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368859887123108\n",
            "Epoch [36/100],               Loss: 0.3139,               Train Perplexity: 1.368798017501831\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3688302040100098\n",
            "Epoch [37/100],               Loss: 0.3138,               Train Perplexity: 1.3685476779937744\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3688018321990967\n",
            "Epoch [38/100],               Loss: 0.3140,               Train Perplexity: 1.3688825368881226\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3687752485275269\n",
            "Epoch [39/100],               Loss: 0.3136,               Train Perplexity: 1.3684091567993164\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3687498569488525\n",
            "Epoch [40/100],               Loss: 0.3138,               Train Perplexity: 1.3686186075210571\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3687258958816528\n",
            "Epoch [41/100],               Loss: 0.3139,               Train Perplexity: 1.3687593936920166\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368703007698059\n",
            "Epoch [42/100],               Loss: 0.3142,               Train Perplexity: 1.3691896200180054\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3686814308166504\n",
            "Epoch [43/100],               Loss: 0.3137,               Train Perplexity: 1.3684498071670532\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3686609268188477\n",
            "Epoch [44/100],               Loss: 0.3137,               Train Perplexity: 1.3684937953948975\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3686413764953613\n",
            "Epoch [45/100],               Loss: 0.3136,               Train Perplexity: 1.368397831916809\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3686225414276123\n",
            "Epoch [46/100],               Loss: 0.3138,               Train Perplexity: 1.3685566186904907\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3686045408248901\n",
            "Epoch [47/100],               Loss: 0.3143,               Train Perplexity: 1.3692359924316406\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3685874938964844\n",
            "Epoch [48/100],               Loss: 0.3137,               Train Perplexity: 1.368489146232605\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368571162223816\n",
            "Epoch [49/100],               Loss: 0.3139,               Train Perplexity: 1.3687890768051147\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3685554265975952\n",
            "Epoch [50/100],               Loss: 0.3138,               Train Perplexity: 1.3686046600341797\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3685404062271118\n",
            "Epoch [51/100],               Loss: 0.3139,               Train Perplexity: 1.368720293045044\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3685258626937866\n",
            "Epoch [52/100],               Loss: 0.3142,               Train Perplexity: 1.369149923324585\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3685119152069092\n",
            "Epoch [53/100],               Loss: 0.3136,               Train Perplexity: 1.368369698524475\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368498682975769\n",
            "Epoch [54/100],               Loss: 0.3136,               Train Perplexity: 1.3682812452316284\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3684860467910767\n",
            "Epoch [55/100],               Loss: 0.3137,               Train Perplexity: 1.368517518043518\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3684738874435425\n",
            "Epoch [56/100],               Loss: 0.3135,               Train Perplexity: 1.3682323694229126\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3684619665145874\n",
            "Epoch [57/100],               Loss: 0.3137,               Train Perplexity: 1.3685381412506104\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368450403213501\n",
            "Epoch [58/100],               Loss: 0.3142,               Train Perplexity: 1.369145154953003\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3684395551681519\n",
            "Epoch [59/100],               Loss: 0.3134,               Train Perplexity: 1.3680731058120728\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3684290647506714\n",
            "Epoch [60/100],               Loss: 0.3136,               Train Perplexity: 1.368316411972046\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3684189319610596\n",
            "Epoch [61/100],               Loss: 0.3136,               Train Perplexity: 1.368345022201538\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3684089183807373\n",
            "Epoch [62/100],               Loss: 0.3139,               Train Perplexity: 1.368705153465271\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3683993816375732\n",
            "Epoch [63/100],               Loss: 0.3140,               Train Perplexity: 1.3688586950302124\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3683900833129883\n",
            "Epoch [64/100],               Loss: 0.3143,               Train Perplexity: 1.3693068027496338\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368381142616272\n",
            "Epoch [65/100],               Loss: 0.3135,               Train Perplexity: 1.368220567703247\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3683725595474243\n",
            "Epoch [66/100],               Loss: 0.3136,               Train Perplexity: 1.3683326244354248\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3683642148971558\n",
            "Epoch [67/100],               Loss: 0.3135,               Train Perplexity: 1.3681540489196777\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3683559894561768\n",
            "Epoch [68/100],               Loss: 0.3139,               Train Perplexity: 1.3686999082565308\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368348240852356\n",
            "Epoch [69/100],               Loss: 0.3135,               Train Perplexity: 1.368147611618042\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3683404922485352\n",
            "Epoch [70/100],               Loss: 0.3139,               Train Perplexity: 1.3687909841537476\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3683332204818726\n",
            "Epoch [71/100],               Loss: 0.3139,               Train Perplexity: 1.3687148094177246\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.36832594871521\n",
            "Epoch [72/100],               Loss: 0.3136,               Train Perplexity: 1.3683280944824219\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3683189153671265\n",
            "Epoch [73/100],               Loss: 0.3139,               Train Perplexity: 1.3687971830368042\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368312120437622\n",
            "Epoch [74/100],               Loss: 0.3139,               Train Perplexity: 1.368705153465271\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3683055639266968\n",
            "Epoch [75/100],               Loss: 0.3135,               Train Perplexity: 1.3682001829147339\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682992458343506\n",
            "Epoch [76/100],               Loss: 0.3136,               Train Perplexity: 1.3683652877807617\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682929277420044\n",
            "Epoch [77/100],               Loss: 0.3135,               Train Perplexity: 1.3682271242141724\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682869672775269\n",
            "Epoch [78/100],               Loss: 0.3135,               Train Perplexity: 1.3681442737579346\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682812452316284\n",
            "Epoch [79/100],               Loss: 0.3135,               Train Perplexity: 1.3682386875152588\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682754039764404\n",
            "Epoch [80/100],               Loss: 0.3134,               Train Perplexity: 1.3681339025497437\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368269920349121\n",
            "Epoch [81/100],               Loss: 0.3138,               Train Perplexity: 1.3685861825942993\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682644367218018\n",
            "Epoch [82/100],               Loss: 0.3135,               Train Perplexity: 1.3681398630142212\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682591915130615\n",
            "Epoch [83/100],               Loss: 0.3135,               Train Perplexity: 1.368175745010376\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682540655136108\n",
            "Epoch [84/100],               Loss: 0.3135,               Train Perplexity: 1.368200421333313\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682489395141602\n",
            "Epoch [85/100],               Loss: 0.3137,               Train Perplexity: 1.3684122562408447\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682440519332886\n",
            "Epoch [86/100],               Loss: 0.3140,               Train Perplexity: 1.3688278198242188\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682392835617065\n",
            "Epoch [87/100],               Loss: 0.3135,               Train Perplexity: 1.3682599067687988\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682345151901245\n",
            "Epoch [88/100],               Loss: 0.3135,               Train Perplexity: 1.368152141571045\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682301044464111\n",
            "Epoch [89/100],               Loss: 0.3134,               Train Perplexity: 1.3681014776229858\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682255744934082\n",
            "Epoch [90/100],               Loss: 0.3136,               Train Perplexity: 1.3683029413223267\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682212829589844\n",
            "Epoch [91/100],               Loss: 0.3134,               Train Perplexity: 1.3680342435836792\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.36821711063385\n",
            "Epoch [92/100],               Loss: 0.3135,               Train Perplexity: 1.3682141304016113\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682128190994263\n",
            "Epoch [93/100],               Loss: 0.3138,               Train Perplexity: 1.368651270866394\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368208885192871\n",
            "Epoch [94/100],               Loss: 0.3135,               Train Perplexity: 1.3681857585906982\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682047128677368\n",
            "Epoch [95/100],               Loss: 0.3139,               Train Perplexity: 1.3687580823898315\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3682007789611816\n",
            "Epoch [96/100],               Loss: 0.3135,               Train Perplexity: 1.368249535560608\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.368196964263916\n",
            "Epoch [97/100],               Loss: 0.3138,               Train Perplexity: 1.3686814308166504\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.36819326877594\n",
            "Epoch [98/100],               Loss: 0.3138,               Train Perplexity: 1.3686493635177612\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3681894540786743\n",
            "Epoch [99/100],               Loss: 0.3136,               Train Perplexity: 1.3683782815933228\n",
            "Test Accuracy: 100.0%                 Test Perplexity: 1.3681858777999878\n",
            "Epoch [100/100],               Loss: 0.3142,               Train Perplexity: 1.3692086935043335\n",
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_data_short = IMDBDataset('train', vocab, samples = 1000)\n",
        "test_data_short = IMDBDataset('test', vocab, samples = 1000)\n",
        "\n",
        "train_loader = DataLoader(train_data_short, batch_size=batch_size, shuffle=train_shuffle)\n",
        "test_loader = DataLoader(test_data_short, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = OneHotMLP_2logits(vocab_size)\n",
        "train_and_eval(model, best_LR, 100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA024",
      "language": "python",
      "name": "ia024"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
