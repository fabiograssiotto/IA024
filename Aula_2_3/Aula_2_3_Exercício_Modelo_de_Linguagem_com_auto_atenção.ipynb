{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção\n",
        "\n",
        "Este exercício é similar ao da aula passada, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada.\n",
        "\n",
        "Na camada de auto-atenção, deve-se implementar (vide slide 34):\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "Instrucões:\n",
        "- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente, mas fácil de entender) e outra matricial (eficiente mas difícil de entender). Usar slide 36 como referência.\n",
        "\n",
        "- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.\n",
        "\n",
        "- No treinamento, usar apenas a implementação matricial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global variables\n",
        "\n",
        "# Vocabulary\n",
        "vocab_size = 3000\n",
        "context_size = 5\n",
        "pattern = r'\\w+|[,;.:!?\\']'\n",
        "\n",
        "# Training\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "\n",
        "# Model\n",
        "embedding_dim = 64\n",
        "hidden_dim = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "# Check if download is necessary\n",
        "if not os.path.exists(\"67724.txt.utf-8\"):\n",
        "    print(\"Downloading Gutenberg texts\")\n",
        "\n",
        "    !wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "    !wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4969"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = open(\"67724.txt.utf-8\",\"r\").read()\n",
        "text += open(\"67725.txt.utf-8\",\"r\").read()\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg eBook of O Guarany: romance brazileiro, Vol. 1 (of 2)\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n"
          ]
        }
      ],
      "source": [
        "# Checking the text\n",
        "print(paragraphs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " QUINTA EDIÇÃO\n",
            "\n",
            "Louca, perdida, hallucinada, ella ergueu-se, seu seio dilatou-se, e sua bocca, entreabrindo-se, collou-se aos labios frios e gelados de seu amante; era o seu primeiro e ultimo beijo; o seu beijo de noiva.\n",
            "\n",
            "--Conheces tão bem como eu, Ayres, o caracter desses selvagens; sabes que a sua paixão dominante é a vingança, e que por ella sacrificão tudo, a vida e a liberdade.\n",
            "\n",
            "Sahindo de sua cabana, Pery entrou no jardim: Cecilia estava sentada n'um tapete de pelles sobre a relva, e amimava ao seio a sua rolinha predilecta, offerecendo os labios de carmim ás caricias que a ave lhe fazia com o bico delicado.\n",
            "\n",
            "--Perdôo-te a offensa que me fizeste, amigo; porque é ainda uma prova de tua grande dedicação. Mas acredita-me; se fosse preciso que eu me votasse só ao sacrificio barbaro dos selvagens para salvar minha filha, eu o faria sorrindo.\n",
            "\n",
            "Number of paragraphs: 4892\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4892"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "# Print 5 random paragraphs\n",
        "num_paragraphs = len(cleaned_paragraphs)\n",
        "for i in range(0,5):\n",
        "    idx = random.randrange(num_paragraphs)\n",
        "    print(f\"{cleaned_paragraphs[idx]}\\n\")\n",
        "\n",
        "print(\"Number of paragraphs: \" + str(num_paragraphs))\n",
        "\n",
        "len(cleaned_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12610"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(pattern, text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Frequent Words: ['.', ',', 'a', 'que', 'o', 'de', 'e', 'se', ';', 'um']\n",
            "Vocabulary Size: 3000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Most Frequent Words: {most_frequent_words[:10]}\")\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever.\n",
            "[0, 146, 0, 23, 0, 2, 8, 50, 117, 276, 266, 2669, 13, 1071, 0, 2, 4, 193, 137, 287, 5, 2264, 8, 0, 3, 2672, 1]\n"
          ]
        }
      ],
      "source": [
        "#pattern = r'\\w+'\n",
        "\n",
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(pattern, sentence.lower())]\n",
        "\n",
        "print(cleaned_paragraphs[20])\n",
        "print(encode_sentence(cleaned_paragraphs[20], vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, paragraphs, vocab, context):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.vocab = vocab\n",
        "    self.context = context\n",
        "    self.tokens, self.targets = self.setup()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.tokens[idx]), torch.tensor(self.targets[idx])\n",
        "  \n",
        "  def setup(self):\n",
        "    tokens = []\n",
        "    targets = []\n",
        "    for paragraph in self.paragraphs:\n",
        "      encoded = encode_sentence(paragraph, self.vocab)\n",
        "      \n",
        "      # If paragraph is smaller than the context, skip it.\n",
        "      if len(encoded) < self.context + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(len(encoded) - self.context):\n",
        "        tks = encoded[i:i+self.context]\n",
        "        tgt = encoded[i+self.context]\n",
        "        # Only add if there are no unknown tokens in both context and target.\n",
        "        bad_token = 0\n",
        "        if not (bad_token in tks or tgt == bad_token):\n",
        "          tokens.append(tks)\n",
        "          targets.append(tgt)\n",
        "    return tokens, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 3913\n",
            "Validation samples: 979\n",
            "\n",
            "Training dataset samples: 44547\n",
            "Validation dataset samples: 12257\n"
          ]
        }
      ],
      "source": [
        "# Train/Validation split\n",
        "train_data, val_data = train_test_split(cleaned_paragraphs, test_size=0.2, random_state=18)\n",
        "\n",
        "train_dataset = CustomDataset(train_data, vocab, context_size)\n",
        "val_dataset = CustomDataset(val_data, vocab, context_size)\n",
        "\n",
        "# Counting all Samples\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print()\n",
        "print(f\"Training dataset samples: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[   5,  304,    2,    5,  186],\n",
            "        [ 476,  941,    2,    3,  219],\n",
            "        [ 234,    2,  564,    1,  268],\n",
            "        [  31,   17,  465,    3,  366],\n",
            "        [1557,   46,   24,  336,   15],\n",
            "        [1077,   11,  107,    6,  383],\n",
            "        [   7, 1502,    5,  493,    6],\n",
            "        [ 242,    6,   17,  321,    1],\n",
            "        [  16,  477,    2,    3,  778],\n",
            "        [   2,    7,   94,    4,  232],\n",
            "        [ 342,    6,    4,   17,   57],\n",
            "        [   2, 1096,   70,   10,   97],\n",
            "        [   2, 1256,    1,  140,   53],\n",
            "        [   1,    1,    1,    1,    1],\n",
            "        [ 112,    9,   13, 1788, 1124],\n",
            "        [  55,    7,   24,  162,    5],\n",
            "        [   2,  765,   11, 1982, 1932],\n",
            "        [ 289,  859,    2,    5,    4],\n",
            "        [   9,   15,  198,  169, 2034],\n",
            "        [2269,    7,   84,  773,    2],\n",
            "        [1666,    6,  946,    7,  710],\n",
            "        [1131,    2,    7,   60,  296],\n",
            "        [   5,  192,  320,   11,   43],\n",
            "        [1180,   13, 2134,    4,  701],\n",
            "        [  17,  410,   18,   15, 2297],\n",
            "        [ 297,    2,   23,  269,   11],\n",
            "        [ 154,    8,  424,   11,  227],\n",
            "        [  16,    5,  376,    6,   10],\n",
            "        [ 105,  696, 1534,    2,    4],\n",
            "        [   2,  956,  549,   10,  245],\n",
            "        [   3, 1688,    6,   17,  396],\n",
            "        [   6,   10,  608, 1193,    7]]), tensor([   9,    4,   44,    6, 2135,    1,    8,    8,    8,  820,   72,   38,\n",
            "         719,    1,  630,  972,    4, 1185,    1,    4,    1,  765,    2,   69,\n",
            "         110,  186,    2,  589,   32,    6,   28, 1023])]\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "sample = next(iter(train_loader))\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "class BengioModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(BengioModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.linear2 = nn.Linear(h, vocab_size+1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        # Flatten embeddings\n",
        "        embeds = embeds.view(embeds.size(0), -1)\n",
        "        # Linear layer with Relu activation\n",
        "        out = self.linear1(embeds)\n",
        "        out = F.relu(out)\n",
        "        # Second layer\n",
        "        out = self.linear2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "model = BengioModel(vocab_size, embedding_dim, context_size, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 5])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]\n",
        "\n",
        "print(input.shape)\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HGbJcT5KzJ_K"
      },
      "outputs": [],
      "source": [
        "output = model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um0lR4mNzJ_K",
        "outputId": "e6041da8-ca7f-4c9d-b28b-9e1250e820f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 701, 2352, 1552,  229, 2528, 1874, 1281, 2318, 1405, 2436,   54, 2824,\n",
              "          54, 2436,  995,   54, 1405, 1058, 1020, 1214,   12,  652, 1560,   54,\n",
              "         726, 1990, 2436, 2436, 2436, 1729, 2436, 2858])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la-b-f8jzJ_L",
        "outputId": "f040cef4-e409-4d20-d335-a3133aaeb63c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 452,    3,    7,  306,    1,    1,   14,    1,   28,   25,  232,    2,\n",
              "         337,  466,   24,   54, 1174,    7,  113,    1,    3,    2,   20,  318,\n",
              "          28,   65,  236,  421,   10,   27,  176,   72])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "a054092b-d801-4c60-eb75-85abfe57151d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BengioModel(\n",
              "  (embeddings): Embedding(3001, 64)\n",
              "  (linear1): Linear(in_features=320, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=3001, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O modelo tem um total de 620,281 parâmetros.\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Exemplo de uso:\n",
        "total_params = count_parameters(model)\n",
        "print(f'O modelo tem um total de {total_params:,} parâmetros.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Loss: 8.0498\n",
            "Initial Perplexity: 3133.0754\n"
          ]
        }
      ],
      "source": [
        "# Initial Perplexity and Loss\n",
        "# Before training\n",
        "model.eval()\n",
        "\n",
        "loss = 0\n",
        "perp = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "loss /= len(train_loader)\n",
        "perp = torch.exp(torch.tensor(loss))\n",
        "\n",
        "print(f'Initial Loss: {loss:.4f}')\n",
        "print(f'Initial Perplexity: {perp:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 6.1845, Accuracy: 0.10%, Perplexity: 485.1484\n",
            "Epoch [2/10], Loss: 5.7353, Accuracy: 0.10%, Perplexity: 309.6022\n",
            "Epoch [3/10], Loss: 5.6988, Accuracy: 0.10%, Perplexity: 298.5034\n",
            "Epoch [4/10], Loss: 5.6931, Accuracy: 0.10%, Perplexity: 296.8030\n",
            "Epoch [5/10], Loss: 5.6892, Accuracy: 0.10%, Perplexity: 295.6436\n",
            "Epoch [6/10], Loss: 5.7015, Accuracy: 0.10%, Perplexity: 299.3212\n",
            "Epoch [7/10], Loss: 5.6923, Accuracy: 0.11%, Perplexity: 296.5868\n",
            "Epoch [8/10], Loss: 5.7009, Accuracy: 0.11%, Perplexity: 299.1445\n",
            "Epoch [9/10], Loss: 5.7010, Accuracy: 0.10%, Perplexity: 299.1768\n",
            "Epoch [10/10], Loss: 5.6977, Accuracy: 0.11%, Perplexity: 298.1689\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Metrics\n",
        "  epoch_loss = 0\n",
        "  epoch_correct = 0\n",
        "  epoch_samples = 0\n",
        "\n",
        "  for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)  # Move input data to the device\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Predicted\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        epoch_correct += (predicted == targets).sum().item()\n",
        "        epoch_samples += targets.size(0)\n",
        "\n",
        "  # Calculate average loss and accuracy for epoch\n",
        "  avg_loss = epoch_loss / len(train_loader)\n",
        "  acc = epoch_correct / epoch_samples\n",
        "\n",
        "  # Perplexity\n",
        "  perp = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "  # Print epoch statistics\n",
        "  print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%, Perplexity: {perp:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "## Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nXXO78GSDqPg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 15.93%\n",
            "Average Loss: 5.72\n",
            "Average Perplexity: 306.33\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "loss_sum = 0\n",
        "total_sum = 0\n",
        "correct_sum = 0\n",
        "eval_round = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)        \n",
        "        loss_sum += loss\n",
        "\n",
        "        # Get the predicted labels\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total_sum += targets.size(0)\n",
        "        correct_sum += (predicted == targets).sum().item()\n",
        "        eval_round += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "acc = 100 * correct_sum/total_sum\n",
        "\n",
        "# Calculate average perplexity\n",
        "average_loss = loss_sum / eval_round\n",
        "average_perplexity = torch.exp(average_loss)\n",
        "\n",
        "print(f'Test Accuracy: {acc:.2f}%')\n",
        "print(f'Average Loss: {average_loss:.2f}')\n",
        "print(f'Average Perplexity: {average_perplexity:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frase: A moça vendo Alvaro a seus pés, supplicante, tinha-se tornado livida; seu coração batia com tanta violencia que via-se o peito de seu vestido elevar-se com as palpitações fortes e apressadas: o seu olhar ardente cahia sobre o moço e o fascinava.\n",
            "[3, 112, 291, 49, 3, 45, 242, 2, 910]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x576 and 320x128)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m context_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m\n\u001b[0;32m     40\u001b[0m max_length\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m---> 41\u001b[0m generate_text(model, vocab, text, max_length, context_size)\n",
            "Cell \u001b[1;32mIn[26], line 27\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(model, vocab, text, max_length, context_size)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[0;32m     26\u001b[0m     words_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(context[\u001b[38;5;241m-\u001b[39mcontext_size:], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 27\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(words_tensor)\n\u001b[0;32m     28\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32md:\\Development\\conda-envs\\IA024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Development\\conda-envs\\IA024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[14], line 13\u001b[0m, in \u001b[0;36mBengioModel.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     11\u001b[0m embeds \u001b[38;5;241m=\u001b[39m embeds\u001b[38;5;241m.\u001b[39mview(embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Linear layer with Relu activation\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(embeds)\n\u001b[0;32m     14\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Second layer\u001b[39;00m\n",
            "File \u001b[1;32md:\\Development\\conda-envs\\IA024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Development\\conda-envs\\IA024\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32md:\\Development\\conda-envs\\IA024\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x576 and 320x128)"
          ]
        }
      ],
      "source": [
        "# Código adaptado da implementação do Cesar Bastos\n",
        "from colorama import Fore, Style\n",
        "\n",
        "text = cleaned_paragraphs\n",
        "model.to(device)\n",
        "def generate_text(model, vocab, text, max_length, context_size):\n",
        "    words = []\n",
        "    # Ensure there are enough words for at least one sequence\n",
        "    while len(words) < context_size:\n",
        "        random_number = random.randint(1, 4891)\n",
        "        words = encode_sentence(text[random_number], vocab)\n",
        "        if not words:\n",
        "            words = []\n",
        "            continue  # Skip if the sentence cannot be encoded\n",
        "        words = words[:context_size]\n",
        "        #print(words)\n",
        "        if any(token == 0 for token in words):\n",
        "            words = []\n",
        "            continue  # Skip if any token is zero (assuming 0 is a special token)\n",
        "        context = words\n",
        "\n",
        "    print(f\"Frase: {cleaned_paragraphs[random_number]}\")\n",
        "    print(words)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        words_tensor = torch.tensor(context[-context_size:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        logits = model(words_tensor)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        context.append(next_token.item())\n",
        "        print(context)\n",
        "    frase = []\n",
        "    for i in context: ##Agradecimentos a Ramon Abilio\n",
        "        word = next((word for word, code in vocab.items() if code == i), \"<UNKNOWN>\")\n",
        "        frase.append(word)\n",
        "\n",
        "    print(f\"{Fore.BLUE}{frase[:context_size]}{Style.RESET_ALL} {Fore.RED}{frase[-max_length:]}{Style.RESET_ALL} \")\n",
        "\n",
        "context_size = 9\n",
        "max_length= 10\n",
        "generate_text(model, vocab, text, max_length, context_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA024",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
