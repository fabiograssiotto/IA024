{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção\n",
        "\n",
        "Este exercício é similar ao da aula passada, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada.\n",
        "\n",
        "Na camada de auto-atenção, deve-se implementar (vide slide 34):\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "Instrucões:\n",
        "- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente, mas fácil de entender) e outra matricial (eficiente mas difícil de entender). Usar slide 36 como referência.\n",
        "\n",
        "- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.\n",
        "\n",
        "- No treinamento, usar apenas a implementação matricial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variáveis Globais e Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global variables\n",
        "\n",
        "# Vocabulary\n",
        "vocab_size = 5000\n",
        "context_size = 5\n",
        "pattern = r'\\w+|[,;.:!?\\']'\n",
        "\n",
        "# Training\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "lr = 0.1\n",
        "\n",
        "# Model\n",
        "embedding_dim = 256\n",
        "hidden_dim = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if (IN_COLAB):\n",
        "    %pip install colorama\n",
        "\n",
        "    # Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    project_folder=\"/content/drive/MyDrive/Classes/IA024/Aula_2_3\"\n",
        "    os.chdir(project_folder)\n",
        "    !ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "# Check if download is necessary\n",
        "if not os.path.exists(\"67724.txt.utf-8\"):\n",
        "    print(\"Downloading Gutenberg texts\")\n",
        "\n",
        "    !wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "    !wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4969"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = open(\"67724.txt.utf-8\",\"r\").read()\n",
        "text += open(\"67725.txt.utf-8\",\"r\").read()\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg eBook of O Guarany: romance brazileiro, Vol. 1 (of 2)\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n"
          ]
        }
      ],
      "source": [
        "# Checking the text\n",
        "print(paragraphs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Agora sabeis tudo; o meu affecto vai de novo entrar no meu coração, donde nunca sahiria se não fosse a fatalidade que fez com que vos aproximasseis de mim, e me dirigisseis algumas palavras doces. A esperança para as almas que não a conhecêrão ainda, illude tanto e fascina, que devo merecer-vos desculpa. Esquecei-me, meu irmão, antes que lembrar-vos de mim para odiar-me!\n",
            "\n",
            "D. Antonio admirava-se que os selvagens, depois do ataque da manhã, se conservassem tranquillos no seu campo, e não tivessem investido a habitação uma só vez. Passou-lhe pelo espirito a idéa de que se tivessem retirado com a perda de alguns dos seus principaes guerreiros; mas elle conhecia de ha muito o espirito vingativo e tenacidade dessa raça para admittir semelhante supposição.\n",
            "\n",
            "E com o ouvido attento, com os labios entreabertos, o seio palpitante, ella esperava o som dessa voz querida e o echo dessa primeira e ultima palavra de seu triste amor.\n",
            "\n",
            "Seu vestido de lapim côr de fumo de cintura comprida, um pouco curto na frente, tinha uma cauda respeitavel, que ella arrastava com um certo donaire de fidalga, resto de sua belleza, ha muito perdida. Longas arrecadas de ouro com pingentes de esmeralda, que lhe roçavão quasi os hombros, e um collar com uma cruz de ouro ao pescoço, erão todos os seus ornatos.\n",
            "\n",
            "D. Antonio abraçou-a.\n",
            "\n",
            "Number of paragraphs: 4892\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4892"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "# Print 5 random paragraphs\n",
        "num_paragraphs = len(cleaned_paragraphs)\n",
        "for i in range(0,5):\n",
        "    idx = random.randrange(num_paragraphs)\n",
        "    print(f\"{cleaned_paragraphs[idx]}\\n\")\n",
        "\n",
        "print(\"Number of paragraphs: \" + str(num_paragraphs))\n",
        "\n",
        "len(cleaned_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12610"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(pattern, text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Frequent Words: ['.', ',', 'a', 'que', 'o', 'de', 'e', 'se', ';', 'um']\n",
            "Vocabulary Size: 5000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Most Frequent Words: {most_frequent_words[:10]}\")\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever.\n",
            "[0, 146, 4383, 23, 0, 2, 8, 50, 117, 276, 266, 2669, 13, 1071, 0, 2, 4, 193, 137, 287, 5, 2264, 8, 0, 3, 2672, 1]\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(pattern, sentence.lower())]\n",
        "\n",
        "print(cleaned_paragraphs[20])\n",
        "print(encode_sentence(cleaned_paragraphs[20], vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, paragraphs, vocab, context):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.vocab = vocab\n",
        "    self.context = context\n",
        "    self.tokens, self.targets = self.setup()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.tokens[idx]), torch.tensor(self.targets[idx])\n",
        "  \n",
        "  def setup(self):\n",
        "    tokens = []\n",
        "    targets = []\n",
        "    for paragraph in self.paragraphs:\n",
        "      encoded = encode_sentence(paragraph, self.vocab)\n",
        "      \n",
        "      # If paragraph is smaller than the context, skip it.\n",
        "      if len(encoded) < self.context + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(len(encoded) - self.context):\n",
        "        tks = encoded[i:i+self.context]\n",
        "        tgt = encoded[i+self.context]\n",
        "        # Only add if there are no unknown tokens in both context and target.\n",
        "        bad_token = 0\n",
        "        if not (bad_token in tks or tgt == bad_token):\n",
        "          tokens.append(tks)\n",
        "          targets.append(tgt)\n",
        "    return tokens, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 3913\n",
            "Validation samples: 979\n",
            "\n",
            "Training dataset samples: 59646\n",
            "Validation dataset samples: 16215\n"
          ]
        }
      ],
      "source": [
        "# Train/Validation split\n",
        "train_data, val_data = train_test_split(cleaned_paragraphs, test_size=0.2, random_state=18)\n",
        "\n",
        "train_dataset = CustomDataset(train_data, vocab, context_size)\n",
        "val_dataset = CustomDataset(val_data, vocab, context_size)\n",
        "\n",
        "# Counting all Samples\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print()\n",
        "print(f\"Training dataset samples: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[   2,  171,   35,    9,   12],\n",
            "        [ 762,    2,    7,  237,   82],\n",
            "        [   1,    1,    1,    1,    1],\n",
            "        [   5,   19,  430, 3108,    9],\n",
            "        [ 112,    2,    5,   43,  260],\n",
            "        [ 722,  264,    7,  505,    3],\n",
            "        [  47,    6,   73,    7, 3522],\n",
            "        [  32,  951,  726,    3, 1049],\n",
            "        [  75, 2888,   13,  621, 1272],\n",
            "        [  26,  235, 2020,  572,    4],\n",
            "        [ 263,  698,    2,  210,  242],\n",
            "        [   9,    3, 2815,   42, 2513],\n",
            "        [   3,  788, 2059,    2,  287],\n",
            "        [ 137,   32, 2156, 1506, 1371],\n",
            "        [   6,  174,    7,  696,    4],\n",
            "        [   2,  256,    6,  838,  182],\n",
            "        [   4,  106,  390,   33,   40],\n",
            "        [ 276,  297,  319,    7, 1534],\n",
            "        [ 766,    6,  493,    7, 1553],\n",
            "        [  89,  246,    5,  119,    7],\n",
            "        [   3,   21,    5,  380,    7],\n",
            "        [  55,   27,  207,   18, 1137],\n",
            "        [  13, 1146,    8,  863, 4123],\n",
            "        [ 559,  354,    8, 2462,   13],\n",
            "        [  13,  318, 4305,    2,  338],\n",
            "        [  12, 1617,   20,   28,  486],\n",
            "        [ 181,  134,   61,  193,  707],\n",
            "        [  30,   61,  325,    7,  207],\n",
            "        [   1,    5,   43,  975,  170],\n",
            "        [ 240,  894,   27,  304,  424],\n",
            "        [  77,    2,   37,    3,  685],\n",
            "        [2913,    4,   29, 3700,    5],\n",
            "        [ 159,  158,   95,  200,    3],\n",
            "        [  15,  578,   42,  836, 3534],\n",
            "        [ 168,  844,    4,    5, 1017],\n",
            "        [ 334, 1372,    2,   49,   32],\n",
            "        [ 107,    6,  383,    2,    5],\n",
            "        [ 137,  248,  137, 4286,    6],\n",
            "        [  20,    5,  742,    4,  127],\n",
            "        [ 683,   63,   45,  313, 3041],\n",
            "        [ 213,   25,    5,    4,  192],\n",
            "        [   8,    3,  143,  343,    5],\n",
            "        [ 280,  216,    6,   17,   57],\n",
            "        [   2,   49,  294,    3,   17],\n",
            "        [  91,   11,  475,    2,  424],\n",
            "        [  53,   41,  379, 1726,    2],\n",
            "        [  28,    5,    6,   10,   87],\n",
            "        [ 145,   23,  132,    3,   17],\n",
            "        [   2,   36,  786,   11,  439],\n",
            "        [2520,    7,  657,    1,  241],\n",
            "        [   9,  875,    6,    4,    3],\n",
            "        [1774,    6,   30,    1,   47],\n",
            "        [  56, 4094,    3,   83,    7],\n",
            "        [   4, 2944,   24,  304,    2],\n",
            "        [  42,   45,  212,    9,    3],\n",
            "        [   4,   12,  590,    2,    4],\n",
            "        [1318,    2,   39,   12,  756],\n",
            "        [ 353,   38,    3,  204,    2],\n",
            "        [ 681, 1413,    2, 4236,    2],\n",
            "        [ 866,    2,  965,  145,  461],\n",
            "        [ 486,    2,  195,   20,    1],\n",
            "        [  18, 1848,    3,  787,   42],\n",
            "        [   1,  883,  884,  491,   86],\n",
            "        [   5,    4,    8,   96,  394],\n",
            "        [  75, 1107,    8,   31,  149],\n",
            "        [  11,  209,  110,    3,  112],\n",
            "        [   8,   24,   19,  399,    2],\n",
            "        [   9,  679,    4,    5,   87],\n",
            "        [ 121,    3, 1302, 1031,   26],\n",
            "        [ 433,    6,  626,  100,  242],\n",
            "        [  58,    2, 4228, 4186,    7],\n",
            "        [  38,   22, 1173,   11,  107],\n",
            "        [   7, 1206,    9,   89,   54],\n",
            "        [  30,    1,   47,    6,   73],\n",
            "        [ 215, 3082,    2, 1298,   18],\n",
            "        [  50,   29,   16,   13,  153],\n",
            "        [  13, 2710, 1650, 2965,   59],\n",
            "        [ 872,   24,  475,   20,    1],\n",
            "        [  98,  865,  146,  207,    4],\n",
            "        [ 133,  575, 4129,    2,   39],\n",
            "        [ 861,   14,  236,    4,  203],\n",
            "        [  71,    8,  285,   84,    4],\n",
            "        [ 505,   15,   55,   27,  207],\n",
            "        [ 364,   11,  205,   67,  254],\n",
            "        [4770,   14,  213,    2, 1641],\n",
            "        [  51, 1151,    2,    5,  135],\n",
            "        [  12,    8,  965,  136,  492],\n",
            "        [  50,    5,   67,    3,  160],\n",
            "        [  10, 1231, 2586,   23, 1608],\n",
            "        [  13,  199,    2,   84,  763],\n",
            "        [  30,    1,   47,    6,   73],\n",
            "        [ 487,    9,   39,   35, 1295],\n",
            "        [ 203,  121,   24,   91,    6],\n",
            "        [3914,   23,  615,   26,   10],\n",
            "        [   8,  123,   13,  792, 2695],\n",
            "        [ 254,   70,    5, 2371, 2583],\n",
            "        [ 748,    2,    7,   33,  123],\n",
            "        [2256,   27,  456, 2730,   25],\n",
            "        [   1,    1,    1,    1,    1],\n",
            "        [ 618,    2,   50,    5,   80],\n",
            "        [ 796,  319,   20,  173,    3],\n",
            "        [  67,   33,   13, 1218,    4],\n",
            "        [ 905,    5, 1289,    7,  868],\n",
            "        [  19,  401,  625,    1,  930],\n",
            "        [2178,    2,  247,    4,   10],\n",
            "        [   7,    5,  227, 2145,   28],\n",
            "        [ 268,  220,    5,  408, 1274],\n",
            "        [   2,   48,   21,    2,    4],\n",
            "        [  24,   19,  399, 1083,    7],\n",
            "        [   9, 1440,   22,  817,   36],\n",
            "        [1499,  121,   28,   12, 1425],\n",
            "        [1121,   20,  323,  272, 2131],\n",
            "        [ 253, 2088,    4,    3, 1656],\n",
            "        [2068,    7, 1053,   42,  152],\n",
            "        [ 436,    9,  121,   28,    3],\n",
            "        [ 620,    6,  934,    3,   94],\n",
            "        [   2,    7,  237,    8,   18],\n",
            "        [1200,   51,  244,    2,    5],\n",
            "        [  16,   22,  168,  200,    3],\n",
            "        [2119,   33,    5,  339,   23],\n",
            "        [  68,    2,  679,   23,   64],\n",
            "        [  34,  275,    3,  109,    7],\n",
            "        [  42,  217,    9,  246,    4],\n",
            "        [   5,  328,    4, 1239,    5],\n",
            "        [  21, 3989,    3,   17,  580],\n",
            "        [   1,   70,  127,  111,   71],\n",
            "        [   3,  465,   11,  130,   32],\n",
            "        [ 193,   71,    8,  285,   84]]), tensor([2835,   79,    1,    7,  970,  109,    8,    7,    2, 2401,  103,   37,\n",
            "           3, 2536, 1150,  813,    1,    4,    4,    5,   22,    5,   27, 3317,\n",
            "           2,   46, 2549,   13, 2804,    9,  124,  328, 1844,    1,  657,  256,\n",
            "        2451, 2617,    6,    2,  647,  608,    2,  386,    2,   41,    4,  598,\n",
            "         510,  131,  303,    6,    3,  103,  633,  257,    5,  169,    7,    2,\n",
            "           1,  322,   41,    9,   14,    8, 4298,    4,  110,    6, 1796,    2,\n",
            "          33,  579,    3,  688, 1651,    1, 1504,  257,    3,    5,    2,   26,\n",
            "        2078,  866,    2,  189,    1,    5, 4233,    8,  710, 1263,  611,   14,\n",
            "        1995,   12, 3378,    2,   58,   12,    1,  917,  316,   10,    7, 1407,\n",
            "          70,  757,    5,   52,    2,    1,  950,  284,   15,  227, 2700,    4,\n",
            "           1,  519,  169,  407,   31,   24,   13,    4])]\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "sample = next(iter(train_loader))\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação das camadas de self-attention (Loop e Matricial)\n",
        "#### Positional Encoding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Positional Embedding - as described in \"Attention is All You Need\"\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_sequence, embedding_dim):\n",
        "        super().__init__()\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.positional_encoding = torch.zeros(max_sequence, embedding_dim, device=device)\n",
        "        position = torch.arange(0, max_sequence, device=device).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2, device=device) * (-math.log(10000.0) / embedding_dim))\n",
        "        self.positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.positional_encoding = self.positional_encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, seq_length, _ = x.size()\n",
        "        positional_encoding = self.positional_encoding[:, :seq_length, :]\n",
        "        positional_encoding = positional_encoding.to(x.device)\n",
        "        # Position encoding is added to the input embeddings.\n",
        "        return x + positional_encoding   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação em Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adaptado da implementação do Ramon, muito elegante e traduziu bem o slide em aula.\n",
        "\n",
        "class SelfAttention_Loop(nn.Module):\n",
        "  def __init__(self, WQ, WK, WV, WO):\n",
        "    super(SelfAttention_Loop, self).__init__()\n",
        "    self.WQ = WQ\n",
        "    self.WK = WK\n",
        "    self.WV = WV\n",
        "    self.WO = WO\n",
        "\n",
        "  def forward(self, seq):\n",
        "    E = []  \n",
        "    for Xq in seq:\n",
        "        q = self.WQ(Xq)\n",
        "        scores = []\n",
        "        for Xk in seq:\n",
        "            k = self.WK(Xk)\n",
        "            score = torch.dot(q, k.transpose(-1,0))\n",
        "            scores.append(score)\n",
        "\n",
        "        scores_tensor = torch.tensor(scores)  \n",
        "        probs         = scores_tensor.softmax(dim=-1)\n",
        "\n",
        "        e = 0\n",
        "        for xv, p in zip(seq, probs):\n",
        "            v = self.WV(xv)\n",
        "            e += v * p\n",
        "\n",
        "        e = self.WO(e)\n",
        "        E.append(e)\n",
        "\n",
        "    return torch.stack(E) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementação Matricial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix Implementation\n",
        "class SelfAttention_Matrix(nn.Module):\n",
        "  def __init__(self, embedding_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.WQ = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.WK = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.WV = nn.Linear(embedding_dim, embedding_dim)\n",
        "    self.WO = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Linear projections\n",
        "    Q = self.WQ(inputs)\n",
        "    K = self.WK(inputs)\n",
        "    V = self.WV(inputs)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    probs = F.softmax(scores, dim=-1)\n",
        "    new_embedding = torch.matmul(probs, V)\n",
        "    # Projection in WO\n",
        "    new_embedding = self.WO(new_embedding)\n",
        "    return new_embedding\n",
        "\n",
        "# For testing purposes only\n",
        "class SelfAttention_Matrix_tst(nn.Module):\n",
        "  def __init__(self, WQ, WK, WV, WO):\n",
        "    super(SelfAttention_Matrix_tst, self).__init__()\n",
        "    self.WQ = WQ\n",
        "    self.WK = WK\n",
        "    self.WV = WV\n",
        "    self.WO = WO\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Linear projections\n",
        "    Q = self.WQ(inputs)\n",
        "    K = self.WK(inputs)\n",
        "    V = self.WV(inputs)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    probs = F.softmax(scores, dim=-1)\n",
        "    new_embedding = torch.matmul(probs, V)\n",
        "    # Projection in WO\n",
        "    new_embedding = self.WO(new_embedding)\n",
        "    return new_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teste das Camadas de Atenção"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loop Embeds:\n",
            "tensor([[-0.4956, -0.3305, -0.4353,  0.1282, -0.4671,  0.0350,  0.4431, -0.1060,\n",
            "         -0.2229, -0.4506],\n",
            "        [-0.4742, -0.4740, -0.3511,  0.2415, -0.4355,  0.0392,  0.5719, -0.0477,\n",
            "         -0.1488, -0.4727],\n",
            "        [-0.4850, -0.3991, -0.3931,  0.1825, -0.4508,  0.0385,  0.5071, -0.0785,\n",
            "         -0.1871, -0.4587],\n",
            "        [-0.5073, -0.2770, -0.4758,  0.0848, -0.4796,  0.0327,  0.3908, -0.1384,\n",
            "         -0.2611, -0.4411],\n",
            "        [-0.4787, -0.4238, -0.3703,  0.2034, -0.4415,  0.0432,  0.5369, -0.0652,\n",
            "         -0.1692, -0.4570]], grad_fn=<StackBackward0>)\n",
            "Matrix Embeds:\n",
            "tensor([[-0.4956, -0.3305, -0.4353,  0.1282, -0.4671,  0.0350,  0.4431, -0.1060,\n",
            "         -0.2229, -0.4506],\n",
            "        [-0.4742, -0.4740, -0.3511,  0.2415, -0.4355,  0.0392,  0.5719, -0.0477,\n",
            "         -0.1488, -0.4727],\n",
            "        [-0.4850, -0.3991, -0.3931,  0.1825, -0.4508,  0.0385,  0.5071, -0.0785,\n",
            "         -0.1871, -0.4587],\n",
            "        [-0.5073, -0.2770, -0.4758,  0.0848, -0.4796,  0.0327,  0.3908, -0.1384,\n",
            "         -0.2611, -0.4411],\n",
            "        [-0.4787, -0.4238, -0.3703,  0.2034, -0.4415,  0.0432,  0.5369, -0.0652,\n",
            "         -0.1692, -0.4570]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loop and Matrix results are the same: True\n"
          ]
        }
      ],
      "source": [
        "# Test data\n",
        "tst_dim = 5\n",
        "tst_vocab = 10\n",
        "data = torch.randint(0, tst_vocab, (tst_dim,))\n",
        "embedding = nn.Embedding(tst_vocab, tst_dim)\n",
        "embeds = embedding(data)\n",
        "\n",
        "# Projections (need to be the same for this test)\n",
        "WQ = nn.Linear(tst_dim, tst_dim)\n",
        "WK = nn.Linear(tst_dim, tst_dim)\n",
        "WV = nn.Linear(tst_dim, tst_dim)\n",
        "WO = nn.Linear(tst_dim, tst_vocab)\n",
        "\n",
        "# Loop\n",
        "attn_loop = SelfAttention_Loop(WQ, WK, WV, WO)\n",
        "embeds_attn_loop = attn_loop(embeds)\n",
        "# Matrix\n",
        "attn_matrix = SelfAttention_Matrix_tst(WQ, WK, WV, WO)\n",
        "embeds_attn_matrix = attn_matrix(embeds)\n",
        "\n",
        "print(\"Loop Embeds:\")\n",
        "print(embeds_attn_loop)\n",
        "\n",
        "print(\"Matrix Embeds:\")\n",
        "print(embeds_attn_matrix)\n",
        "\n",
        "# Check results\n",
        "print()\n",
        "print(f'Loop and Matrix results are the same: {torch.allclose(embeds_attn_loop, embeds_attn_matrix)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementação dos Modelos (Com e sem atenção, e com embeddings de posição)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BengioModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(BengioModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear2 = nn.Linear(h, vocab_size+1)\n",
        "        # Softmax to scale outputs\n",
        "        self.logSoftMax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        # Flatten embeddings\n",
        "        embeds = embeds.view(embeds.size(0), -1)\n",
        "        # Linear layer\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.relu(out)\n",
        "        # Second layer\n",
        "        out = self.linear2(out)\n",
        "        # Softmax output\n",
        "        out = self.logSoftMax(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BengioModel_SelfAttentionMatrix(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(BengioModel_SelfAttentionMatrix, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
        "        self.attention = SelfAttention_Matrix(embedding_dim)        \n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear2 = nn.Linear(h, vocab_size+1)\n",
        "        # Softmax to scale outputs\n",
        "        self.logSoftMax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        x = torch.stack(torch.unbind(embeds, dim=1), dim=1)\n",
        "        # Camada de autoatenção\n",
        "        attention  = self.attention(x)\n",
        "        # Flatten embeddings\n",
        "        embeds = embeds.view(attention.size(0), -1)\n",
        "        # Linear layer\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.relu(out)\n",
        "        # Second layer\n",
        "        out = self.linear2(out)\n",
        "        # Softmax output\n",
        "        out = self.logSoftMax(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "class BengioModel_SelfAttentionMatrix_PosEncoding(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(BengioModel_SelfAttentionMatrix_PosEncoding, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
        "        self.posencoding = PositionalEncoding(context_size, embedding_dim)\n",
        "        self.attention = SelfAttention_Matrix(embedding_dim)        \n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear2 = nn.Linear(h, vocab_size+1)\n",
        "        # Softmax to scale outputs\n",
        "        self.logSoftMax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        embeds_pos = self.posencoding(embeds)\n",
        "        x = torch.stack(torch.unbind(embeds_pos, dim=1), dim=1)\n",
        "        # Camada de autoatenção\n",
        "        attention  = self.attention(x)\n",
        "        # Flatten embeddings\n",
        "        embeds = embeds.view(attention.size(0), -1)\n",
        "        # Linear layer\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.relu(out)\n",
        "        # Second layer\n",
        "        out = self.linear2(out)\n",
        "        # Softmax output\n",
        "        out = self.logSoftMax(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funções de Treinamento e Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'The model has a total of {total_params:,} parameters.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initial_eval(model):\n",
        "    # Initial Perplexity and Loss\n",
        "    # Before training\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss += criterion(outputs, targets).item()\n",
        "\n",
        "    loss /= len(train_loader)\n",
        "    perp = torch.exp(torch.tensor(loss))\n",
        "\n",
        "    print(f'Initial Loss: {loss:.4f}')\n",
        "    print(f'Initial Perplexity: {perp:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer):\n",
        "      # Training Loop\n",
        "      model.train()\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "            epoch_start = time.time()\n",
        "            # Metrics\n",
        "            epoch_loss = 0\n",
        "            epoch_correct = 0\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for inputs, targets in train_loader:\n",
        "                  inputs = inputs.to(device)  # Move input data to the device\n",
        "                  targets = targets.to(device)\n",
        "\n",
        "                  # Forward pass\n",
        "                  outputs = model(inputs)\n",
        "                  loss = criterion(outputs, targets)\n",
        "\n",
        "                  # Backward pass and optimization\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "\n",
        "                  # Loss\n",
        "                  epoch_loss += loss.item()\n",
        "\n",
        "                  # Predicted\n",
        "                  _, predicted = torch.max(outputs, 1)\n",
        "                  epoch_correct += (predicted == targets).sum().item()\n",
        "                  epoch_samples += targets.size(0)\n",
        "\n",
        "            # Calculate average loss and accuracy for epoch\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            acc = epoch_correct / epoch_samples\n",
        "\n",
        "            # Perplexity\n",
        "            perp = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "            epoch_end = time.time()\n",
        "            epoch_time = epoch_end - epoch_start\n",
        "            # Print epoch statistics\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Time:{epoch_time:.2f}, Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%, Perplexity: {perp:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nXXO78GSDqPg"
      },
      "outputs": [],
      "source": [
        "def eval(model, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    loss_sum = 0\n",
        "    total_sum = 0\n",
        "    correct_sum = 0\n",
        "    eval_round = 0\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)      \n",
        "            loss_sum += loss\n",
        "\n",
        "            # Get the predicted labels\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total_sum += targets.size(0)\n",
        "            correct_sum += (predicted == targets).sum().item()\n",
        "            eval_round += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = 100 * correct_sum / total_sum\n",
        "\n",
        "    # Calculate average perplexity\n",
        "    average_loss = loss_sum / len(val_loader)\n",
        "    average_perplexity = torch.exp(average_loss)\n",
        "\n",
        "    print(f'Test Accuracy: {acc:.2f}%')\n",
        "    print(f'Average Loss: {average_loss:.2f}')\n",
        "    print(f'Average Perplexity: {average_perplexity:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Avaliação dos Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 1. Sem camada de atenção"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model without Self Attention:\n",
            "\n",
            "The model has a total of 2,089,353 parameters.\n",
            "\n",
            "Training Start\n",
            "\n",
            "Epoch [1/10], Time:6.26, Loss: 6.4169, Accuracy: 0.11%, Perplexity: 612.1053\n",
            "Epoch [2/10], Time:2.73, Loss: 5.5622, Accuracy: 0.15%, Perplexity: 260.4016\n",
            "Epoch [3/10], Time:2.76, Loss: 5.1807, Accuracy: 0.18%, Perplexity: 177.8030\n",
            "Epoch [4/10], Time:2.70, Loss: 4.8932, Accuracy: 0.20%, Perplexity: 133.3847\n",
            "Epoch [5/10], Time:2.70, Loss: 4.6501, Accuracy: 0.22%, Perplexity: 104.5990\n",
            "Epoch [6/10], Time:3.06, Loss: 4.4325, Accuracy: 0.24%, Perplexity: 84.1414\n",
            "Epoch [7/10], Time:2.98, Loss: 4.2320, Accuracy: 0.26%, Perplexity: 68.8520\n",
            "Epoch [8/10], Time:2.85, Loss: 4.0425, Accuracy: 0.28%, Perplexity: 56.9679\n",
            "Epoch [9/10], Time:2.89, Loss: 3.8613, Accuracy: 0.30%, Perplexity: 47.5288\n",
            "Epoch [10/10], Time:3.00, Loss: 3.6897, Accuracy: 0.32%, Perplexity: 40.0310\n",
            "\n",
            "Evaluation Start\n",
            "\n",
            "Test Accuracy: 19.20%\n",
            "Average Loss: 5.33\n",
            "Average Perplexity: 206.12\n"
          ]
        }
      ],
      "source": [
        "model = BengioModel(vocab_size, embedding_dim, context_size, hidden_dim)\n",
        "print(\"Model without Self Attention:\")\n",
        "print()\n",
        "count_parameters(model)\n",
        "\n",
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print()\n",
        "print(\"Training Start\")\n",
        "print()\n",
        "train(model, criterion, optimizer)\n",
        "\n",
        "print()\n",
        "print(\"Evaluation Start\")\n",
        "print()\n",
        "eval(model, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Com camada de atenção"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model with Self Attention:\n",
            "\n",
            "The model has a total of 3,571,729 parameters.\n",
            "\n",
            "Training Start\n",
            "\n",
            "Epoch [1/10], Time:3.35, Loss: 6.4260, Accuracy: 0.11%, Perplexity: 617.6747\n",
            "Epoch [2/10], Time:3.30, Loss: 5.5715, Accuracy: 0.15%, Perplexity: 262.8355\n",
            "Epoch [3/10], Time:3.26, Loss: 5.1889, Accuracy: 0.18%, Perplexity: 179.2775\n",
            "Epoch [4/10], Time:3.31, Loss: 4.9006, Accuracy: 0.20%, Perplexity: 134.3738\n",
            "Epoch [5/10], Time:3.29, Loss: 4.6556, Accuracy: 0.22%, Perplexity: 105.1771\n",
            "Epoch [6/10], Time:3.30, Loss: 4.4408, Accuracy: 0.24%, Perplexity: 84.8386\n",
            "Epoch [7/10], Time:3.45, Loss: 4.2377, Accuracy: 0.26%, Perplexity: 69.2466\n",
            "Epoch [8/10], Time:3.23, Loss: 4.0455, Accuracy: 0.28%, Perplexity: 57.1412\n",
            "Epoch [9/10], Time:3.27, Loss: 3.8643, Accuracy: 0.30%, Perplexity: 47.6678\n",
            "Epoch [10/10], Time:3.11, Loss: 3.6902, Accuracy: 0.32%, Perplexity: 40.0544\n",
            "\n",
            "Evaluation Start\n",
            "\n",
            "Test Accuracy: 19.78%\n",
            "Average Loss: 5.29\n",
            "Average Perplexity: 198.64\n"
          ]
        }
      ],
      "source": [
        "model_attn = BengioModel_SelfAttentionMatrix(vocab_size, embedding_dim, context_size, hidden_dim)\n",
        "print(\"Model with Self Attention:\")\n",
        "print()\n",
        "count_parameters(model_attn)\n",
        "\n",
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model_attn.parameters(), lr)\n",
        "\n",
        "model_attn.to(device)\n",
        "\n",
        "print()\n",
        "print(\"Training Start\")\n",
        "print()\n",
        "train(model_attn, criterion, optimizer)\n",
        "\n",
        "print()\n",
        "print(\"Evaluation Start\")\n",
        "print()\n",
        "eval(model_attn, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Com camada de atenção e Embeddings Posicionais\n",
        "#### Descrição do Modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model with Self Attention and Positional Encodings:\n",
            "\n",
            "The model has a total of 3,571,729 parameters.\n",
            "\n",
            "Model:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BengioModel_SelfAttentionMatrix_PosEncoding(\n",
              "  (embeddings): Embedding(5001, 256)\n",
              "  (posencoding): PositionalEncoding()\n",
              "  (attention): SelfAttention_Matrix(\n",
              "    (WQ): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (WK): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (WV): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (WO): Linear(in_features=256, out_features=5000, bias=True)\n",
              "  )\n",
              "  (linear1): Linear(in_features=1280, out_features=128, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (linear2): Linear(in_features=128, out_features=5001, bias=True)\n",
              "  (logSoftMax): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_attn_pos = BengioModel_SelfAttentionMatrix_PosEncoding(vocab_size, embedding_dim, context_size, hidden_dim)\n",
        "print(\"Model with Self Attention and Positional Encodings:\")\n",
        "print()\n",
        "count_parameters(model_attn_pos)\n",
        "print()\n",
        "print(\"Model:\")\n",
        "model_attn_pos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Perplexidade Inicial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Initial Loss: 8.5433\n",
            "Initial Perplexity: 5132.2856\n"
          ]
        }
      ],
      "source": [
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model_attn_pos.parameters(), lr)\n",
        "\n",
        "model_attn_pos.to(device)\n",
        "print()\n",
        "initial_eval(model_attn_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Treinamento e Avaliação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Start\n",
            "\n",
            "Epoch [1/10], Time:3.69, Loss: 6.4514, Accuracy: 0.11%, Perplexity: 633.5771\n",
            "Epoch [2/10], Time:3.31, Loss: 5.5795, Accuracy: 0.15%, Perplexity: 264.9455\n",
            "Epoch [3/10], Time:3.31, Loss: 5.1947, Accuracy: 0.18%, Perplexity: 180.3217\n",
            "Epoch [4/10], Time:3.52, Loss: 4.9035, Accuracy: 0.20%, Perplexity: 134.7610\n",
            "Epoch [5/10], Time:3.49, Loss: 4.6591, Accuracy: 0.22%, Perplexity: 105.5382\n",
            "Epoch [6/10], Time:3.56, Loss: 4.4416, Accuracy: 0.24%, Perplexity: 84.9146\n",
            "Epoch [7/10], Time:3.30, Loss: 4.2390, Accuracy: 0.26%, Perplexity: 69.3410\n",
            "Epoch [8/10], Time:3.22, Loss: 4.0496, Accuracy: 0.28%, Perplexity: 57.3754\n",
            "Epoch [9/10], Time:3.27, Loss: 3.8674, Accuracy: 0.30%, Perplexity: 47.8200\n",
            "Epoch [10/10], Time:3.24, Loss: 3.6952, Accuracy: 0.32%, Perplexity: 40.2549\n",
            "\n",
            "Evaluation Start\n",
            "\n",
            "Test Accuracy: 19.53%\n",
            "Average Loss: 5.32\n",
            "Average Perplexity: 203.59\n"
          ]
        }
      ],
      "source": [
        "print()\n",
        "print(\"Training Start\")\n",
        "print()\n",
        "train(model_attn_pos, criterion, optimizer)\n",
        "\n",
        "print()\n",
        "print(\"Evaluation Start\")\n",
        "print()\n",
        "eval(model_attn_pos, criterion)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA024",
      "language": "python",
      "name": "ia024"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
