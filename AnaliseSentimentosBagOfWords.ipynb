{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4HuVIYGgXg"
      },
      "source": [
        "# Notebook Processo Seletivo Aluno Especial IA-024 1S2024 FEEC-UNICAMP\n",
        "versão 5 de fevereiro de 2024, 19h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA5BWLDCKmw3",
        "outputId": "d393f695-7da4-46d6-8d34-9b44c12e9e59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (0.17.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torchtext) (4.66.1)\n",
            "Requirement already satisfied: requests in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.0 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torchtext) (2.2.0+cu118)\n",
            "Requirement already satisfied: numpy in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torchtext) (1.26.3)\n",
            "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torch==2.2.0->torchtext) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torch==2.2.0->torchtext) (4.8.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torch==2.2.0->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torch==2.2.0->torchtext) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torch==2.2.0->torchtext) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torch==2.2.0->torchtext) (2023.4.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from requests->torchtext) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from requests->torchtext) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from jinja2->torch==2.2.0->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\fabio.grassiotto\\research\\venv\\ml\\lib\\site-packages (from sympy->torch==2.2.0->torchtext) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: \"'portalocker\"\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext\n",
        "!pip install 'portalocker>=2.0.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VorDvF62iyXF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.datasets import IMDB\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ovJE02CwKT"
      },
      "source": [
        "## I - Vocabulário e Tokenização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqzUqy3diz0X",
        "outputId": "16424773-5f91-40a2-847d-36a3a8eb6a79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amostras positivas, negativas e totais:\n",
            "Counter({'neg': 2500, 'total': 2500, 'pos': 0})\n",
            "\n",
            "Comprimento médio do texto em palavras\n",
            "263.7684\n",
            "\n",
            "Cinco palavras mais frequentes:\n",
            "['the', 'a', 'and', 'of', 'to']\n",
            "\n",
            "Cinco palavras menos frequentes:\n",
            "['concealed', 'Estella', 'Daena', 'plans,', 'technological']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# limit the vocabulary size to 20000 most frequent tokens\n",
        "vocab_size = 20000\n",
        "\n",
        "#n_samples = 200\n",
        "n_samples = 2500\n",
        "\n",
        "# I.1. Na célula de calcular o vocabulário, aproveite o laço sobre IMDB de treinamento e utilize um segundo contador\n",
        "# para calcular o número de amostras positivas e amostras negativas.\n",
        "# Calcule também o comprimento médio do texto em número de palavras dos textos das amostras.\n",
        "\n",
        "counter = Counter()\n",
        "counter_lbl = Counter({\"pos\": 0, \"neg\": 0, \"total\": 0})\n",
        "total_review_len = 0\n",
        "avg_review_len = 0\n",
        "\n",
        "for (label, line) in list(IMDB(split='train'))[:n_samples]:\n",
        "    counter.update(line.split())\n",
        "\n",
        "    # Número de amostras positivas e negativas\n",
        "    if (label == 1):\n",
        "      counter_lbl['neg'] += 1\n",
        "    else:\n",
        "      counter_lbl['pos'] += 1\n",
        "    counter_lbl['total'] += 1\n",
        "\n",
        "    # Comprimento médio do texto das reviews em palavras\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "    # tokenize the sentence\n",
        "    tokens = tokenizer(line)\n",
        "\n",
        "    # count the number of words\n",
        "    total_review_len += len(tokens)\n",
        "\n",
        "# Comprimento médio\n",
        "avg_review_len = total_review_len / counter_lbl['total']\n",
        "\n",
        "# I.2 Mostre as cinco palavras mais frequentes do vocabulário e as cinco palavras menos frequentes.\n",
        "# Qual é o código do token que está sendo utilizado quando a palavra não está no vocabulário?\n",
        "# Calcule quantos tokens das frases do conjunto de treinamento que não estão no vocabulário.\n",
        "\n",
        "# create a vocabulary of the 20000 most frequent tokens\n",
        "most_frequent_words = sorted(counter, key=counter.get, reverse=True)[:vocab_size]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)} # words indexed from 1 to 20000\n",
        "vocab_size = len(vocab) #Errata\n",
        "\n",
        "print(\"Amostras positivas, negativas e totais:\")\n",
        "print(counter_lbl)\n",
        "print()\n",
        "\n",
        "print(\"Comprimento médio do texto em palavras\")\n",
        "print(avg_review_len)\n",
        "print()\n",
        "\n",
        "print(\"Cinco palavras mais frequentes:\")\n",
        "print(most_frequent_words[:5])\n",
        "print()\n",
        "\n",
        "print(\"Cinco palavras menos frequentes:\")\n",
        "print(most_frequent_words[-5:])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rZn-m1Mi110",
        "outputId": "6968e8cd-b054-4489-dfd6-2fa2384d2e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de tokens que não estão no vocabulário na base de treinamento:\n",
            "43515\n"
          ]
        }
      ],
      "source": [
        "# I.2 Calcule quantos tokens das frases do conjunto de treinamento que não estão no vocabulário.\n",
        "\n",
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in sentence.split()] # 0 for OOV\n",
        "\n",
        "encode_sentence(\"I like Pizza\", vocab)\n",
        "\n",
        "# Cálculo do número de tokens que não estão no vocabulário na base de treinamento:\n",
        "tokens = []\n",
        "for (label, line) in list(IMDB(split='train'))[:n_samples]:\n",
        "  tokens.extend(encode_sentence(line, vocab))\n",
        "\n",
        "print(\"Número de tokens que não estão no vocabulário na base de treinamento:\")\n",
        "print(tokens.count(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7tJ5XHQS7g6"
      },
      "source": [
        "#### I.2 Qual é o código do token que está sendo utilizado quando a palavra não está no vocabulário?\n",
        "\n",
        "Na função de dicionário dict.get() o segundo parâmetro indica o valor default caso a palavra não seja encontrada no dicionário. Nesse caso o código do token usado é o número zero.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri9lyeL_zx-1"
      },
      "source": [
        "#### I.3.a) Qual é a razão pela qual o modelo preditivo conseguiu acertar 100% das amostras de teste do dataset selecionado com apenas as primeiras 200 amostras?\n",
        "\n",
        "Ao reduzirmos a base de treinamento para apenas 200 amostras, a base se tornou totalmente desbalanceada. Como pudemos verificar, temos 200 amostras classificadas como negativas e nenhuma como positiva.\n",
        "Portanto a taxa de acurácia calculada sobre a classificação da base de testes depende unicamente da percentagem de amostras positivas ou negativas nesta base.\n",
        "\n",
        "#### I.3.b) Modifique a forma de selecionar 200 amostras do dataset, porém garantindo que ele continue balanceado, isto é, aproximadamente 100 amostras positivas e 100 amostras negativas.\n",
        "\n",
        "Para obtermos um dataset balanceado, usaremos uma função que seleciona amostras do dataset de acordo com a classificação e cria um dataset com a quantidade de amostras de cada classificação desejada conforme abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwYM7hAGBCeQ",
        "outputId": "fd5eb4cb-fd80-4f97-8dbc-6494fbc6c16f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprimento médio do texto em palavras na base balanceada\n",
            "267.2208\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Função para selecionar dados balanceados\n",
        "\n",
        "def balanced_dataset(data, size):\n",
        "  data_pos = [(label,line) for label, line in data if label == 2][:int(size/2)]\n",
        "  data_neg = [(label,line) for label, line in data if label == 1][:int(size/2)]\n",
        "\n",
        "  data_bal = data_pos + data_neg\n",
        "\n",
        "  return data_bal\n",
        "\n",
        "# Aplicando sobre a base de treinamento\n",
        "\n",
        "train_data = IMDB(split='train')\n",
        "counter = Counter()\n",
        "total_review_len = 0\n",
        "avg_review_len = 0\n",
        "\n",
        "for (label, line) in list(balanced_dataset(train_data, n_samples)):\n",
        "    counter.update(line.split())\n",
        "\n",
        "    # Comprimento médio do texto das reviews em palavras\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "    # tokenize the sentence\n",
        "    tokens = tokenizer(line)\n",
        "\n",
        "    # count the number of words\n",
        "    total_review_len += len(tokens)\n",
        "\n",
        "# Comprimento médio\n",
        "avg_review_len = total_review_len / n_samples\n",
        "\n",
        "print(\"Comprimento médio do texto em palavras na base balanceada\")\n",
        "print(avg_review_len)\n",
        "print()\n",
        "\n",
        "# create a vocabulary of the 20000 most frequent tokens\n",
        "#most_frequent_words = sorted(counter, key=counter.get, reverse=True)[:vocab_size]\n",
        "#vocab = {word: i for i, word in enumerate(most_frequent_words, 1)} # words indexed from 1 to 20000\n",
        "#vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iV4bF8cDAj1"
      },
      "source": [
        "## II - Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDUyZoTPi262",
        "outputId": "359eee71-f441-4b39-c93e-b2cb89d68881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amostras positivas, negativas e totais:\n",
            "Counter({'total': 2500, 'pos': 1250, 'neg': 1250})\n",
            "\n",
            "Quantidade média de palavras codificadas em cada vetor one-hot\n",
            "131.6824\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.functional import one_hot\n",
        "# Dataset Class with One-hot Encoding\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, split, vocab):\n",
        "        #self.data = list(IMDB(split=split))[:n_samples]\n",
        "        self.data = list(balanced_dataset(IMDB(split=split), n_samples))        \n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, line = self.data[idx]\n",
        "        label = 1 if label == 1 else 0\n",
        "\n",
        "        # one-hot encoding\n",
        "        X = torch.zeros(len(self.vocab) + 1)\n",
        "        for word in encode_sentence(line, self.vocab):\n",
        "            X[word] = 1\n",
        "\n",
        "        return X, torch.tensor(label)\n",
        "\n",
        "# Load Data with One-hot Encoding\n",
        "train_data = IMDBDataset('train', vocab)\n",
        "test_data = IMDBDataset('test', vocab)\n",
        "\n",
        "# II.1.a) Investigue o dataset criado na linha 24. Faça um código que aplique um laço sobre o dataset train_data\n",
        "# e calcule novamente quantas amostras positivas e negativas do dataset.\n",
        "# II.1.b) Calcule também o número médio de palavras codificadas em cada vetor one-hot.\n",
        "# Compare este valor com o comprimento médio de cada texto (contado em palavras), conforme calculado no exercício\n",
        "\n",
        "counter_lbl = Counter({\"pos\": 0, \"neg\": 0, \"total\": 0})\n",
        "words_encoded = 0\n",
        "for (oneHot, sentiment) in train_data:\n",
        "\n",
        "    words = oneHot.tolist()\n",
        "    label = sentiment.item()\n",
        "\n",
        "    # Número de amostras positivas e negativas\n",
        "    if (label == 1):\n",
        "      counter_lbl['neg'] += 1\n",
        "    else:\n",
        "      counter_lbl['pos'] += 1\n",
        "    counter_lbl['total'] += 1\n",
        "\n",
        "    hot_encoded = sum(words[i] for i in range(len(words)) if words[i] != 0)\n",
        "    words_encoded +=  hot_encoded\n",
        "\n",
        "avg_words_enc = words_encoded / counter_lbl['total']\n",
        "\n",
        "print(\"Amostras positivas, negativas e totais:\")\n",
        "print(counter_lbl)\n",
        "print()\n",
        "\n",
        "print(\"Quantidade média de palavras codificadas em cada vetor one-hot\")\n",
        "print(avg_words_enc)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdM6ojduRqrw"
      },
      "source": [
        "#### II.1.b Compare este valor com o comprimento médio de cada texto (contado em palavras), conforme calculado no exercício I.1.c. e explique a diferença.\n",
        "\n",
        "No exercício I.1.c, o comprimento médio do texto em palavras depois de passar pelo tokenizador foi de cerca de 265 palavras. Essa diferença do vetor One-Hot se deve ao fato que o vetor one-hot só codifica as palavras que foram identificadas no dicionário, enquanto que o comprimento médio considera todas as palavras das sentenças. Ou seja, palavras que não foram codificadas no dicionário serão representadas por zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7RMPSvMDL5U"
      },
      "source": [
        "## III - Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y7tcZv2YDIog"
      },
      "outputs": [],
      "source": [
        "#batch_size = 128\n",
        "batch_size = 256\n",
        "# define dataloaders\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwPeJ7h8DahT"
      },
      "source": [
        "## IV - Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6QuDhWvji7lt"
      },
      "outputs": [],
      "source": [
        "class OneHotMLP(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(OneHotMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(vocab_size+1, 200)\n",
        "        self.fc2 = nn.Linear(200, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.fc1(x.float())\n",
        "        o = self.relu(o)\n",
        "        return self.fc2(o)\n",
        "\n",
        "# Model instantiation\n",
        "model = OneHotMLP(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVAhdFGXDepU"
      },
      "source": [
        "## V - Laço de Treinamento - Otimização da função de Perda pelo Gradiente descendente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaH1Uv3yHih5",
        "outputId": "1658dbf7-719d-4cc5-840e-7694a626b24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA GeForce RTX 2060\n"
          ]
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nh_pe8rni93_",
        "outputId": "7c15ba5f-83a6-4288-f327-d21d74e12522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5],             Loss: 0.6927,             Elapsed Time: 9.30 sec\n",
            "Epoch [2/5],             Loss: 0.6934,             Elapsed Time: 5.89 sec\n",
            "Epoch [3/5],             Loss: 0.6927,             Elapsed Time: 5.76 sec\n",
            "Epoch [4/5],             Loss: 0.6930,             Elapsed Time: 5.84 sec\n",
            "Epoch [5/5],             Loss: 0.6930,             Elapsed Time: 5.79 sec\n"
          ]
        }
      ],
      "source": [
        "# II.2 Com a o notebook configurado para GPU T4, meça o tempo de dois laços dentro do\n",
        "# for da linha 13 (coloque um break após dois laços) e determine quanto demora\n",
        "# demora para o passo de forward (linhas 14 a 18), para o backward (linhas 20, 21 e 22)\n",
        "# e o tempo total de um laço. Faça as contas e identifique o trecho que é mais demorado.\n",
        "# II.2.a) Tempo do laço = ; Tempo do forward = ;Tempo do backward = ; Conclusão.\n",
        "\n",
        "import time\n",
        "\n",
        "# Debug\n",
        "print_loop = False\n",
        "\n",
        "model = model.to(device)\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# II.2.b) Trecho que precisa ser otimizado. (Esse é um problema mais difícil)\n",
        "# II.2.c) Otimize o código e explique aqui.\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()  # Start time of the epoch\n",
        "    model.train()\n",
        "\n",
        "    loop_count = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        loop_start = time.time()\n",
        "        if(loop_count == 2 and print_loop):\n",
        "          # Para medição do tempo do loop.\n",
        "          break\n",
        "\n",
        "        forward_start = time.time()\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        gpu_cpy_time = time.time() - forward_start\n",
        "        # Forward pass\n",
        "        model_start = time.time()\n",
        "        outputs = model(inputs)\n",
        "        model_time = time.time() - model_start\n",
        "        loss = criterion(outputs.squeeze(), labels.float())\n",
        "        forward_time = time.time() - forward_start\n",
        "\n",
        "        # Backward and optimize\n",
        "        backward_start = time.time()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        backward_time = time.time() - backward_start\n",
        "\n",
        "        # Loop optimization\n",
        "        loop_count += 1\n",
        "        loop_time = time.time() - loop_start\n",
        "        if (epoch == 0 and print_loop):\n",
        "          print(\"Loop #\", loop_count)\n",
        "          print(\"Tempo de loop = \", loop_time)\n",
        "          print(\"Forward pass = \", forward_time)\n",
        "          print(\"Gpu cpy =\", gpu_cpy_time)\n",
        "          print(\"Model =\", model_time)\n",
        "          print(\"Backward pass = \", backward_time)\n",
        "          print()\n",
        "\n",
        "    end_time = time.time()  # End time of the epoch\n",
        "    epoch_duration = end_time - start_time  # Duration of epoch\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], \\\n",
        "            Loss: {loss.item():.4f}, \\\n",
        "            Elapsed Time: {epoch_duration:.2f} sec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBswOILbX39h"
      },
      "source": [
        "#### II.2.a) Medição dos tempos de loop\n",
        "\n",
        "Notamos que o tempo do passo do forward leva mais tempo que o passo de backward, conforme os dados obtidos abaixo para a primeira época do treinamento.\n",
        "\n",
        "**Para 200 amostras**:\n",
        "\n",
        "```\n",
        "Loop # 1\n",
        "Tempo de loop =  0.7379188537597656\n",
        "Forward pass =  0.34381794929504395\n",
        "Backward pass =  0.09396243095397949\n",
        "\n",
        "Loop # 2\n",
        "Tempo de loop =  0.9603068828582764\n",
        "Forward pass =  0.0016582012176513672\n",
        "Backward pass =  0.004204511642456055\n",
        "```\n",
        "#### II.2.b) Trecho que precisa ser otimizado. (Esse é um problema mais difícil)\n",
        "#### II.2.c) Otimize o código e explique aqui.\n",
        "\n",
        "Para otimizarmos o loop, podemos utilizar um otimizador mais eficiente, como é o caso do Adam. Alteramos este trecho no código:\n",
        "\n",
        "```\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "```\n",
        "\n",
        "Notamos os novos resultados para **200 amostras**:\n",
        "\n",
        "```\n",
        "Loop # 1\n",
        "Tempo de loop =  0.27255821228027344\n",
        "Forward pass =  0.002206563949584961\n",
        "Backward pass =  0.0019736289978027344\n",
        "\n",
        "Loop # 2\n",
        "Tempo de loop =  0.4292025566101074\n",
        "Forward pass =  0.0016396045684814453\n",
        "Backward pass =  0.0016634464263916016\n",
        "```\n",
        "\n",
        "#### Novos tempos de "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_SUgwEGXw40"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwvahen5D1oM"
      },
      "source": [
        "## VI - Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DtTPUBfjBj-",
        "outputId": "92800f0b-d653-40d3-c9cf-603f1b15415d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 50.04%\n"
          ]
        }
      ],
      "source": [
        "## evaluation\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(outputs.squeeze()))\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA024",
      "language": "python",
      "name": "ia024"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
