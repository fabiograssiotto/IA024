{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "Nome: Fabio Grassiotto  \n",
        "RA: 890441\n",
        "\n",
        "**Exercício: LoRA**\n",
        "\n",
        "- Exercício didático para entender a técnica de fazer ajuste fino em modelos grandes usando poucos recursos\n",
        "- Aplicar no pré exercício de análise de sentimento ou no segundo exercício, e modelo de linguagem, com vocabulário de 3000 palavras, embedding size e 2 camadas, treinados da forma usual (medir tempo de treinamento/época)\n",
        "- Modificar o seu modelo para adotar a técnica do LoRA no embedding e nas 2 camadas, e fazer o ajuste-fino, isto é, continuar o treinamento anterior, lembrando que as matrizes originais ficarão congeladas e o ajuste dos pesos serão apenas aplicados nas matrizes do LoRA. Medir o tempo de treinamento/época.\n",
        "- Por último, substituir o modelo original, com os novos pesos calculados pelo W + LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "# Vocabulary\n",
        "vocab_size = 3000\n",
        "context_size = 9\n",
        "pattern = r'\\w+|[,;.:!?\\']'\n",
        "\n",
        "# Training\n",
        "batch_size = 32\n",
        "lr = 0.05\n",
        "pretrain_epochs = 10\n",
        "finetune_epochs = 50\n",
        "\n",
        "# Model\n",
        "embedding_dim = 64\n",
        "hidden_dim = 200\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# LoRA parameters\n",
        "lora_r = 1         # Rank adaptation\n",
        "lora_alpha = 1     # Scaling factor\n",
        "lora_scaling = lora_alpha / lora_r "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Colab environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if (IN_COLAB):\n",
        "    # Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    project_folder=\"/content/drive/MyDrive/Classes/IA024/Aula_5_6\"\n",
        "    os.chdir(project_folder)\n",
        "    !ls -la\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset\n",
        "\n",
        "Definimos dois datasets: um de pré-treinamento, com livros do José de Alencar, e um segundo dataset com apenas uma obra do Machado de Assis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def list_files(directory):\n",
        "    file_list = []\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            file_list.append(file_path)\n",
        "    return file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "metadata": {},
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "# Check if download is necessary\n",
        "if not os.path.exists(\"dataset/pretrain/67724.txt.utf-8\"):\n",
        "    print(\"Downloading Gutenberg texts\")\n",
        "\n",
        "    # José de Alencar - pretrain\n",
        "    !wget https://www.gutenberg.org/ebooks/67724.txt.utf-8 -P dataset/pretrain/\n",
        "    !wget https://www.gutenberg.org/ebooks/67725.txt.utf-8 -P dataset/pretrain/\n",
        "    !wget https://www.gutenberg.org/ebooks/67740.txt.utf-8 -P dataset/pretrain/\n",
        "    !wget https://www.gutenberg.org/ebooks/38496.txt.utf-8 -P dataset/pretrain/\n",
        "    !wget https://www.gutenberg.org/ebooks/44540.txt.utf-8 -P dataset/pretrain/\n",
        "    !wget https://www.gutenberg.org/ebooks/29040.txt.utf-8 -P dataset/pretrain/\n",
        "    !wget https://www.gutenberg.org/ebooks/67831.txt.utf-8 -P dataset/pretrain/\n",
        "\n",
        "    # Machado de Assis - fine-tuning\n",
        "    !wget https://www.gutenberg.org/ebooks/54829.txt.utf-8 -P dataset/fine-tune/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limpeza do texto principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "    text_start = text.find(start_marker)\n",
        "    text_end = text.find(end_marker)\n",
        "\n",
        "    text_content= text[text_start:text_end].replace('\\r','')\n",
        "    paragraphs = []\n",
        "    for paragraph in text_content.split(\"\\n\\n\"):\n",
        "        paragraph = paragraph.replace('\\n', ' ').strip()\n",
        "        # Validation of length and index lines\n",
        "        if (len(paragraph) > 10 and '....' not in paragraph):\n",
        "            paragraphs.append(paragraph)\n",
        "    return paragraphs\n",
        "\n",
        "pretrain_texts = list_files(\"dataset/pretrain\")\n",
        "finetune_texts = list_files(\"dataset/fine-tune\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of paragraphs (pretrain dataset): 9162\n",
            "Number of paragraphs (fine-tune dataset): 1298\n"
          ]
        }
      ],
      "source": [
        "pretrain_cleaned_paragraphs=[]\n",
        "finetune_cleaned_paragraphs=[]\n",
        "\n",
        "for file_path in pretrain_texts:\n",
        "    text = open(file_path, \"r\").read()\n",
        "    cleaned = clean_text(text)\n",
        "    pretrain_cleaned_paragraphs += cleaned\n",
        "\n",
        "for file_path in finetune_texts:\n",
        "    text = open(file_path, \"r\").read()\n",
        "    cleaned = clean_text(text)\n",
        "    finetune_cleaned_paragraphs += cleaned\n",
        "\n",
        "print(f'Number of paragraphs (pretrain dataset): {len(pretrain_cleaned_paragraphs)}')\n",
        "print(f'Number of paragraphs (fine-tune dataset): {len(finetune_cleaned_paragraphs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "metadata": {},
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words on pretrain dataset: 22543\n",
            "Words on fine-tune dataset: 10196\n"
          ]
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(pattern, text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "word_counts_pretrain = count_words(pretrain_cleaned_paragraphs)\n",
        "word_counts_finetune = count_words(finetune_cleaned_paragraphs)\n",
        "\n",
        "print(f'Words on pretrain dataset: {len(word_counts_pretrain)}')\n",
        "print(f'Words on fine-tune dataset: {len(word_counts_finetune)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FiP7OCo9zJ_I",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "pretrain_most_frequent_words = [word for word, count in word_counts_pretrain.most_common(vocab_size)]\n",
        "pretrain_vocab = {word: i for i, word in enumerate(pretrain_most_frequent_words, 1)}\n",
        "\n",
        "finetune_most_frequent_words = [word for word, count in word_counts_finetune.most_common(vocab_size)]\n",
        "finetune_vocab = {word: i for i, word in enumerate(finetune_most_frequent_words, 1)}\n",
        "\n",
        "# Teste\n",
        "finetune_vocab = pretrain_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrain:\n",
            "Most Frequent Words: [',', '.', 'a', 'o', 'de', 'que', 'e', 'se', ';', 'do']\n",
            "Vocabulary Size: 15000\n",
            "Finetune:\n",
            "Most Frequent Words: [',', '.', 'a', 'que', 'de', 'e', 'o', ';', 'não', 'um']\n",
            "Vocabulary Size: 15000\n"
          ]
        }
      ],
      "source": [
        "print(\"Pretrain:\")\n",
        "print(f\"Most Frequent Words: {pretrain_most_frequent_words[:10]}\")\n",
        "print(f\"Vocabulary Size: {len(pretrain_vocab)}\")\n",
        "\n",
        "print(\"Finetune:\")\n",
        "print(f\"Most Frequent Words: {finetune_most_frequent_words[:10]}\")\n",
        "print(f\"Vocabulary Size: {len(finetune_vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Codificação / Decodificação das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "metadata": {},
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Seq: Será d'aquelle, onde se referem as circumstancias, á que attribuo a predilecção de meu espirito pela fórma litteraria do romance.\n",
            "Encoded: [438, 39, 46, 195, 1, 60, 8, 5695, 19, 1178, 1, 22, 6, 7451, 3, 5696, 5, 62, 164, 50, 665, 1114, 10, 579, 2]\n",
            "Decoded: ['será', 'd', \"'\", 'aquelle', ',', 'onde', 'se', 'referem', 'as', 'circumstancias', ',', 'á', 'que', 'attribuo', 'a', 'predilecção', 'de', 'meu', 'espirito', 'pela', 'fórma', 'litteraria', 'do', 'romance', '.']\n",
            "Reconstructed Seq: será d ' aquelle , onde se referem as circumstancias , á que attribuo a predilecção de meu espirito pela fórma litteraria do romance .\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(pattern, sentence.lower())]\n",
        "\n",
        "def decode_sentence(encoded_sentence, vocab):\n",
        "    words = []\n",
        "    for index in encoded_sentence:\n",
        "        word = next((word for word, code in vocab.items() if code == index), \"<UNK>\")\n",
        "        words.append(word)\n",
        "\n",
        "    return words\n",
        "\n",
        "seq = pretrain_cleaned_paragraphs[20]\n",
        "spc = ' '\n",
        "encoded = encode_sentence(seq, pretrain_vocab)\n",
        "decoded = decode_sentence(encoded, pretrain_vocab)\n",
        "\n",
        "print(f'Original Seq: {seq}')\n",
        "print(f'Encoded: {encoded}')\n",
        "print(f'Decoded: {decoded}')\n",
        "print(f'Reconstructed Seq: {spc.join(decoded)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Iy-elI1magRR",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class BagOfWordsDataset(Dataset):\n",
        "  def __init__(self, paragraphs, vocab, context):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.vocab = vocab\n",
        "    self.context = context\n",
        "    self.tokens, self.targets = self.setup()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.tokens[idx]), torch.tensor(self.targets[idx])\n",
        "  \n",
        "  def setup(self):\n",
        "    tokens = []\n",
        "    targets = []\n",
        "    for paragraph in self.paragraphs:\n",
        "      encoded = encode_sentence(paragraph, self.vocab)\n",
        "      \n",
        "      # If paragraph is smaller than the context, skip it.\n",
        "      if len(encoded) < self.context + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(len(encoded) - self.context):\n",
        "        tks = encoded[i:i+self.context]\n",
        "        tgt = encoded[i+self.context]\n",
        "        # Only add if there are no unknown tokens in both context and target.\n",
        "        bad_token = 0\n",
        "        if not (bad_token in tks or tgt == bad_token):\n",
        "          tokens.append(tks)\n",
        "          targets.append(tgt)\n",
        "    return tokens, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "metadata": {},
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrain dataset:\n",
            "Training samples: 7329\n",
            "Validation samples: 1833\n",
            "Training dataset samples: 120482\n",
            "Validation dataset samples: 30919\n",
            "\n",
            "Finetune dataset:\n",
            "Training samples: 1038\n",
            "Validation samples: 260\n",
            "Training dataset samples: 15583\n",
            "Validation dataset samples: 4353\n"
          ]
        }
      ],
      "source": [
        "# Train/Validation split\n",
        "pretrain_train_data, pretrain_val_data = train_test_split(pretrain_cleaned_paragraphs, test_size=0.2, random_state=18)\n",
        "finetune_train_data, finetune_val_data = train_test_split(finetune_cleaned_paragraphs, test_size=0.2, random_state=18)\n",
        "\n",
        "pretrain_train_dataset = BagOfWordsDataset(pretrain_train_data, pretrain_vocab, context_size)\n",
        "pretrain_val_dataset = BagOfWordsDataset(pretrain_val_data, pretrain_vocab, context_size)\n",
        "finetune_train_dataset = BagOfWordsDataset(finetune_train_data, finetune_vocab, context_size)\n",
        "finetune_val_dataset = BagOfWordsDataset(finetune_val_data, finetune_vocab, context_size)\n",
        "\n",
        "# Counting all Samples\n",
        "print(\"Pretrain dataset:\")\n",
        "print(f\"Training samples: {len(pretrain_train_data)}\")\n",
        "print(f\"Validation samples: {len(pretrain_val_data)}\")\n",
        "print(f\"Training dataset samples: {len(pretrain_train_dataset)}\")\n",
        "print(f\"Validation dataset samples: {len(pretrain_val_dataset)}\")\n",
        "print()\n",
        "print(\"Finetune dataset:\")\n",
        "print(f\"Training samples: {len(finetune_train_data)}\")\n",
        "print(f\"Validation samples: {len(finetune_val_data)}\")\n",
        "print(f\"Training dataset samples: {len(finetune_train_dataset)}\")\n",
        "print(f\"Validation dataset samples: {len(finetune_val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[   9,  249,   19, 6673, 6618, 5261,    4,  367,   17]]), tensor([3])]\n"
          ]
        }
      ],
      "source": [
        "tst_loader = DataLoader(pretrain_train_dataset, batch_size = 1, shuffle=True)\n",
        "sample = next(iter(tst_loader))\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gC0C5qn2zJ_J",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Train/val loaders\n",
        "pretrain_train_loader = DataLoader(pretrain_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "pretrain_val_loader = DataLoader(pretrain_val_dataset, batch_size=batch_size, shuffle=True)\n",
        "finetune_train_loader = DataLoader(finetune_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "finetune_val_loader = DataLoader(finetune_val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Modelo (Modificado para habilitar ou desabilitar *low-rank adaptation*)\n",
        "Se o parâmetro é ativado, as três camandas do modelo base (embedding, linear_1 e linear_2) terão seu pesos congelados para a utilização de *low-rank adaptation*, com apenas as matrizes A e B da LoRA sendo treinadas. Quando o parâmetro é desativado, as três camadas voltam a ser treinadas. Toda essa lógica é implementada na função *forward()* abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "I2qKG9YczJ_K",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "class BengioModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BengioModel, self).__init__()\n",
        "        self.LoRA_enabled = False # Default\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # LoRA parameters\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.lora_r = lora_r\n",
        "        self.scaling = lora_scaling\n",
        "        \n",
        "        # Embeddings layer\n",
        "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
        "        # First Linear Layer\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim, bias=True)\n",
        "        # Activation and Dropout\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "        # Second Linear Layer\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size+1, bias=True)\n",
        "\n",
        "        # LoRA Matrixes\n",
        "        # LoRA on embeddings layer\n",
        "        self.embeddings_lora_B = nn.Parameter(torch.zeros(vocab_size+1, self.lora_r), requires_grad=False)\n",
        "        self.embeddings_lora_A = nn.Parameter(torch.randn(self.lora_r, embedding_dim), requires_grad=False)\n",
        "        # LoRA on the first linear layer\n",
        "        self.linear1_lora_B = nn.Parameter(torch.zeros(context_size*embedding_dim, self.lora_r), requires_grad=False)\n",
        "        self.linear1_lora_A = nn.Parameter(torch.randn(self.lora_r, hidden_dim), requires_grad=False)\n",
        "        # LoRA on the second linear layer\n",
        "        self.linear2_lora_B = nn.Parameter(torch.zeros(hidden_dim, self.lora_r), requires_grad=False)\n",
        "        self.linear2_lora_A = nn.Parameter(torch.randn(self.lora_r, vocab_size+1), requires_grad=False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Embeddings\n",
        "        embeds = self.embeddings(inputs)\n",
        "        if (self.LoRA_enabled):\n",
        "            one_hot = F.one_hot(inputs, self.vocab_size+1).to(torch.float32)\n",
        "            embeddings_LoRA = one_hot @ (self.embeddings_lora_B @ self.embeddings_lora_A)\n",
        "            embeddings_LoRA = embeddings_LoRA * self.scaling\n",
        "            embeds = embeds + embeddings_LoRA\n",
        "\n",
        "        # Flatten embeddings\n",
        "        embeds = embeds.view(embeds.size(0), -1)\n",
        "        \n",
        "        # First linear layer\n",
        "        out = self.linear1(embeds)\n",
        "        if (self.LoRA_enabled):\n",
        "            linear1_lora_out = embeds @ (self.linear1_lora_B @ self.linear1_lora_A)\n",
        "            linear1_lora_out = linear1_lora_out * self.scaling\n",
        "            out = out + linear1_lora_out\n",
        "        \n",
        "        activation = self.tanh(out)\n",
        "        activation = self.dropout(activation)\n",
        "\n",
        "        # Second linear layer\n",
        "        out = self.linear2(activation)\n",
        "        if (self.LoRA_enabled):\n",
        "            linear2_lora_out = activation @ (self.linear2_lora_B @ self.linear2_lora_A)\n",
        "            linear2_lora_out = linear2_lora_out * self.scaling\n",
        "            out = out + linear2_lora_out\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def enable_LoRA(self):\n",
        "        self.LoRA_enabled = True\n",
        "        # Freeze base model parameters\n",
        "        print(\"Freezing Embeddings\")\n",
        "        self.embeddings.weight.requires_grad = False\n",
        "        print(\"Freezing Layer 1\")\n",
        "        self.linear1.weight.requires_grad = False\n",
        "        print(\"Freezing Layer 2\")\n",
        "        self.linear2.weight.requires_grad = False\n",
        "        # Unfreeze LoRA parameters\n",
        "        print(\"Unfreezing LoRA parameters\")\n",
        "        self.embeddings_lora_A.requires_grad = True\n",
        "        self.embeddings_lora_B.requires_grad = True\n",
        "        self.linear1_lora_A.requires_grad = True\n",
        "        self.linear1_lora_B.requires_grad = True\n",
        "        self.linear2_lora_A.requires_grad = True\n",
        "        self.linear2_lora_B.requires_grad = True\n",
        "\n",
        "    def disable_LoRA(self):\n",
        "        self.LoRA_enabled = False\n",
        "        print(\"Unfreezing Embeddings\")\n",
        "        self.embeddings.weight.requires_grad = True\n",
        "        print(\"Unfreezing Layer 1\")\n",
        "        self.linear1.weight.requires_grad = True\n",
        "        print(\"Unfreezing Layer 2\")\n",
        "        self.linear2.weight.requires_grad = True\n",
        "        print(\"Freezing LoRA parameters\")\n",
        "        self.embeddings_lora_A.requires_grad = False\n",
        "        self.embeddings_lora_B.requires_grad = False\n",
        "        self.linear1_lora_A.requires_grad = False\n",
        "        self.linear1_lora_B.requires_grad = False\n",
        "        self.linear2_lora_A.requires_grad = False\n",
        "        self.linear2_lora_B.requires_grad = False\n",
        "\n",
        "    \n",
        "    def apply_LoRA_weights(self):\n",
        "        # Apply LoRA weights to the main model.\n",
        "        \n",
        "        lora_embeddings_weights = (self.embeddings_lora_B @ self.embeddings_lora_A) * self.scaling\n",
        "        lora_linear1_weights = (self.linear1_lora_B @ self.linear1_lora_A).transpose(0, 1) * self.scaling\n",
        "        lora_linear2_weights = (self.linear2_lora_B @ self.linear2_lora_A).transpose(0, 1) * self.scaling\n",
        "        self.embeddings.weight.data += lora_embeddings_weights\n",
        "        self.linear1.weight.data += lora_linear1_weights\n",
        "        self.linear2.weight.data += lora_linear2_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7yjQ1KXOzJ_K",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "model = BengioModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Teste básico do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xmsD59TfzJ_K",
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 9])\n",
            "torch.Size([32])\n",
            "tensor([11676, 14712, 12358,  1281, 11373,  4140,     5, 14324,  8178,  1939,\n",
            "        13843,  2772, 11263,  3064,  6023,  2521,  9948,  1708,  3948,  4851,\n",
            "         2365,  4647,    38, 12391,  2148,  7197,  3087, 10173,   361,  3092,\n",
            "         8001,  2247])\n",
            "tensor([  221,    14,     2,    11,    14,    18,   120,    25,    21,   156,\n",
            "           30,     4,   565,    14,     7,     4,    16,     1,   543,   242,\n",
            "           52,     1,  4881,     9,  3157,    56, 11521,    49,   804,    27,\n",
            "          194,     2])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(pretrain_train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]\n",
        "\n",
        "print(input.shape)\n",
        "print(target.shape)\n",
        "\n",
        "output = model(input)\n",
        "pred = output.argmax(dim=1)\n",
        "\n",
        "print(pred)\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Treinamento e Avaliação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funções de Treinamento e Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Função para Contagem de Parâmetros do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O modelo tem um total de 4,090,665 parâmetros.\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Exemplo de uso:\n",
        "total_params = count_parameters(model)\n",
        "print(f'O modelo tem um total de {total_params:,} parâmetros.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Função para Avaliação Inicial do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def init_eval(model, train_loader):\n",
        "    # Initial Perplexity and Loss\n",
        "    # Before training\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss += criterion(outputs, targets).item()\n",
        "\n",
        "    loss /= len(train_loader)\n",
        "    perp = torch.exp(torch.tensor(loss))\n",
        "\n",
        "    print(f'Initial Loss: {loss:.4f}')\n",
        "    print(f'Initial Perplexity: {perp:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Função para Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, train_loader):\n",
        "      # Training Loop\n",
        "      model.train()\n",
        "\n",
        "      # Overall training time stats\n",
        "      epoch_time_total = 0\n",
        "      epoch_time_fwd_total = 0\n",
        "      \n",
        "      for epoch in range(epochs):\n",
        "\n",
        "            epoch_start = time.time()\n",
        "            # Metrics\n",
        "            epoch_loss = 0\n",
        "            epoch_correct = 0\n",
        "            epoch_samples = 0\n",
        "            \n",
        "            # Training times\n",
        "            forward_time = 0\n",
        "\n",
        "            for inputs, targets in tqdm(train_loader):\n",
        "                  inputs = inputs.to(device)  # Move input data to the device\n",
        "                  targets = targets.to(device)\n",
        "\n",
        "                  # Forward pass\n",
        "                  forward_start = time.time()\n",
        "                  outputs = model(inputs)\n",
        "                  forward_time += (time.time() - forward_start)\n",
        "\n",
        "                  loss = criterion(outputs, targets)\n",
        "\n",
        "                  # Backward pass and optimization\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "\n",
        "                  optimizer.step()\n",
        "\n",
        "                  # Loss\n",
        "                  epoch_loss += loss.item()\n",
        "\n",
        "                  # Predicted\n",
        "                  predicted = outputs.argmax(dim=1)\n",
        "                  epoch_correct += (predicted == targets).sum().item()\n",
        "                  epoch_samples += targets.size(0)\n",
        "\n",
        "            # Calculate average loss and accuracy for epoch\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            acc = epoch_correct / epoch_samples\n",
        "\n",
        "            # Perplexity\n",
        "            perp = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "            epoch_end = time.time()\n",
        "            epoch_time = epoch_end - epoch_start\n",
        "            \n",
        "            # Total training time\n",
        "            epoch_time_total += epoch_time\n",
        "            epoch_time_fwd_total += forward_time\n",
        "            \n",
        "            # Print epoch statistics\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Epoch Time: {epoch_time:.2f}, Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%, Perplexity: {perp:.4f}')\n",
        "      \n",
        "      # Overall training average times\n",
        "      epoch_time_avg = epoch_time_total / epochs\n",
        "      epoch_time_fwd_avg = epoch_time_fwd_total / epochs\n",
        "      epoch_time_bwd_avg = epoch_time_avg - epoch_time_fwd_avg\n",
        "      print()\n",
        "      print(f'Average Times per Epoch: {epoch_time_avg:.2f}, Forward Pass: {epoch_time_fwd_avg:.2f}, Backward Pass: {epoch_time_bwd_avg:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "#### Função para Avaliação na Base de Validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nXXO78GSDqPg",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def eval(model, val_loader):\n",
        "    model.eval()\n",
        "\n",
        "    loss_sum = 0\n",
        "    total_sum = 0\n",
        "    correct_sum = 0\n",
        "    eval_round = 0\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)      \n",
        "            loss_sum += loss\n",
        "\n",
        "            # Get the predicted labels\n",
        "            predicted = outputs.argmax(dim=1)\n",
        "\n",
        "            total_sum += targets.size(0)\n",
        "            correct_sum += (predicted == targets).sum().item()\n",
        "            eval_round += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = 100 * correct_sum / total_sum\n",
        "\n",
        "    # Calculate average perplexity\n",
        "    average_loss = loss_sum / len(val_loader)\n",
        "    average_perplexity = torch.exp(average_loss)\n",
        "\n",
        "    print(f'Test Accuracy: {acc:.2f}%')\n",
        "    print(f'Average Loss: {average_loss:.2f}')\n",
        "    print(f'Average Perplexity: {average_perplexity:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento e Avaliação do modelo base (sem LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BengioModel(\n",
            "  (embeddings): Embedding(15001, 64)\n",
            "  (linear1): Linear(in_features=576, out_features=200, bias=True)\n",
            "  (tanh): Tanh()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (linear2): Linear(in_features=200, out_features=15001, bias=True)\n",
            ")\n",
            "\n",
            "Base model parameters = 4090665\n"
          ]
        }
      ],
      "source": [
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "base_parameters = count_parameters(model)\n",
        "print()\n",
        "print(f'Base model parameters = {base_parameters}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base Model - No LoRA\n",
            "\n",
            "Initial Evaluation\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:07<00:00, 537.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Loss: 9.6609\n",
            "Initial Perplexity: 15691.1396\n",
            "\n",
            "Training the Model\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:15<00:00, 239.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Epoch Time: 15.70, Loss: 6.8787, Accuracy: 0.08%, Perplexity: 971.4037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:15<00:00, 247.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10], Epoch Time: 15.25, Loss: 6.2056, Accuracy: 0.09%, Perplexity: 495.5381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:14<00:00, 255.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10], Epoch Time: 14.74, Loss: 5.9914, Accuracy: 0.10%, Perplexity: 399.9790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:15<00:00, 250.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/10], Epoch Time: 15.03, Loss: 5.8389, Accuracy: 0.11%, Perplexity: 343.4047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:14<00:00, 252.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/10], Epoch Time: 14.91, Loss: 5.7130, Accuracy: 0.11%, Perplexity: 302.7672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:14<00:00, 253.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/10], Epoch Time: 14.85, Loss: 5.6041, Accuracy: 0.12%, Perplexity: 271.5383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:14<00:00, 254.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/10], Epoch Time: 14.78, Loss: 5.5037, Accuracy: 0.12%, Perplexity: 245.6085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:14<00:00, 256.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/10], Epoch Time: 14.67, Loss: 5.4102, Accuracy: 0.13%, Perplexity: 223.6745\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:14<00:00, 259.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/10], Epoch Time: 14.51, Loss: 5.3231, Accuracy: 0.13%, Perplexity: 205.0199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:14<00:00, 256.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/10], Epoch Time: 14.71, Loss: 5.2367, Accuracy: 0.13%, Perplexity: 188.0536\n",
            "\n",
            "Average Times per Epoch: 14.91, Forward Pass: 1.70, Backward Pass: 13.21\n",
            "\n",
            "Evaluation on the Validation Dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3766/3766 [00:08<00:00, 461.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 15.68%\n",
            "Average Loss: 4.98\n",
            "Average Perplexity: 146.15\n"
          ]
        }
      ],
      "source": [
        "print(\"Base Model - No LoRA\")\n",
        "print()\n",
        "print(\"Initial Evaluation\")\n",
        "print()\n",
        "init_eval(model, pretrain_train_loader)\n",
        "print()\n",
        "print(\"Training the Model\")\n",
        "print()\n",
        "train(model, pretrain_epochs, pretrain_train_loader)\n",
        "print()\n",
        "print(\"Evaluation on the Validation Dataset\")\n",
        "eval(model, pretrain_train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ativação do LoRA e *fine-tuning* do modelo com o dataset de fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezing Embeddings\n",
            "Freezing Layer 1\n",
            "Freezing Layer 2\n",
            "Unfreezing LoRA parameters\n",
            "BengioModel(\n",
            "  (embeddings): Embedding(15001, 64)\n",
            "  (linear1): Linear(in_features=576, out_features=200, bias=True)\n",
            "  (tanh): Tanh()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (linear2): Linear(in_features=200, out_features=15001, bias=True)\n",
            ")\n",
            "\n",
            "LoRA model parameters = 46243\n",
            "Percentage of base = 0.01%\n"
          ]
        }
      ],
      "source": [
        "model.enable_LoRA()\n",
        "print(model)\n",
        "\n",
        "lora_parameters = count_parameters(model)\n",
        "\n",
        "print()\n",
        "print(f'LoRA model parameters = {lora_parameters}')\n",
        "print(f'Percentage of base = {lora_parameters/base_parameters:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base Model - LoRA enabled\n",
            "Train the model with the fine-tune dataset\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 192.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Epoch Time: 2.54, Loss: 6.4791, Accuracy: 0.09%, Perplexity: 651.3954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 197.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/50], Epoch Time: 2.46, Loss: 6.1002, Accuracy: 0.11%, Perplexity: 445.9691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 201.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/50], Epoch Time: 2.42, Loss: 6.0531, Accuracy: 0.11%, Perplexity: 425.4445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 195.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/50], Epoch Time: 2.50, Loss: 6.0224, Accuracy: 0.11%, Perplexity: 412.5819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 198.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/50], Epoch Time: 2.46, Loss: 5.9969, Accuracy: 0.11%, Perplexity: 402.1859\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 203.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/50], Epoch Time: 2.40, Loss: 5.9689, Accuracy: 0.11%, Perplexity: 391.0882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 202.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/50], Epoch Time: 2.41, Loss: 5.9511, Accuracy: 0.12%, Perplexity: 384.1693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 205.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/50], Epoch Time: 2.37, Loss: 5.9436, Accuracy: 0.12%, Perplexity: 381.3236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 204.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/50], Epoch Time: 2.39, Loss: 5.9367, Accuracy: 0.11%, Perplexity: 378.6850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 202.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/50], Epoch Time: 2.40, Loss: 5.9209, Accuracy: 0.12%, Perplexity: 372.7441\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 206.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/50], Epoch Time: 2.36, Loss: 5.9219, Accuracy: 0.12%, Perplexity: 373.1026\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 201.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/50], Epoch Time: 2.42, Loss: 5.9086, Accuracy: 0.11%, Perplexity: 368.1720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 204.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/50], Epoch Time: 2.38, Loss: 5.9004, Accuracy: 0.12%, Perplexity: 365.2015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 205.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/50], Epoch Time: 2.37, Loss: 5.8870, Accuracy: 0.12%, Perplexity: 360.3291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 205.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/50], Epoch Time: 2.38, Loss: 5.8816, Accuracy: 0.12%, Perplexity: 358.3825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 208.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/50], Epoch Time: 2.34, Loss: 5.8682, Accuracy: 0.12%, Perplexity: 353.6273\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 203.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/50], Epoch Time: 2.39, Loss: 5.8532, Accuracy: 0.12%, Perplexity: 348.3333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 205.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/50], Epoch Time: 2.38, Loss: 5.8434, Accuracy: 0.12%, Perplexity: 344.9329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 207.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/50], Epoch Time: 2.35, Loss: 5.8410, Accuracy: 0.12%, Perplexity: 344.1245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 204.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/50], Epoch Time: 2.39, Loss: 5.8326, Accuracy: 0.12%, Perplexity: 341.2381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 206.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [21/50], Epoch Time: 2.36, Loss: 5.8263, Accuracy: 0.12%, Perplexity: 339.1054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 209.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [22/50], Epoch Time: 2.33, Loss: 5.8235, Accuracy: 0.12%, Perplexity: 338.1449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 205.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [23/50], Epoch Time: 2.38, Loss: 5.8141, Accuracy: 0.12%, Perplexity: 334.9956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 204.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [24/50], Epoch Time: 2.38, Loss: 5.8082, Accuracy: 0.12%, Perplexity: 333.0078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 205.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [25/50], Epoch Time: 2.37, Loss: 5.8071, Accuracy: 0.12%, Perplexity: 332.6433\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 204.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [26/50], Epoch Time: 2.39, Loss: 5.7899, Accuracy: 0.12%, Perplexity: 326.9748\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 207.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [27/50], Epoch Time: 2.35, Loss: 5.7920, Accuracy: 0.12%, Perplexity: 327.6667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 205.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [28/50], Epoch Time: 2.38, Loss: 5.7719, Accuracy: 0.12%, Perplexity: 321.1365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 205.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [29/50], Epoch Time: 2.38, Loss: 5.7678, Accuracy: 0.12%, Perplexity: 319.8279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 205.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [30/50], Epoch Time: 2.37, Loss: 5.7545, Accuracy: 0.12%, Perplexity: 315.6187\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 208.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [31/50], Epoch Time: 2.34, Loss: 5.7519, Accuracy: 0.12%, Perplexity: 314.8011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 206.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [32/50], Epoch Time: 2.36, Loss: 5.7446, Accuracy: 0.12%, Perplexity: 312.4868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 204.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [33/50], Epoch Time: 2.39, Loss: 5.7402, Accuracy: 0.12%, Perplexity: 311.1296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 206.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [34/50], Epoch Time: 2.36, Loss: 5.7440, Accuracy: 0.12%, Perplexity: 312.2980\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 202.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [35/50], Epoch Time: 2.41, Loss: 5.7266, Accuracy: 0.12%, Perplexity: 306.9321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 203.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [36/50], Epoch Time: 2.40, Loss: 5.7211, Accuracy: 0.12%, Perplexity: 305.2431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 204.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [37/50], Epoch Time: 2.38, Loss: 5.7184, Accuracy: 0.13%, Perplexity: 304.4091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 207.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [38/50], Epoch Time: 2.35, Loss: 5.7091, Accuracy: 0.13%, Perplexity: 301.6022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 208.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [39/50], Epoch Time: 2.34, Loss: 5.7032, Accuracy: 0.12%, Perplexity: 299.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 207.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [40/50], Epoch Time: 2.36, Loss: 5.6995, Accuracy: 0.13%, Perplexity: 298.7159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 202.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [41/50], Epoch Time: 2.41, Loss: 5.6955, Accuracy: 0.13%, Perplexity: 297.5109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 204.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [42/50], Epoch Time: 2.38, Loss: 5.6835, Accuracy: 0.13%, Perplexity: 293.9886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 203.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [43/50], Epoch Time: 2.40, Loss: 5.6907, Accuracy: 0.12%, Perplexity: 296.0911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 208.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [44/50], Epoch Time: 2.34, Loss: 5.6818, Accuracy: 0.13%, Perplexity: 293.4756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 208.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [45/50], Epoch Time: 2.34, Loss: 5.6808, Accuracy: 0.12%, Perplexity: 293.1930\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 207.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [46/50], Epoch Time: 2.35, Loss: 5.6642, Accuracy: 0.13%, Perplexity: 288.3504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 206.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [47/50], Epoch Time: 2.36, Loss: 5.6628, Accuracy: 0.13%, Perplexity: 287.9425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 203.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [48/50], Epoch Time: 2.39, Loss: 5.6561, Accuracy: 0.13%, Perplexity: 286.0380\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 198.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [49/50], Epoch Time: 2.46, Loss: 5.6593, Accuracy: 0.12%, Perplexity: 286.9468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 487/487 [00:02<00:00, 200.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50/50], Epoch Time: 2.43, Loss: 5.6547, Accuracy: 0.13%, Perplexity: 285.6438\n",
            "\n",
            "Average Times per Epoch: 2.39, Forward Pass: 0.52, Backward Pass: 1.86\n",
            "\n",
            "Evaluation on the Validation Dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 137/137 [00:00<00:00, 304.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 12.06%\n",
            "Average Loss: 5.76\n",
            "Average Perplexity: 316.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Base Model - LoRA enabled\")\n",
        "print(\"Train the model with the fine-tune dataset\")\n",
        "print()\n",
        "train(model, finetune_epochs, finetune_train_loader)\n",
        "print()\n",
        "print(\"Evaluation on the Validation Dataset\")\n",
        "eval(model, finetune_val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Desativação do LoRA e Avaliação do Modelo com os novos pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unfreezing Embeddings\n",
            "Unfreezing Layer 1\n",
            "Unfreezing Layer 2\n",
            "Freezing LoRA parameters\n",
            "BengioModel(\n",
            "  (embeddings): Embedding(15001, 64)\n",
            "  (linear1): Linear(in_features=576, out_features=200, bias=True)\n",
            "  (tanh): Tanh()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (linear2): Linear(in_features=200, out_features=15001, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4090665"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.apply_LoRA_weights()\n",
        "model.disable_LoRA()\n",
        "print(model)\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base Model - Weights = W + LoRA\n",
            "Evaluation on the Validation Dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 967/967 [00:01<00:00, 513.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 9.69%\n",
            "Average Loss: 6.10\n",
            "Average Perplexity: 444.83\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Base Model - Weights = W + LoRA\")\n",
        "print(\"Evaluation on the Validation Dataset\")\n",
        "eval(model, pretrain_val_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA024",
      "language": "python",
      "name": "ia024"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
