{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: LoRA\n",
        "\n",
        "- Exercício didático para entender a técnica de fazer ajuste fino em modelos grandes usando poucos recursos\n",
        "- Aplicar no pré exercício de análise de sentimento ou no segundo exercício, e modelo de linguagem, com vocabulário de 3000 palavras, embedding size e 2 camadas, treinados da forma usual (medir tempo de treinamento/época)\n",
        "- Modificar o seu modelo para adotar a técnica do LoRA no embedding e nas 2 camadas, e fazer o ajuste-fino, isto é, continuar o treinamento anterior, lembrando que as matrizes originais ficarão congeladas e o ajuste dos pesos serão apenas aplicados nas matrizes do LoRA. Medir o tempo de treinamento/época.\n",
        "- Por último, substituir o modelo original, com os novos pesos calculados pelo W + LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global variables\n",
        "\n",
        "# Vocabulary\n",
        "vocab_size = 3000\n",
        "context_size = 9\n",
        "pattern = r'\\w+|[,;.:!?\\']'\n",
        "\n",
        "# Training\n",
        "batch_size = 32\n",
        "epochs = 25\n",
        "lr = 0.01\n",
        "\n",
        "# Model\n",
        "embedding_dim = 64\n",
        "hidden_dim = 200\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# LoRA parameters\n",
        "lora_r = 1        # Rank adaptation\n",
        "lora_alpha = 2    # Scaling factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Colab environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if (IN_COLAB):\n",
        "    %pip install colorama\n",
        "\n",
        "    # Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    project_folder=\"/content/drive/MyDrive/Classes/IA024/Aula_2_3\"\n",
        "    os.chdir(project_folder)\n",
        "    !ls -la\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "# Check if download is necessary\n",
        "if not os.path.exists(\"67724.txt.utf-8\"):\n",
        "    print(\"Downloading Gutenberg texts\")\n",
        "\n",
        "    !wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "    !wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limpeza do texto principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of paragraphs: 4596\n"
          ]
        }
      ],
      "source": [
        "text_1 = open(\"67724.txt.utf-8\",\"r\").read()\n",
        "text_2 = open(\"67725.txt.utf-8\",\"r\").read()\n",
        "\n",
        "def clean_text(text):\n",
        "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "    text_start = text.find(start_marker)\n",
        "    text_end = text.find(end_marker)\n",
        "\n",
        "    text_content= text[text_start:text_end].replace('\\r','')\n",
        "    paragraphs = []\n",
        "    for paragraph in text_content.split(\"\\n\\n\"):\n",
        "        paragraph = paragraph.replace('\\n', ' ').strip()\n",
        "        # Validation of length and index lines\n",
        "        if (len(paragraph) > 10 and '....' not in paragraph):\n",
        "            paragraphs.append(paragraph)\n",
        "    return paragraphs\n",
        "\n",
        "cleaned_paragraphs = clean_text(text_1)+clean_text(text_2)\n",
        "print(f'Number of paragraphs: {len(cleaned_paragraphs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11875"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(pattern, text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Frequent Words: [',', '.', 'a', 'que', 'o', 'de', 'e', 'se', ';', 'um']\n",
            "Vocabulary Size: 3000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Most Frequent Words: {most_frequent_words[:10]}\")\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Codificação / Decodificação das sentenças"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Seq: Ahi, o _Paquequer_ lança-se rapido sobre o seu leito, e atravessa as florestas como o tapir, espumando, deixando o pello esparso pelas pontas de rochedo, e enchendo a solidão com o estampido de sua carreira. De repente, falta-lhe o espaço, foge-lhe a terra; o soberbo rio recúa um momento para concentrar as suas forças e precipita-se de um só arremesso, como o tigre sobre a presa.\n",
            "Encoded: [235, 1, 5, 723, 0, 8, 762, 38, 5, 19, 324, 1, 7, 0, 23, 634, 28, 5, 2447, 1, 0, 1, 763, 5, 1776, 0, 269, 1065, 6, 486, 1, 7, 2448, 3, 687, 16, 5, 2449, 6, 17, 1777, 2, 6, 240, 1, 522, 29, 5, 612, 1, 0, 29, 3, 128, 9, 5, 1577, 99, 0, 10, 72, 18, 0, 23, 87, 591, 7, 2450, 8, 6, 10, 74, 0, 1, 28, 5, 592, 38, 3, 979, 2]\n",
            "Decoded: ['ahi', ',', 'o', '_paquequer_', '<UNK>', 'se', 'rapido', 'sobre', 'o', 'seu', 'leito', ',', 'e', '<UNK>', 'as', 'florestas', 'como', 'o', 'tapir', ',', '<UNK>', ',', 'deixando', 'o', 'pello', '<UNK>', 'pelas', 'pontas', 'de', 'rochedo', ',', 'e', 'enchendo', 'a', 'solidão', 'com', 'o', 'estampido', 'de', 'sua', 'carreira', '.', 'de', 'repente', ',', 'falta', 'lhe', 'o', 'espaço', ',', '<UNK>', 'lhe', 'a', 'terra', ';', 'o', 'soberbo', 'rio', '<UNK>', 'um', 'momento', 'para', '<UNK>', 'as', 'suas', 'forças', 'e', 'precipita', 'se', 'de', 'um', 'só', '<UNK>', ',', 'como', 'o', 'tigre', 'sobre', 'a', 'presa', '.']\n",
            "Reconstructed Seq: ahi , o _paquequer_ <UNK> se rapido sobre o seu leito , e <UNK> as florestas como o tapir , <UNK> , deixando o pello <UNK> pelas pontas de rochedo , e enchendo a solidão com o estampido de sua carreira . de repente , falta lhe o espaço , <UNK> lhe a terra ; o soberbo rio <UNK> um momento para <UNK> as suas forças e precipita se de um só <UNK> , como o tigre sobre a presa .\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(pattern, sentence.lower())]\n",
        "\n",
        "def decode_sentence(encoded_sentence, vocab):\n",
        "    words = []\n",
        "    for index in encoded_sentence:\n",
        "        word = next((word for word, code in vocab.items() if code == index), \"<UNK>\")\n",
        "        words.append(word)\n",
        "\n",
        "    return words\n",
        "\n",
        "seq = cleaned_paragraphs[20]\n",
        "spc = ' '\n",
        "encoded = encode_sentence(seq, vocab)\n",
        "decoded = decode_sentence(encoded, vocab)\n",
        "\n",
        "print(f'Original Seq: {seq}')\n",
        "print(f'Encoded: {encoded}')\n",
        "print(f'Decoded: {decoded}')\n",
        "print(f'Reconstructed Seq: {spc.join(decoded)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class BagOfWordsDataset(Dataset):\n",
        "  def __init__(self, paragraphs, vocab, context):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.vocab = vocab\n",
        "    self.context = context\n",
        "    self.tokens, self.targets = self.setup()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.tokens[idx]), torch.tensor(self.targets[idx])\n",
        "  \n",
        "  def setup(self):\n",
        "    tokens = []\n",
        "    targets = []\n",
        "    for paragraph in self.paragraphs:\n",
        "      encoded = encode_sentence(paragraph, self.vocab)\n",
        "      \n",
        "      # If paragraph is smaller than the context, skip it.\n",
        "      if len(encoded) < self.context + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(len(encoded) - self.context):\n",
        "        tks = encoded[i:i+self.context]\n",
        "        tgt = encoded[i+self.context]\n",
        "        # Only add if there are no unknown tokens in both context and target.\n",
        "        bad_token = 0\n",
        "        if not (bad_token in tks or tgt == bad_token):\n",
        "          tokens.append(tks)\n",
        "          targets.append(tgt)\n",
        "    return tokens, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 3676\n",
            "Validation samples: 920\n",
            "\n",
            "Training dataset samples: 24360\n",
            "Validation dataset samples: 5851\n"
          ]
        }
      ],
      "source": [
        "# Train/Validation split\n",
        "train_data, val_data = train_test_split(cleaned_paragraphs, test_size=0.2, random_state=18)\n",
        "\n",
        "train_dataset = BagOfWordsDataset(train_data, vocab, context_size)\n",
        "val_dataset = BagOfWordsDataset(val_data, vocab, context_size)\n",
        "\n",
        "# Counting all Samples\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print()\n",
        "print(f\"Training dataset samples: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[ 27, 742,  10,  90,  18, 141,   8,   3,  17]]), tensor([144])]\n"
          ]
        }
      ],
      "source": [
        "tst_loader = DataLoader(train_dataset, batch_size = 1, shuffle=True)\n",
        "sample = next(iter(tst_loader))\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [],
      "source": [
        "# Train/val loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Modelo (Modificado para receber o parâmetro booleano *apply_LoRA*)\n",
        "#### Se o parâmetro é ativado, as três camandas do modelo base (embedding, linear_1 e linear_2) terão seu pesos congelados para a utilização de *low-rank adaptation*, com apenas as camadas específicas da LoRA sendo treinadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "class BengioModel(torch.nn.Module):\n",
        "    def __init__(self, apply_LoRA):\n",
        "        super(BengioModel, self).__init__()\n",
        "        self.apply_LoRA = apply_LoRA\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # LoRA parameters\n",
        "        self.lora_alpha = lora_alpha\n",
        "        self.lora_r = lora_r\n",
        "        self.scaling = self.lora_alpha / self.lora_r\n",
        "        \n",
        "        # Embeddings layer\n",
        "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
        "        if (self.apply_LoRA):\n",
        "            # Freeze weights\n",
        "            print(\"Freezing Embeddings\")\n",
        "            self.embeddings.weight.requires_grad = False\n",
        "\n",
        "            # LoRA on embeddings layer\n",
        "            self.embeddings_lora_A = torch.nn.Parameter(torch.empty(self.lora_r, embedding_dim))\n",
        "            self.embeddings_lora_B = torch.nn.Parameter(torch.empty(vocab_size+1, self.lora_r))\n",
        "            torch.nn.init.zeros_(self.embeddings_lora_A)\n",
        "            torch.nn.init.normal_(self.embeddings_lora_B)\n",
        "\n",
        "        # First Linear Layer\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim, bias=True)\n",
        "        if (self.apply_LoRA):\n",
        "            # Freeze weights\n",
        "            print(\"Freezing Layer 1\")\n",
        "            self.linear1.weight.requires_grad = False\n",
        "\n",
        "            # LoRA on first linear layer\n",
        "            self.linear1_lora_A = torch.nn.Parameter(torch.empty(self.lora_r, hidden_dim))\n",
        "            self.linear1_lora_B = torch.nn.Parameter(torch.empty(context_size*embedding_dim, self.lora_r))\n",
        "            torch.nn.init.zeros_(self.linear1_lora_A)\n",
        "            torch.nn.init.normal_(self.linear1_lora_B)\n",
        "\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Second Linear Layer\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size+1, bias=True)\n",
        "        if (self.apply_LoRA):\n",
        "            # Freeze weights\n",
        "            print(\"Freezing Layer 2\")\n",
        "            \n",
        "            # LoRA on second linear layer\n",
        "            self.linear2_lora_A = torch.nn.Parameter(torch.empty(self.lora_r, vocab_size+1))\n",
        "            self.linear2_lora_B = torch.nn.Parameter(torch.empty(hidden_dim, self.lora_r))\n",
        "            torch.nn.init.zeros_(self.linear2_lora_A)\n",
        "            torch.nn.init.normal_(self.linear2_lora_B)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Embeddings\n",
        "        embeds = self.embeddings(inputs)\n",
        "        if (self.apply_LoRA):\n",
        "            one_hot = torch.nn.functional.one_hot(inputs, self.vocab_size+1).to(torch.float32)\n",
        "            embeddings_LoRA = torch.matmul(one_hot,\n",
        "                                           torch.matmul(self.embeddings_lora_B, self.embeddings_lora_A))\n",
        "            embeddings_LoRA = embeddings_LoRA * self.scaling\n",
        "            embeds = embeds + embeddings_LoRA\n",
        "\n",
        "        # Flatten embeddings\n",
        "        embeds = embeds.view(embeds.size(0), -1)\n",
        "        \n",
        "        # First linear layer\n",
        "        out = self.linear1(embeds)\n",
        "        if (self.apply_LoRA):\n",
        "            linear1_lora_out = torch.matmul(embeds,\n",
        "                                            torch.matmul(self.linear1_lora_B, self.linear1_lora_A))\n",
        "            linear1_lora_out = linear1_lora_out * self.scaling\n",
        "            out = out + linear1_lora_out\n",
        "        \n",
        "        activation = self.tanh(out)\n",
        "        activation = self.dropout(activation)\n",
        "\n",
        "        # Second linear layer\n",
        "        out = self.linear2(activation)\n",
        "        if (self.apply_LoRA):\n",
        "            linear2_lora_out = torch.matmul(activation,\n",
        "                                            torch.matmul(self.linear2_lora_B, self.linear2_lora_A))\n",
        "            linear2_lora_out = linear2_lora_out * self.scaling\n",
        "            out = out + linear2_lora_out\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "model = BengioModel(apply_LoRA=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Teste básico do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 9])\n",
            "torch.Size([32])\n",
            "tensor([1825, 2830, 2202, 2294,  999,  158,  114, 2817,  107, 1391,  594, 1727,\n",
            "        2816, 2676, 1107, 1001,  188, 1861, 2980, 2211, 1684,  721, 1706, 2508,\n",
            "        2633, 2442,  921, 2086,  587, 1426, 1626, 1048])\n",
            "tensor([ 328,   24,   14, 2607,    2,   38,    4,   26,  128,    7,    4,   46,\n",
            "         453,   14,    1, 2688, 1122,    5,    5,    6,    4,    5,   34,    4,\n",
            "          43,    6,    9,  407,  313,   30,    6,    3])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]\n",
        "\n",
        "print(input.shape)\n",
        "print(target.shape)\n",
        "\n",
        "output = model(input)\n",
        "pred = output.argmax(dim=1)\n",
        "\n",
        "print(pred)\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funções de Treinamento e Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Função para Contagem de Parâmetros do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O modelo tem um total de 910,665 parâmetros.\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Exemplo de uso:\n",
        "total_params = count_parameters(model)\n",
        "print(f'O modelo tem um total de {total_params:,} parâmetros.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Função para Avaliação Inicial do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_eval(model):\n",
        "    # Initial Perplexity and Loss\n",
        "    # Before training\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss += criterion(outputs, targets).item()\n",
        "\n",
        "    loss /= len(train_loader)\n",
        "    perp = torch.exp(torch.tensor(loss))\n",
        "\n",
        "    print(f'Initial Loss: {loss:.4f}')\n",
        "    print(f'Initial Perplexity: {perp:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Função para Treinamento do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model):\n",
        "      # Training Loop\n",
        "      model.train()\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "            epoch_start = time.time()\n",
        "            # Metrics\n",
        "            epoch_loss = 0\n",
        "            epoch_correct = 0\n",
        "            epoch_samples = 0\n",
        "            \n",
        "            # Training times\n",
        "            forward_time = 0\n",
        "\n",
        "            for inputs, targets in train_loader:\n",
        "                  inputs = inputs.to(device)  # Move input data to the device\n",
        "                  targets = targets.to(device)\n",
        "\n",
        "                  # Forward pass\n",
        "                  forward_start = time.time()\n",
        "                  outputs = model(inputs)\n",
        "                  forward_time += (time.time() - forward_start)\n",
        "\n",
        "                  loss = criterion(outputs, targets)\n",
        "\n",
        "                  # Backward pass and optimization\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "\n",
        "                  optimizer.step()\n",
        "\n",
        "                  # Loss\n",
        "                  epoch_loss += loss.item()\n",
        "\n",
        "                  # Predicted\n",
        "                  predicted = outputs.argmax(dim=1)\n",
        "                  epoch_correct += (predicted == targets).sum().item()\n",
        "                  epoch_samples += targets.size(0)\n",
        "\n",
        "            # Calculate average loss and accuracy for epoch\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            acc = epoch_correct / epoch_samples\n",
        "\n",
        "            # Perplexity\n",
        "            perp = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "            epoch_end = time.time()\n",
        "            epoch_time = epoch_end - epoch_start\n",
        "            \n",
        "            # Print epoch statistics\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%, Perplexity: {perp:.4f}')\n",
        "            print(f'Training Times Epoch: {epoch_time:.2f}, Forward Pass: {forward_time:.2f}, Backward Pass: {epoch_time-forward_time:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "#### Função para Avaliação na Base de Validação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nXXO78GSDqPg"
      },
      "outputs": [],
      "source": [
        "def eval(model):\n",
        "    model.eval()\n",
        "\n",
        "    loss_sum = 0\n",
        "    total_sum = 0\n",
        "    correct_sum = 0\n",
        "    eval_round = 0\n",
        "\n",
        "    loss = 0\n",
        "    perp = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)      \n",
        "            loss_sum += loss\n",
        "\n",
        "            # Get the predicted labels\n",
        "            predicted = outputs.argmax(dim=1)\n",
        "\n",
        "            total_sum += targets.size(0)\n",
        "            correct_sum += (predicted == targets).sum().item()\n",
        "            eval_round += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = 100 * correct_sum / total_sum\n",
        "\n",
        "    # Calculate average perplexity\n",
        "    average_loss = loss_sum / len(val_loader)\n",
        "    average_perplexity = torch.exp(average_loss)\n",
        "\n",
        "    print(f'Test Accuracy: {acc:.2f}%')\n",
        "    print(f'Average Loss: {average_loss:.2f}')\n",
        "    print(f'Average Perplexity: {average_perplexity:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento e Avaliação (Sem utilizar *low-rank adaptation*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BengioModel(\n",
            "  (embeddings): Embedding(3001, 64)\n",
            "  (linear1): Linear(in_features=576, out_features=200, bias=True)\n",
            "  (tanh): Tanh()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (linear2): Linear(in_features=200, out_features=3001, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "910665"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "model.to(device)\n",
        "print(model)\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base Model - No LoRA\n",
            "\n",
            "Initial Evaluation\n",
            "\n",
            "Initial Loss: 8.0377\n",
            "Initial Perplexity: 3095.4580\n",
            "\n",
            "Training the Model\n",
            "\n",
            "Epoch [1/25], Loss: 7.6686, Accuracy: 0.04%, Perplexity: 2140.1570\n",
            "Training Times Epoch: 2.93, Forward Pass: 0.37, Backward Pass: 2.56\n",
            "Epoch [2/25], Loss: 6.5710, Accuracy: 0.09%, Perplexity: 714.0520\n",
            "Training Times Epoch: 3.02, Forward Pass: 0.38, Backward Pass: 2.64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/25], Loss: 6.1775, Accuracy: 0.10%, Perplexity: 481.7852\n",
            "Training Times Epoch: 2.98, Forward Pass: 0.38, Backward Pass: 2.60\n",
            "Epoch [4/25], Loss: 5.9759, Accuracy: 0.11%, Perplexity: 393.8247\n",
            "Training Times Epoch: 3.42, Forward Pass: 0.37, Backward Pass: 3.05\n",
            "Epoch [5/25], Loss: 5.8366, Accuracy: 0.12%, Perplexity: 342.6158\n",
            "Training Times Epoch: 3.16, Forward Pass: 0.37, Backward Pass: 2.79\n",
            "Epoch [6/25], Loss: 5.7329, Accuracy: 0.12%, Perplexity: 308.8754\n",
            "Training Times Epoch: 3.41, Forward Pass: 0.39, Backward Pass: 3.02\n",
            "Epoch [7/25], Loss: 5.6434, Accuracy: 0.12%, Perplexity: 282.4118\n",
            "Training Times Epoch: 3.01, Forward Pass: 0.35, Backward Pass: 2.66\n",
            "Epoch [8/25], Loss: 5.5714, Accuracy: 0.13%, Perplexity: 262.8052\n",
            "Training Times Epoch: 3.30, Forward Pass: 0.35, Backward Pass: 2.95\n",
            "Epoch [9/25], Loss: 5.5036, Accuracy: 0.14%, Perplexity: 245.5740\n",
            "Training Times Epoch: 3.15, Forward Pass: 0.35, Backward Pass: 2.81\n",
            "Epoch [10/25], Loss: 5.4441, Accuracy: 0.14%, Perplexity: 231.3852\n",
            "Training Times Epoch: 2.92, Forward Pass: 0.32, Backward Pass: 2.60\n",
            "Epoch [11/25], Loss: 5.3886, Accuracy: 0.14%, Perplexity: 218.9048\n",
            "Training Times Epoch: 3.18, Forward Pass: 0.33, Backward Pass: 2.85\n",
            "Epoch [12/25], Loss: 5.3319, Accuracy: 0.14%, Perplexity: 206.8285\n",
            "Training Times Epoch: 2.92, Forward Pass: 0.33, Backward Pass: 2.60\n",
            "Epoch [13/25], Loss: 5.2840, Accuracy: 0.15%, Perplexity: 197.1494\n",
            "Training Times Epoch: 3.11, Forward Pass: 0.31, Backward Pass: 2.79\n",
            "Epoch [14/25], Loss: 5.2326, Accuracy: 0.15%, Perplexity: 187.2741\n",
            "Training Times Epoch: 2.99, Forward Pass: 0.34, Backward Pass: 2.65\n",
            "Epoch [15/25], Loss: 5.1861, Accuracy: 0.15%, Perplexity: 178.7680\n",
            "Training Times Epoch: 3.15, Forward Pass: 0.33, Backward Pass: 2.82\n",
            "Epoch [16/25], Loss: 5.1410, Accuracy: 0.16%, Perplexity: 170.8907\n",
            "Training Times Epoch: 3.22, Forward Pass: 0.34, Backward Pass: 2.88\n",
            "Epoch [17/25], Loss: 5.0961, Accuracy: 0.16%, Perplexity: 163.3843\n",
            "Training Times Epoch: 2.96, Forward Pass: 0.33, Backward Pass: 2.63\n",
            "Epoch [18/25], Loss: 5.0539, Accuracy: 0.16%, Perplexity: 156.6325\n",
            "Training Times Epoch: 3.13, Forward Pass: 0.33, Backward Pass: 2.79\n",
            "Epoch [19/25], Loss: 5.0140, Accuracy: 0.17%, Perplexity: 150.5022\n",
            "Training Times Epoch: 2.93, Forward Pass: 0.34, Backward Pass: 2.60\n",
            "Epoch [20/25], Loss: 4.9670, Accuracy: 0.17%, Perplexity: 143.5905\n",
            "Training Times Epoch: 3.24, Forward Pass: 0.34, Backward Pass: 2.91\n",
            "Epoch [21/25], Loss: 4.9275, Accuracy: 0.17%, Perplexity: 138.0347\n",
            "Training Times Epoch: 2.84, Forward Pass: 0.35, Backward Pass: 2.49\n",
            "Epoch [22/25], Loss: 4.8907, Accuracy: 0.17%, Perplexity: 133.0432\n",
            "Training Times Epoch: 3.07, Forward Pass: 0.35, Backward Pass: 2.72\n",
            "Epoch [23/25], Loss: 4.8465, Accuracy: 0.18%, Perplexity: 127.2974\n",
            "Training Times Epoch: 3.15, Forward Pass: 0.32, Backward Pass: 2.83\n",
            "Epoch [24/25], Loss: 4.8055, Accuracy: 0.18%, Perplexity: 122.1751\n",
            "Training Times Epoch: 3.17, Forward Pass: 0.35, Backward Pass: 2.82\n",
            "Epoch [25/25], Loss: 4.7711, Accuracy: 0.19%, Perplexity: 118.0453\n",
            "Training Times Epoch: 2.98, Forward Pass: 0.33, Backward Pass: 2.65\n",
            "\n",
            "Evaluation on the Validation Dataset\n",
            "Test Accuracy: 9.98%\n",
            "Average Loss: 5.64\n",
            "Average Perplexity: 282.18\n"
          ]
        }
      ],
      "source": [
        "print(\"Base Model - No LoRA\")\n",
        "print()\n",
        "print(\"Initial Evaluation\")\n",
        "print()\n",
        "init_eval(model)\n",
        "print()\n",
        "print(\"Training the Model\")\n",
        "print()\n",
        "train(model)\n",
        "print()\n",
        "print(\"Evaluation on the Validation Dataset\")\n",
        "eval(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Treinamento e Avaliação (Utilizando *low-rank adaptation*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezing Embeddings\n",
            "Freezing Layer 1\n",
            "Freezing Layer 2\n",
            "BengioModel(\n",
            "  (embeddings): Embedding(3001, 64)\n",
            "  (linear1): Linear(in_features=576, out_features=200, bias=True)\n",
            "  (tanh): Tanh()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (linear2): Linear(in_features=200, out_features=3001, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "610443"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_LoRA = BengioModel(apply_LoRA=True)\n",
        "\n",
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model_LoRA.parameters(), lr)\n",
        "\n",
        "model_LoRA.to(device)\n",
        "print(model_LoRA)\n",
        "count_parameters(model_LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base Model - With LoRA\n",
            "\n",
            "Initial Evaluation\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Loss: 8.0337\n",
            "Initial Perplexity: 3083.0664\n",
            "\n",
            "Training the Model\n",
            "\n",
            "Epoch [1/25], Loss: 7.5687, Accuracy: 0.03%, Perplexity: 1936.6826\n",
            "Training Times Epoch: 3.93, Forward Pass: 0.85, Backward Pass: 3.08\n",
            "Epoch [2/25], Loss: 6.9522, Accuracy: 0.04%, Perplexity: 1045.4222\n",
            "Training Times Epoch: 3.90, Forward Pass: 0.83, Backward Pass: 3.08\n",
            "Epoch [3/25], Loss: 6.7645, Accuracy: 0.04%, Perplexity: 866.5404\n",
            "Training Times Epoch: 3.84, Forward Pass: 0.82, Backward Pass: 3.02\n",
            "Epoch [4/25], Loss: 6.6441, Accuracy: 0.04%, Perplexity: 768.2319\n",
            "Training Times Epoch: 3.90, Forward Pass: 0.83, Backward Pass: 3.07\n",
            "Epoch [5/25], Loss: 6.4999, Accuracy: 0.05%, Perplexity: 665.0814\n",
            "Training Times Epoch: 3.89, Forward Pass: 0.81, Backward Pass: 3.08\n",
            "Epoch [6/25], Loss: 6.3723, Accuracy: 0.05%, Perplexity: 585.4117\n",
            "Training Times Epoch: 3.82, Forward Pass: 0.82, Backward Pass: 2.99\n",
            "Epoch [7/25], Loss: 6.3059, Accuracy: 0.05%, Perplexity: 547.7678\n",
            "Training Times Epoch: 3.82, Forward Pass: 0.81, Backward Pass: 3.01\n",
            "Epoch [8/25], Loss: 6.2233, Accuracy: 0.05%, Perplexity: 504.3567\n",
            "Training Times Epoch: 4.70, Forward Pass: 1.01, Backward Pass: 3.69\n",
            "Epoch [9/25], Loss: 6.1544, Accuracy: 0.06%, Perplexity: 470.7683\n",
            "Training Times Epoch: 6.62, Forward Pass: 1.49, Backward Pass: 5.13\n",
            "Epoch [10/25], Loss: 6.0954, Accuracy: 0.06%, Perplexity: 443.8180\n",
            "Training Times Epoch: 11.85, Forward Pass: 2.69, Backward Pass: 9.16\n",
            "Epoch [11/25], Loss: 6.0646, Accuracy: 0.06%, Perplexity: 430.3413\n",
            "Training Times Epoch: 7.31, Forward Pass: 1.60, Backward Pass: 5.72\n",
            "Epoch [12/25], Loss: 6.0325, Accuracy: 0.06%, Perplexity: 416.7688\n",
            "Training Times Epoch: 6.10, Forward Pass: 1.33, Backward Pass: 4.77\n",
            "Epoch [13/25], Loss: 6.0116, Accuracy: 0.06%, Perplexity: 408.1440\n",
            "Training Times Epoch: 5.44, Forward Pass: 1.21, Backward Pass: 4.24\n",
            "Epoch [14/25], Loss: 5.9864, Accuracy: 0.07%, Perplexity: 397.9750\n",
            "Training Times Epoch: 5.23, Forward Pass: 1.18, Backward Pass: 4.05\n",
            "Epoch [15/25], Loss: 5.9598, Accuracy: 0.07%, Perplexity: 387.5238\n",
            "Training Times Epoch: 5.23, Forward Pass: 1.16, Backward Pass: 4.07\n",
            "Epoch [16/25], Loss: 5.9396, Accuracy: 0.07%, Perplexity: 379.7647\n",
            "Training Times Epoch: 7.12, Forward Pass: 1.60, Backward Pass: 5.52\n",
            "Epoch [17/25], Loss: 5.9231, Accuracy: 0.07%, Perplexity: 373.5854\n",
            "Training Times Epoch: 8.07, Forward Pass: 1.76, Backward Pass: 6.31\n",
            "Epoch [18/25], Loss: 5.9102, Accuracy: 0.07%, Perplexity: 368.7757\n",
            "Training Times Epoch: 5.17, Forward Pass: 1.13, Backward Pass: 4.03\n",
            "Epoch [19/25], Loss: 5.8820, Accuracy: 0.07%, Perplexity: 358.5129\n",
            "Training Times Epoch: 4.85, Forward Pass: 1.08, Backward Pass: 3.78\n",
            "Epoch [20/25], Loss: 5.8742, Accuracy: 0.07%, Perplexity: 355.7343\n",
            "Training Times Epoch: 9.10, Forward Pass: 2.02, Backward Pass: 7.08\n",
            "Epoch [21/25], Loss: 5.8478, Accuracy: 0.07%, Perplexity: 346.4618\n",
            "Training Times Epoch: 6.74, Forward Pass: 1.46, Backward Pass: 5.28\n",
            "Epoch [22/25], Loss: 5.8372, Accuracy: 0.07%, Perplexity: 342.8350\n",
            "Training Times Epoch: 5.63, Forward Pass: 1.20, Backward Pass: 4.44\n",
            "Epoch [23/25], Loss: 5.8267, Accuracy: 0.07%, Perplexity: 339.2389\n",
            "Training Times Epoch: 5.23, Forward Pass: 1.17, Backward Pass: 4.06\n",
            "Epoch [24/25], Loss: 5.8099, Accuracy: 0.07%, Perplexity: 333.5715\n",
            "Training Times Epoch: 6.71, Forward Pass: 1.53, Backward Pass: 5.18\n",
            "Epoch [25/25], Loss: 5.7982, Accuracy: 0.07%, Perplexity: 329.7007\n",
            "Training Times Epoch: 6.43, Forward Pass: 1.37, Backward Pass: 5.07\n",
            "\n",
            "Evaluation on the Validation Dataset\n",
            "Test Accuracy: 7.02%\n",
            "Average Loss: 6.06\n",
            "Average Perplexity: 426.36\n"
          ]
        }
      ],
      "source": [
        "print(\"Base Model - With LoRA\")\n",
        "print()\n",
        "print(\"Initial Evaluation\")\n",
        "print()\n",
        "init_eval(model_LoRA)\n",
        "print()\n",
        "print(\"Training the Model\")\n",
        "print()\n",
        "train(model_LoRA)\n",
        "print()\n",
        "print(\"Evaluation on the Validation Dataset\")\n",
        "eval(model_LoRA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Geração de Sentenças"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'o indio atravessou a sala , e <UNK> se continuava tua o no ella parecia face no do passasse'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a random sentence of context size\n",
        "\n",
        "seq = \"O indio atravessou a sala, e collocando-se\"\n",
        "inputs = encode_sentence(seq, vocab)\n",
        "inputs = inputs[len(inputs)-context_size:len(inputs)]\n",
        "inputs = torch.tensor([inputs])\n",
        "\n",
        "new_tokens = 10\n",
        "with torch.no_grad():\n",
        "    context = torch.tensor(inputs, dtype=torch.long).squeeze().to(device)\n",
        "    \n",
        "    for _ in range(new_tokens):\n",
        "\n",
        "        output = model(torch.tensor(inputs).to(device))\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1).squeeze()       \n",
        "        context = torch.cat([context, next_token.reshape(1)], dim=0)\n",
        "        \n",
        "' '.join(decode_sentence(context.tolist(), vocab))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA024",
      "language": "python",
      "name": "ia024"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
