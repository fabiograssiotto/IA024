{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings\n",
        "\n",
        "Neste exercício iremos treinar uma rede neural similar a do Bengio 2003 para prever a próxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Linguagem\".\n",
        "\n",
        "Portanto, você deve implementar o modelo de linguagem inspirado no artigo do Bengio, para prever a próxima palavra usando rede com embeddings e duas camadas.\n",
        "Sugestão de alguns parâmetros:\n",
        "* context_size = 9\n",
        "* max_vocab_size = 3000\n",
        "* embedding_dim = 64\n",
        "* usar pontuação no vocabulário\n",
        "* descartar qualquer contexto ou target que não esteja no vocabulário\n",
        "* É esperado conseguir uma perplexidade da ordem de 50.\n",
        "* Procurem fazer asserts para garantir que partes do seu programa estão testadas\n",
        "\n",
        "Este enunciado não é fixo, podem mudar qualquer um dos parâmetros acima, mas procurem conseguir a perplexidade esperada ou menor.\n",
        "\n",
        "Gerem alguns frases usando um contexto inicial e depois deslocando o contexto e prevendo a próxima palavra gerando frases compridas para ver se está gerando texto plausível.\n",
        "\n",
        "Algumas dicas:\n",
        "- Inclua caracteres de pontuação (ex: `.` e `,`) no vocabulário.\n",
        "- Deixe tudo como caixa baixa (lower-case).\n",
        "- A escolha do tamanho do vocabulario é importante: ser for muito grande, fica difícil para o modelo aprender boas representações. Se for muito pequeno, o modelo apenas conseguirá gerar textos simples.\n",
        "- Remova qualquer exemplo de treino/validação/teste que tenha pelo menos um token desconhecido (ou na entrada ou na saída).\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso.\n",
        "\n",
        "Procure por `TODO` para entender onde você precisa inserir o seu código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global variables\n",
        "\n",
        "# Vocabulary\n",
        "vocab_size = 3000\n",
        "context_size = 5\n",
        "pattern = r'\\w+|[,;.:!?\\']'\n",
        "\n",
        "# Training\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "\n",
        "# Model\n",
        "embedding_dim = 64\n",
        "hidden_dim = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Gutenberg texts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2024-03-17 20:19:43--  https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://www.gutenberg.org/cache/epub/67724/pg67724.txt [following]\n",
            "--2024-03-17 20:19:44--  http://www.gutenberg.org/cache/epub/67724/pg67724.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/cache/epub/67724/pg67724.txt [following]\n",
            "--2024-03-17 20:19:44--  https://www.gutenberg.org/cache/epub/67724/pg67724.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 372908 (364K) [text/plain]\n",
            "Saving to: '67724.txt.utf-8'\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 13%  172K 2s\n",
            "    50K .......... .......... .......... .......... .......... 27%  345K 1s\n",
            "   100K .......... .......... .......... .......... .......... 41% 5.33M 1s\n",
            "   150K .......... .......... .......... .......... .......... 54%  366K 0s\n",
            "   200K .......... .......... .......... .......... .......... 68% 15.0M 0s\n",
            "   250K .......... .......... .......... .......... .......... 82% 7.34M 0s\n",
            "   300K .......... .......... .......... .......... .......... 96% 73.5M 0s\n",
            "   350K .......... ....                                       100%  101M=0.6s\n",
            "\n",
            "2024-03-17 20:19:45 (616 KB/s) - '67724.txt.utf-8' saved [372908/372908]\n",
            "\n",
            "--2024-03-17 20:19:45--  https://www.gutenberg.org/ebooks/67725.txt.utf-8\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://www.gutenberg.org/cache/epub/67725/pg67725.txt [following]\n",
            "--2024-03-17 20:19:46--  http://www.gutenberg.org/cache/epub/67725/pg67725.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/cache/epub/67725/pg67725.txt [following]\n",
            "--2024-03-17 20:19:46--  https://www.gutenberg.org/cache/epub/67725/pg67725.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 345264 (337K) [text/plain]\n",
            "Saving to: '67725.txt.utf-8'\n",
            "\n",
            "     0K .......... .......... .......... .......... .......... 14%  173K 2s\n",
            "    50K .......... .......... .......... .......... .......... 29%  362K 1s\n",
            "   100K .......... .......... .......... .......... .......... 44% 6.23M 1s\n",
            "   150K .......... .......... .......... .......... .......... 59%  370K 0s\n",
            "   200K .......... .......... .......... .......... .......... 74% 8.96M 0s\n",
            "   250K .......... .......... .......... .......... .......... 88% 9.23M 0s\n",
            "   300K .......... .......... .......... .......              100% 30.2M=0.6s\n",
            "\n",
            "2024-03-17 20:19:47 (580 KB/s) - '67725.txt.utf-8' saved [345264/345264]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check if download is necessary\n",
        "if not os.path.exists(\"67724.txt.utf-8\"):\n",
        "    print(\"Downloading Gutenberg texts\")\n",
        "\n",
        "    !wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "    !wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4969"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = open(\"67724.txt.utf-8\",\"r\").read()\n",
        "text += open(\"67725.txt.utf-8\",\"r\").read()\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg eBook of O Guarany: romance brazileiro, Vol. 1 (of 2)\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n"
          ]
        }
      ],
      "source": [
        "# Checking the text\n",
        "print(paragraphs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Confio em Deos, meu pai, que vossos receios serão infundados; mas se elle nos quizesse submetter a tal provança, o unico lugar que compete a vosso filho e herdeiro de vosso nome é nesta casa ameaçada, ao vosso lado, para defender-vos e partilhar a vossa sorte, qualquer que ella seja.\n",
            "\n",
            "--Aqui me tendes! repetio o cavalheiro. Dizei o que quereis de D. Antonio de Mariz, e dizei-o claro e breve. Se fôr de justiça, sereis satisfeitos; se fôr uma falta, tereis a punição que merecerdes.\n",
            "\n",
            "--É impossivel!... gritou elle. Isso que dizeis é falso. Não ha homem que o fizesse.\n",
            "\n",
            "O indio no seu fanatismo não comprehendia que houvesse uma razão capaz de sacrificar a vida de Cecilia, que para elle era sagrada.\n",
            "\n",
            "Deixando Alvaro, a intenção do indio era atalhar os aventureiros, espera-los junto á cerca; e quando elles se separassem para entrar a um e um, mata-los.\n",
            "\n",
            "Number of paragraphs: 4892\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4892"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "# Print 5 random paragraphs\n",
        "num_paragraphs = len(cleaned_paragraphs)\n",
        "for i in range(0,5):\n",
        "    idx = random.randrange(num_paragraphs)\n",
        "    print(f\"{cleaned_paragraphs[idx]}\\n\")\n",
        "\n",
        "print(\"Number of paragraphs: \" + str(num_paragraphs))\n",
        "\n",
        "len(cleaned_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFVN2ihb33Rf"
      },
      "source": [
        "## Análise do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12610"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        word_counts.update(re.findall(pattern, text.lower()))\n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(cleaned_paragraphs)\n",
        "\n",
        "len(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Frequent Words: ['.', ',', 'a', 'que', 'o', 'de', 'e', 'se', ';', 'um']\n",
            "Vocabulary Size: 3000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Most Frequent Words: {most_frequent_words[:10]}\")\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Publicando este livro em 1857, se disse ser aquella primeira edição uma prova typographica, que algum dia talvez o autor se dispuzesse a rever.\n",
            "[0, 146, 0, 23, 0, 2, 8, 50, 117, 276, 266, 2669, 13, 1071, 0, 2, 4, 193, 137, 287, 5, 2264, 8, 0, 3, 2672, 1]\n"
          ]
        }
      ],
      "source": [
        "#pattern = r'\\w+'\n",
        "\n",
        "def encode_sentence(sentence, vocab):\n",
        "    return [vocab.get(word, 0) for word in re.findall(pattern, sentence.lower())]\n",
        "\n",
        "print(cleaned_paragraphs[20])\n",
        "print(encode_sentence(cleaned_paragraphs[20], vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, paragraphs, vocab, context):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.vocab = vocab\n",
        "    self.context = context\n",
        "    self.tokens, self.targets = self.setup()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.tokens)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.tensor(self.tokens[idx]), torch.tensor(self.targets[idx])\n",
        "  \n",
        "  def setup(self):\n",
        "    tokens = []\n",
        "    targets = []\n",
        "    for paragraph in self.paragraphs:\n",
        "      encoded = encode_sentence(paragraph, self.vocab)\n",
        "      \n",
        "      # If paragraph is smaller than the context, skip it.\n",
        "      if len(encoded) < self.context + 1:\n",
        "          continue\n",
        "\n",
        "      for i in range(len(encoded) - self.context):\n",
        "        tks = encoded[i:i+self.context]\n",
        "        tgt = encoded[i+self.context]\n",
        "        # Only add if there are no unknown tokens in both context and target.\n",
        "        bad_token = 0\n",
        "        if not (bad_token in tks or tgt == bad_token):\n",
        "          tokens.append(tks)\n",
        "          targets.append(tgt)\n",
        "    return tokens, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 3913\n",
            "Validation samples: 979\n",
            "\n",
            "Training dataset samples: 44547\n",
            "Validation dataset samples: 12257\n"
          ]
        }
      ],
      "source": [
        "# Train/Validation split\n",
        "train_data, val_data = train_test_split(cleaned_paragraphs, test_size=0.2, random_state=18)\n",
        "\n",
        "train_dataset = CustomDataset(train_data, vocab, context_size)\n",
        "val_dataset = CustomDataset(val_data, vocab, context_size)\n",
        "\n",
        "# Counting all Samples\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Validation samples: {len(val_data)}\")\n",
        "print()\n",
        "print(f\"Training dataset samples: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[   4,   33,   71,    6, 2254],\n",
            "        [  17,  427,    2,  136, 1297],\n",
            "        [  30,    1,  279,    2, 1655],\n",
            "        [1348, 2992,    4,    5, 2102],\n",
            "        [  19,  202,    2,  291,   21],\n",
            "        [   2,    7,  858,   38,   40],\n",
            "        [   4, 1002,  106, 1224,    6],\n",
            "        [  24,   19,  523,    6,  277],\n",
            "        [  79, 2264,   34,  127,   13],\n",
            "        [1658,    9,   33,    5,  192],\n",
            "        [   6,   64,    9,   33,  272],\n",
            "        [   3, 2459,    9,   12,   52],\n",
            "        [ 621,    7, 1993,    4, 1563],\n",
            "        [   1,    1,    1,    1,    1],\n",
            "        [  12,    9,   12,    5,  247],\n",
            "        [  52,    9,   39,  114,   12],\n",
            "        [   6,   75,  165,   95, 1317],\n",
            "        [1414,    2,   13,  773,    2],\n",
            "        [1265,  475,   30,   61,  325],\n",
            "        [ 180,    1,   30,    1,   47],\n",
            "        [   2,   50,   54,   16,   10],\n",
            "        [  14,  149,    5, 1125,  767],\n",
            "        [   9,    5,  102,  573,  579],\n",
            "        [  35, 1060,   56,    9,    7],\n",
            "        [   7, 1226,   27,  227,   22],\n",
            "        [  12,    8,  965,  136,  492],\n",
            "        [ 839,    6,   95, 2193,    9],\n",
            "        [  18,    3,   68,    4,    3],\n",
            "        [  39,  386,   37,    5,    4],\n",
            "        [1118, 1334,   11,  120,    9],\n",
            "        [  35,    2, 2195,    8,  700],\n",
            "        [1412,  218,  297,  113, 2279]]), tensor([   3,   15,   23,   46,   24,  188,  179,    2,  213,   14,    4,  899,\n",
            "          38,    1,    1,  327,    2,   37,    7,   12, 2934,    6,    4,  103,\n",
            "          95,    2,   28,   17,   29,  291,   18,    2])]\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "sample = next(iter(train_loader))\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "class BengioModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(BengioModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.linear2 = nn.Linear(h, vocab_size+1)\n",
        "        #self.relu = torch.nn.ReLU()\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        # Flatten embeddings\n",
        "        embeds = embeds.view(embeds.size(0), -1)\n",
        "        # Linear layer\n",
        "        out = self.linear1(embeds)\n",
        "        #out = self.relu(out)\n",
        "        out = self.tanh(out)\n",
        "        # Second layer\n",
        "        out = self.linear2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7yjQ1KXOzJ_K"
      },
      "outputs": [],
      "source": [
        "model = BengioModel(vocab_size, embedding_dim, context_size, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 5])\n",
            "torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]\n",
        "\n",
        "print(input.shape)\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HGbJcT5KzJ_K"
      },
      "outputs": [],
      "source": [
        "output = model(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um0lR4mNzJ_K",
        "outputId": "e6041da8-ca7f-4c9d-b28b-9e1250e820f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 725, 2406, 2240, 2987, 1755, 2552,  178, 2822, 2837,  974,  430, 2014,\n",
              "        2557,  291, 1038, 2870, 2712, 1895, 1777, 1304, 2482, 2606, 1816, 2397,\n",
              "        2830,  618,  274,  803, 1427,  684,  652, 2701])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la-b-f8jzJ_L",
        "outputId": "f040cef4-e409-4d20-d335-a3133aaeb63c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1746, 2075,   70,    2,  864,   48,   92,    9, 1023,   29,  320,   17,\n",
              "           4,    1,    1,  382,    3,   48,   23,    4,   27,  158,   12,    5,\n",
              "        2518,  110,    8,    1, 1257,   10,    1, 1316])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "a054092b-d801-4c60-eb75-85abfe57151d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BengioModel(\n",
              "  (embeddings): Embedding(3001, 64)\n",
              "  (linear1): Linear(in_features=320, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=3001, bias=True)\n",
              "  (tanh): Tanh()\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cross Entropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O modelo tem um total de 620,281 parâmetros.\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Exemplo de uso:\n",
        "total_params = count_parameters(model)\n",
        "print(f'O modelo tem um total de {total_params:,} parâmetros.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Loss: 8.0380\n",
            "Initial Perplexity: 3096.3850\n"
          ]
        }
      ],
      "source": [
        "# Initial Perplexity and Loss\n",
        "# Before training\n",
        "model.eval()\n",
        "\n",
        "loss = 0\n",
        "perp = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "\n",
        "loss /= len(train_loader)\n",
        "perp = torch.exp(torch.tensor(loss))\n",
        "\n",
        "print(f'Initial Loss: {loss:.4f}')\n",
        "print(f'Initial Perplexity: {perp:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Time:5.30, Loss: 6.3488, Accuracy: 0.11%, Perplexity: 571.7896\n",
            "Epoch [2/10], Time:5.51, Loss: 5.5139, Accuracy: 0.15%, Perplexity: 248.1158\n",
            "Epoch [3/10], Time:5.14, Loss: 5.0005, Accuracy: 0.18%, Perplexity: 148.4801\n",
            "Epoch [4/10], Time:5.14, Loss: 4.6462, Accuracy: 0.21%, Perplexity: 104.1842\n",
            "Epoch [5/10], Time:5.14, Loss: 4.3717, Accuracy: 0.24%, Perplexity: 79.1784\n",
            "Epoch [6/10], Time:5.35, Loss: 4.1872, Accuracy: 0.26%, Perplexity: 65.8386\n",
            "Epoch [7/10], Time:5.90, Loss: 4.0356, Accuracy: 0.28%, Perplexity: 56.5778\n",
            "Epoch [8/10], Time:5.74, Loss: 3.9409, Accuracy: 0.29%, Perplexity: 51.4654\n",
            "Epoch [9/10], Time:5.74, Loss: 3.8735, Accuracy: 0.30%, Perplexity: 48.1094\n",
            "Epoch [10/10], Time:5.88, Loss: 3.7960, Accuracy: 0.31%, Perplexity: 44.5238\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  epoch_start = time.time()\n",
        "  # Metrics\n",
        "  epoch_loss = 0\n",
        "  epoch_correct = 0\n",
        "  epoch_samples = 0\n",
        "\n",
        "  for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)  # Move input data to the device\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Predicted\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        epoch_correct += (predicted == targets).sum().item()\n",
        "        epoch_samples += targets.size(0)\n",
        "\n",
        "  # Calculate average loss and accuracy for epoch\n",
        "  avg_loss = epoch_loss / len(train_loader)\n",
        "  acc = epoch_correct / epoch_samples\n",
        "\n",
        "  # Perplexity\n",
        "  perp = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "  epoch_end = time.time()\n",
        "  epoch_time = epoch_end - epoch_start\n",
        "  # Print epoch statistics\n",
        "  print(f'Epoch [{epoch+1}/{epochs}], Time:{epoch_time:.2f}, Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%, Perplexity: {perp:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSXfwYISDoPN"
      },
      "source": [
        "## Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nXXO78GSDqPg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 19.30%\n",
            "Average Loss: 9.58\n",
            "Average Perplexity: 14409.77\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "loss_sum = 0\n",
        "total_sum = 0\n",
        "correct_sum = 0\n",
        "eval_round = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)        \n",
        "        loss_sum += loss\n",
        "\n",
        "        # Get the predicted labels\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total_sum += targets.size(0)\n",
        "        correct_sum += (predicted == targets).sum().item()\n",
        "        eval_round += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "acc = 100 * correct_sum/total_sum\n",
        "\n",
        "# Calculate average perplexity\n",
        "average_loss = loss_sum / eval_round\n",
        "average_perplexity = torch.exp(average_loss)\n",
        "\n",
        "print(f'Test Accuracy: {acc:.2f}%')\n",
        "print(f'Average Loss: {average_loss:.2f}')\n",
        "print(f'Average Perplexity: {average_perplexity:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frase: --Já que o quereis, força é satisfazer-vos. Noto que a ordem de D. Antonio, e o italiano carregou nesta palavra, manda-vos estar no _Paquequer_ um pouco antes de seis horas, a tempo de ouvir a prece.\n",
            "[72, 4, 5, 463, 2]\n",
            "[72, 4, 5, 463, 2, 2195]\n",
            "[72, 4, 5, 463, 2, 2195, 64]\n",
            "[72, 4, 5, 463, 2, 2195, 64, 12]\n",
            "[72, 4, 5, 463, 2, 2195, 64, 12, 69]\n",
            "[72, 4, 5, 463, 2, 2195, 64, 12, 69, 115]\n",
            "[72, 4, 5, 463, 2, 2195, 64, 12, 69, 115, 252]\n",
            "[72, 4, 5, 463, 2, 2195, 64, 12, 69, 115, 252, 628]\n",
            "[72, 4, 5, 463, 2, 2195, 64, 12, 69, 115, 252, 628, 3]\n",
            "[72, 4, 5, 463, 2, 2195, 64, 12, 69, 115, 252, 628, 3, 17]\n",
            "[72, 4, 5, 463, 2, 2195, 64, 12, 69, 115, 252, 628, 3, 17, 685]\n",
            "\u001b[34m['já', 'que', 'o', 'quereis', ',']\u001b[0m \u001b[31m['encaminhou', 'isabel', 'não', 'estava', 'podia', 'si', 'tornou', 'a', 'sua', 'segunda']\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "# Código adaptado da implementação do Cesar Bastos\n",
        "from colorama import Fore, Style\n",
        "\n",
        "text = cleaned_paragraphs\n",
        "model.to(device)\n",
        "def generate_text(model, vocab, text, max_length, context_size):\n",
        "    words = []\n",
        "    # Ensure there are enough words for at least one sequence\n",
        "    while len(words) < context_size:\n",
        "        random_number = random.randint(1, 4891)\n",
        "        words = encode_sentence(text[random_number], vocab)\n",
        "        if not words:\n",
        "            words = []\n",
        "            continue  # Skip if the sentence cannot be encoded\n",
        "        words = words[:context_size]\n",
        "        #print(words)\n",
        "        if any(token == 0 for token in words):\n",
        "            words = []\n",
        "            continue  # Skip if any token is zero (assuming 0 is a special token)\n",
        "        context = words\n",
        "\n",
        "    print(f\"Frase: {cleaned_paragraphs[random_number]}\")\n",
        "    print(words)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        words_tensor = torch.tensor(context[-context_size:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        logits = model(words_tensor)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        context.append(next_token.item())\n",
        "        print(context)\n",
        "    frase = []\n",
        "    for i in context: ##Agradecimentos a Ramon Abilio\n",
        "        word = next((word for word, code in vocab.items() if code == i), \"<UNKNOWN>\")\n",
        "        frase.append(word)\n",
        "\n",
        "    print(f\"{Fore.BLUE}{frase[:context_size]}{Style.RESET_ALL} {Fore.RED}{frase[-max_length:]}{Style.RESET_ALL} \")\n",
        "\n",
        "\n",
        "max_length= 10\n",
        "generate_text(model, vocab, text, max_length, context_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA024",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
