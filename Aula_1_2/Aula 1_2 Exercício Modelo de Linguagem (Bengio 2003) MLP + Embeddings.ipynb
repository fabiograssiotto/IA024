{"cells":[{"cell_type":"markdown","metadata":{"id":"fMI0JT_YuYF3"},"source":["## Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings\n","\n","Neste exercício iremos treinar uma rede neural similar a do Bengio 2003 para prever a próxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Linguagem\".\n","\n","Portanto, você deve implementar o modelo de linguagem inspirado no artigo do Bengio, para prever a próxima palavra usando rede com embeddings e duas camadas.\n","Sugestão de alguns parâmetros:\n","* context_size = 9\n","* max_vocab_size = 3000\n","* embedding_dim = 64\n","* usar pontuação no vocabulário\n","* descartar qualquer contexto ou target que não esteja no vocabulário\n","* É esperado conseguir uma perplexidade da ordem de 50.\n","* Procurem fazer asserts para garantir que partes do seu programa estão testadas\n","\n","Este enunciado não é fixo, podem mudar qualquer um dos parâmetros acima, mas procurem conseguir a perplexidade esperada ou menor.\n","\n","Gerem alguns frases usando um contexto inicial e depois deslocando o contexto e prevendo a próxima palavra gerando frases compridas para ver se está gerando texto plausível.\n","\n","Algumas dicas:\n","- Inclua caracteres de pontuação (ex: `.` e `,`) no vocabulário.\n","- Deixe tudo como caixa baixa (lower-case).\n","- A escolha do tamanho do vocabulario é importante: ser for muito grande, fica difícil para o modelo aprender boas representações. Se for muito pequeno, o modelo apenas conseguirá gerar textos simples.\n","- Remova qualquer exemplo de treino/validação/teste que tenha pelo menos um token desconhecido (ou na entrada ou na saída).\n","- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de GPU. Somente ligue a GPU quando o seu laço de treinamento já está funcionando\n","- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso.\n","\n","Procure por `TODO` para entender onde você precisa inserir o seu código."]},{"cell_type":"markdown","metadata":{"id":"bYbkEzdD37sZ"},"source":["## Faz download e carrega o dataset"]},{"cell_type":"code","source":["!wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n","!wget https://www.gutenberg.org/ebooks/67725.txt.utf-8\n","!pip install colorama"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qAnqY_q0beK","executionInfo":{"status":"ok","timestamp":1710361436873,"user_tz":180,"elapsed":11056,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"bc13f90e-4eaa-4258-cf93-4e3d12e20ebb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-13 20:23:45--  https://www.gutenberg.org/ebooks/67724.txt.utf-8\n","Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: http://www.gutenberg.org/cache/epub/67724/pg67724.txt [following]\n","--2024-03-13 20:23:45--  http://www.gutenberg.org/cache/epub/67724/pg67724.txt\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://www.gutenberg.org/cache/epub/67724/pg67724.txt [following]\n","--2024-03-13 20:23:46--  https://www.gutenberg.org/cache/epub/67724/pg67724.txt\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 372908 (364K) [text/plain]\n","Saving to: ‘67724.txt.utf-8.13’\n","\n","67724.txt.utf-8.13  100%[===================>] 364.17K  1.56MB/s    in 0.2s    \n","\n","2024-03-13 20:23:46 (1.56 MB/s) - ‘67724.txt.utf-8.13’ saved [372908/372908]\n","\n","--2024-03-13 20:23:46--  https://www.gutenberg.org/ebooks/67725.txt.utf-8\n","Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: http://www.gutenberg.org/cache/epub/67725/pg67725.txt [following]\n","--2024-03-13 20:23:46--  http://www.gutenberg.org/cache/epub/67725/pg67725.txt\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://www.gutenberg.org/cache/epub/67725/pg67725.txt [following]\n","--2024-03-13 20:23:47--  https://www.gutenberg.org/cache/epub/67725/pg67725.txt\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 345264 (337K) [text/plain]\n","Saving to: ‘67725.txt.utf-8.13’\n","\n","67725.txt.utf-8.13  100%[===================>] 337.17K  1.45MB/s    in 0.2s    \n","\n","2024-03-13 20:23:47 (1.45 MB/s) - ‘67725.txt.utf-8.13’ saved [345264/345264]\n","\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (0.4.6)\n"]}]},{"cell_type":"code","source":["text = open(\"67724.txt.utf-8\",\"r\").read()\n","text += open(\"67725.txt.utf-8\",\"r\").read()\n","\n","paragraphs = text.split(\"\\n\\n\")\n","len(paragraphs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_UzC9pV091C","executionInfo":{"status":"ok","timestamp":1710361436873,"user_tz":180,"elapsed":32,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"2ca4a1d5-5e04-4782-8d92-2066fe145fab"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4969"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import random\n","\n","cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n","\n","# lowercase\n","cleaned_paragraphs = [paragraph.lower() for paragraph in paragraphs]\n","\n","# Print 5 random paragraphs\n","num_paragraphs = len(cleaned_paragraphs)\n","for i in range(0,5):\n","    idx = random.randrange(num_paragraphs)\n","    print(f\"{cleaned_paragraphs[idx]}\\n\")\n","\n","print(\"Number of paragraphs: \" + str(num_paragraphs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UhUFjtNdDuG0","executionInfo":{"status":"ok","timestamp":1710361436873,"user_tz":180,"elapsed":23,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"13af8782-c72f-46b5-e189-552552a6a2ee"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--sim.\n","\n","--não ha aqui culpados, sr. d. antonio de mariz, disse o italiano\n","animando-se progressivamente; ha homens que são tratados como cães;\n","que são sacrificados a um capricho vosso, e que estão resolvidos a\n","reivindicarem os seus fóros de homens e de christãos!\n","\n","só quem tem viajado nos sertões e visto esses cardos gigantes, cujas\n","largas palmas crivadas de espinhos se entrelação estreitamente\n","formando uma alta muralha de alguns pés de grossura, poderá fazer\n","idéa da barreira impenetravel que cercava por todos os lados as pessoas\n","cuja voz pery ouvia sem distinguir as palavras.\n","\n","e loredano dizendo esta palavra assentou a mão sobre um seixo que havia\n","ao lado.\n","\n","a confiança que tinha, e com razão, no caracter de d. antonio\n","tranquillisava-o completamente; sabia que em caso algum o fidalgo\n","abriria um testamento que lhe fôra dado em deposito.\n","\n","Number of paragraphs: 4969\n"]}]},{"cell_type":"markdown","metadata":{"id":"bFVN2ihb33Rf"},"source":["## Análise do dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"qSRHqe3H4ZFw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710361436874,"user_tz":180,"elapsed":20,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"018b09c3-5a80-495d-9290-425b004f6789"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["12603"]},"metadata":{},"execution_count":4}],"source":["# Conta as palavras no dataset\n","from collections import Counter\n","import re\n","\n","def count_words(texts):\n","    word_counts = Counter()\n","    for text in texts:\n","        word_counts.update(re.findall(r'\\w+', text.lower()))\n","    return word_counts\n","\n","word_counts = count_words(cleaned_paragraphs)\n","\n","len(word_counts)"]},{"cell_type":"markdown","metadata":{"id":"EyGVDL9KzJ_I"},"source":["## Criando um vocabulário"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"FiP7OCo9zJ_I","executionInfo":{"status":"ok","timestamp":1710361436874,"user_tz":180,"elapsed":17,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}}},"outputs":[],"source":["vocab_size = 3000\n","most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n","vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhbhAsZbzJ_J","executionInfo":{"status":"ok","timestamp":1710361436874,"user_tz":180,"elapsed":16,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"95852cb8-c66c-45f0-acf5-cf16902e18c1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2660]"]},"metadata":{},"execution_count":6}],"source":["def encode_sentence(sentence, vocab):\n","    return [vocab.get(word, 0) for word in re.findall(r'\\w+', sentence.lower())]\n","\n","encode_sentence(cleaned_paragraphs[20], vocab)"]},{"cell_type":"markdown","metadata":{"id":"wia_ygbvzJ_J"},"source":["## Classe do dataset"]},{"cell_type":"code","source":["# Dataset class\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class CustomDataset(Dataset):\n","  def __init__(self, paragraphs, vocab, context):\n","    # Define your data here\n","    self.tokens = []\n","    self.targets = []\n","    for paragraph in paragraphs:\n","      encoded = encode_sentence(paragraph, vocab)\n","      for i in range(len(encoded) - context):\n","        self.tokens.append(encoded[i:i+context])\n","        self.targets.append(encoded[i+context])\n","\n","  def __len__(self):\n","    return len(self.tokens)\n","\n","  def __getitem__(self, idx):\n","    return torch.tensor(self.tokens[idx]), torch.tensor(self.targets[idx])"],"metadata":{"id":"IXBDrR3SrFU_","executionInfo":{"status":"ok","timestamp":1710361438599,"user_tz":180,"elapsed":1740,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","train_data, val_data = train_test_split(cleaned_paragraphs, test_size=0.2, random_state=18)"],"metadata":{"id":"roChN2Avou7x","executionInfo":{"status":"ok","timestamp":1710361438971,"user_tz":180,"elapsed":379,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aD1CVci2zJ_J","executionInfo":{"status":"ok","timestamp":1710361439298,"user_tz":180,"elapsed":332,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"a69c5be1-e250-4ec0-84c0-a6a7bb223841"},"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([ 12,  68,   0,   1, 102,   5,   9,   0,   0]), tensor(2))\n","(tensor([ 42,  70,  20,  84, 568, 716,   0,   1, 293]), tensor(8))\n"]}],"source":["context_size = 9\n","\n","train_dataset = CustomDataset(train_data, vocab, context_size)\n","val_dataset = CustomDataset(val_data, vocab, context_size)\n","\n","# Samples\n","print(train_dataset[0])\n","print(val_dataset[0])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"gC0C5qn2zJ_J","executionInfo":{"status":"ok","timestamp":1710361439299,"user_tz":180,"elapsed":23,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}}},"outputs":[],"source":["batch_size = 128\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","sample = next(iter(train_loader))"]},{"cell_type":"markdown","metadata":{"id":"-5_-Yud0zJ_K"},"source":["## Model"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"I2qKG9YczJ_K","executionInfo":{"status":"ok","timestamp":1710361439299,"user_tz":180,"elapsed":23,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}}},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BengioModel(torch.nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, context_size, h):\n","        super(BengioModel, self).__init__()\n","        self.embeddings = nn.Embedding(vocab_size+1, embedding_dim)\n","        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n","        self.linear2 = nn.Linear(h, vocab_size+1)\n","\n","    def forward(self, inputs):\n","        embeds = self.embeddings(inputs)\n","        # Flatten embeddings\n","        embeds = embeds.view(embeds.size(0), -1)\n","        # Linear layer with Relu activation\n","        out = self.linear1(embeds)\n","        out = F.relu(out)\n","        # Second layer\n","        out = self.linear2(out)\n","        return out"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"7yjQ1KXOzJ_K","executionInfo":{"status":"ok","timestamp":1710361439300,"user_tz":180,"elapsed":23,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}}},"outputs":[],"source":["embedding_dim = 64\n","hidden_dim = 128\n","model = BengioModel(vocab_size, embedding_dim, context_size, hidden_dim)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"xmsD59TfzJ_K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710361439300,"user_tz":180,"elapsed":23,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"479a8180-b55a-48d6-f382-e5f6f79e32b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([128, 9])\n","torch.Size([128])\n"]}],"source":["sample = next(iter(train_loader))\n","input = sample[0]\n","target = sample[1]\n","\n","print(input.shape)\n","print(target.shape)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"HGbJcT5KzJ_K","executionInfo":{"status":"ok","timestamp":1710361439300,"user_tz":180,"elapsed":19,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}}},"outputs":[],"source":["output = model(input)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"um0lR4mNzJ_K","executionInfo":{"status":"ok","timestamp":1710361439300,"user_tz":180,"elapsed":19,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"48337e78-9e7b-4417-d309-c877a403232d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 465, 1550,  655, 2727, 2901, 2278,  998, 2926, 1802, 2469, 2411, 2007,\n","        1469, 1806, 2453, 2926, 1484,  737, 2411,  695, 2989, 2999, 2576, 2771,\n","        2926, 1643,  696, 1176, 1389,  754, 2662, 1384, 1484, 2397, 2453, 2762,\n","        1176, 1802, 1802, 1802, 1720, 2989,  859, 1537, 2504, 1752, 1720,  927,\n","        2926, 2695, 2758,  254, 1864, 1550, 2901, 1427,  342,   73, 2563, 2926,\n","        2242,  695, 1802, 1802,  388, 2737, 1462,   73, 2894, 2771, 1677, 2677,\n","        1872, 2926,  342, 1537,  939, 2759, 2182, 2284, 1677, 1025, 2236, 1034,\n","        1155, 1883, 1856,  313,  313, 2453, 1176, 2878,  338, 1404, 1785, 1722,\n","        1752,  334, 1389, 2282,   75, 1518,  121, 2926, 2005, 2926, 2453, 1484,\n","        1484, 1738, 2759, 1802, 2685, 1802,  616, 1720, 1176,  906,  939, 2453,\n","        2989, 1806,  705,  938,  363,  711,  711, 1759])"]},"metadata":{},"execution_count":15}],"source":["output.argmax(dim=1)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"la-b-f8jzJ_L","executionInfo":{"status":"ok","timestamp":1710361439301,"user_tz":180,"elapsed":17,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"02016ba3-a15c-4125-e4f8-4684f9b78540"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([   4,    4,   13,   10,   37,   11, 1053,  845,   70,   90,  401,    8,\n","        1095,   67,    0,    6,    7,  105,   75,    4,   42,    3,    0,    8,\n","           3, 1697,    1,  808,    5, 1371,    6,   13,   32,   83,   54, 1083,\n","          14,    7,    0,    4,    5,    2, 1057,    0, 1406,    4,  116,  692,\n","           4,    0,    5,    4,   21,  206,    0,    1,    0,   23,    3,    1,\n","        1324,    9,  958,    5,    0,    0, 2429, 2837,    1,   10, 2782,    0,\n","          27,   69,  168,    2,  379,    1,    2,    9,   16,  236, 2994,    5,\n","        1676,    0,    1,    3, 1589,  103,    8,    2,    2,  469,   41, 1460,\n","          11,  440,    8, 1505, 1048,   14,   61, 1222,    2, 2685,    5,    0,\n","           0,    1,    0,   12,  110,  273,  817,    1,    4,  270,    4,    0,\n","        2218,   53,  134, 2225, 2789,  136,  307,   49])"]},"metadata":{},"execution_count":16}],"source":["target"]},{"cell_type":"markdown","metadata":{"id":"UngUhyu7zJ_L"},"source":["## Training"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wntaV50nzJ_L","executionInfo":{"status":"ok","timestamp":1710361439301,"user_tz":180,"elapsed":14,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"6f108567-cb6c-4ff3-ac31-6aa728e868c8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":17}],"source":["# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível, caso contrário, usa a CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"vRwSPiwizJ_L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710361441479,"user_tz":180,"elapsed":2190,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"fce0578d-3004-486b-f49c-7b1203b3217c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["BengioModel(\n","  (embeddings): Embedding(3001, 64)\n","  (linear1): Linear(in_features=576, out_features=128, bias=True)\n","  (linear2): Linear(in_features=128, out_features=3001, bias=True)\n",")"]},"metadata":{},"execution_count":18}],"source":["epochs = 10\n","\n","# Learning rate\n","lr = 0.01\n","\n","# Cross Entropy\n","criterion = nn.CrossEntropyLoss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","model.to(device)"]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Exemplo de uso:\n","total_params = count_parameters(model)\n","print(f'O modelo tem um total de {total_params:,} parâmetros.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kajen3MAK1aO","executionInfo":{"status":"ok","timestamp":1710361441480,"user_tz":180,"elapsed":15,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"713b1039-bdf4-4409-a17c-3e4a8fd72ce2"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["O modelo tem um total de 653,049 parâmetros.\n"]}]},{"cell_type":"code","source":["# Initial Perplexity and Loss\n","# Before training\n","model.eval()\n","\n","loss = 0\n","perp = 0\n","\n","with torch.no_grad():\n","    for inputs, targets in train_loader:\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        outputs = model(inputs)\n","        loss += criterion(outputs, targets).item()\n","\n","loss /= len(train_loader)\n","perp = torch.exp(torch.tensor(loss))\n","\n","print(f'Initial Loss: {loss:.4f}')\n","print(f'Initial Perplexity: {perp:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kG0OZe_j6aj3","executionInfo":{"status":"ok","timestamp":1710361444192,"user_tz":180,"elapsed":2722,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"532dbbe6-193c-42b7-870b-b703bf4f5cfe"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial Loss: 8.0086\n","Initial Perplexity: 3006.7974\n"]}]},{"cell_type":"code","source":["# Training Loop\n","\n","for epoch in range(epochs):\n","  model.train()\n","\n","  # Metrics\n","  epoch_loss = 0\n","  epoch_correct = 0\n","  epoch_samples = 0\n","\n","  for inputs, targets in train_loader:\n","        inputs = inputs.to(device)  # Move input data to the device\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Loss\n","        epoch_loss += loss.item()\n","\n","        # Predicted\n","        _, predicted = torch.max(outputs, 1)\n","        epoch_correct += (predicted == targets).sum().item()\n","        epoch_samples += targets.size(0)\n","\n","  # Calculate average loss and accuracy for epoch\n","  avg_loss = epoch_loss / len(train_loader)\n","  acc = epoch_correct / epoch_samples\n","\n","  # Perplexity\n","  perp = torch.exp(torch.tensor(avg_loss))\n","\n","  # Print epoch statistics\n","  print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%, Perplexity: {perp:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYO6j4mE8CKg","executionInfo":{"status":"ok","timestamp":1710361465324,"user_tz":180,"elapsed":21677,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"6c8121a3-8f71-486c-ac70-9c5d9e6f8e3a"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 5.9605, Accuracy: 0.14%, Perplexity: 387.7994\n","Epoch [2/10], Loss: 5.3782, Accuracy: 0.15%, Perplexity: 216.6312\n","Epoch [3/10], Loss: 5.1707, Accuracy: 0.16%, Perplexity: 176.0393\n","Epoch [4/10], Loss: 5.0128, Accuracy: 0.17%, Perplexity: 150.3320\n","Epoch [5/10], Loss: 4.8967, Accuracy: 0.18%, Perplexity: 133.8424\n","Epoch [6/10], Loss: 4.7999, Accuracy: 0.19%, Perplexity: 121.5022\n","Epoch [7/10], Loss: 4.7320, Accuracy: 0.20%, Perplexity: 113.5183\n","Epoch [8/10], Loss: 4.6710, Accuracy: 0.21%, Perplexity: 106.8021\n","Epoch [9/10], Loss: 4.6319, Accuracy: 0.21%, Perplexity: 102.7060\n","Epoch [10/10], Loss: 4.5889, Accuracy: 0.21%, Perplexity: 98.3830\n"]}]},{"cell_type":"markdown","source":["## Avaliação"],"metadata":{"id":"PSXfwYISDoPN"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","model.eval()\n","\n","loss_sum = 0\n","total_sum = 0\n","correct_sum = 0\n","\n","with torch.no_grad():\n","    for inputs, targets in val_loader:\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = F.cross_entropy(outputs, targets)\n","        loss_sum += loss\n","\n","        # Get the predicted labels\n","        _, predicted = torch.max(outputs, 1)\n","\n","        total_sum += targets.size(0)\n","        correct_sum += (predicted == targets).sum().item()\n","\n","# Calculate accuracy\n","acc = 100 * correct_sum/total_sum\n","\n","# Calculate average perplexity\n","average_loss = loss_sum / len(val_loader)\n","average_perplexity = torch.exp(torch.tensor(average_loss))\n","\n","print(f'Test Accuracy: {acc}%')\n","print(f'Average Perplexity: {average_perplexity}')"],"metadata":{"id":"nXXO78GSDqPg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710361465942,"user_tz":180,"elapsed":631,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"575fb927-6382-4ea3-9fe2-a0e4a2401a49"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 15.575232121994713%\n","Average Perplexity: 340.0611267089844\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-9d4cc1515f8f>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  average_perplexity = torch.exp(torch.tensor(average_loss))\n"]}]},{"cell_type":"markdown","metadata":{"id":"A1zhxVqfzJ_M"},"source":["## Exemplo de uso"]},{"cell_type":"code","source":["# Código adaptado da implementação do Cesar Bastos\n","from colorama import Fore, Style\n","\n","text = cleaned_paragraphs\n","model.to(device)\n","def generate_text(model, vocab, text, max_length, context_size):\n","    words = []\n","    # Ensure there are enough words for at least one sequence\n","    while len(words) < context_size:\n","        random_number = random.randint(1, 4891)\n","        words = encode_sentence(text[random_number], vocab)\n","        if not words:\n","            words = []\n","            continue  # Skip if the sentence cannot be encoded\n","        words = words[:context_size]\n","        #print(words)\n","        if any(token == 0 for token in words):\n","            words = []\n","            continue  # Skip if any token is zero (assuming 0 is a special token)\n","        context = words\n","\n","    print(f\"Frase: {cleaned_paragraphs[random_number]}\")\n","    print(words)\n","\n","    for _ in range(max_length):\n","        words_tensor = torch.tensor(context[-context_size:], dtype=torch.long).unsqueeze(0).to(device)\n","        logits = model(words_tensor)\n","        probs = F.softmax(logits, dim=1)\n","        next_token = torch.multinomial(probs, num_samples=1)\n","        context.append(next_token.item())\n","        print(context)\n","    frase = []\n","    for i in context: ##Agradecimentos a Ramon Abilio\n","        word = next((word for word, code in vocab.items() if code == i), \"<UNKNOWN>\")\n","        frase.append(word)\n","\n","    print(f\"{Fore.BLUE}{frase[:context_size]}{Style.RESET_ALL} {Fore.RED}{frase[-max_length:]}{Style.RESET_ALL} \")\n","\n","context_size = 9\n","max_length= 10\n","generate_text(model, vocab, text, max_length, context_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EhP8xLaUbx6","executionInfo":{"status":"ok","timestamp":1710361465943,"user_tz":180,"elapsed":17,"user":{"displayName":"Fábio Grassiotto","userId":"09326997407296976199"}},"outputId":"5cbdc79f-0fa3-4613-a0be-98800804a083"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Frase: era o melhor leito que podia ter a menina no meio do deserto; puxou a\n","canôa, alcatifou o fundo com as folhas macias das palmeiras, e, tomando\n","cecilia nos braços, deitou-a no seu berço.\n","[28, 3, 506, 344, 2, 108, 152, 1, 52]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1, 674]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1, 674, 5]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1, 674, 5, 24]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1, 674, 5, 24, 0]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1, 674, 5, 24, 0, 0]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1, 674, 5, 24, 0, 0, 11]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1, 674, 5, 24, 0, 0, 11, 1991]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1, 674, 5, 24, 0, 0, 11, 1991, 4]\n","[28, 3, 506, 344, 2, 108, 152, 1, 52, 1, 674, 5, 24, 0, 0, 11, 1991, 4, 17]\n","\u001b[34m['era', 'o', 'melhor', 'leito', 'que', 'podia', 'ter', 'a', 'menina']\u001b[0m \u001b[31m['a', 'partio', 'e', 'lhe', '<UNKNOWN>', '<UNKNOWN>', 'da', 'escapou', 'de', 'pery']\u001b[0m \n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1QJ2sAt8MwXfIf8d0Rg0kZAgZrTiQ3ZqI","timestamp":1710156192124},{"file_id":"1hNRexSlh5CZj2etXscitB7hkQ54nkGv1","timestamp":1709768862713}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}