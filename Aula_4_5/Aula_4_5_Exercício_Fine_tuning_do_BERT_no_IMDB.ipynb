{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência\n",
        "\n",
        "Nome: Fabio Grassiotto RA 890441"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ80hHaftwUd"
      },
      "source": [
        "## Instruções:\n",
        "\n",
        "\n",
        "Treinar e medir a acurácia de um modelo BERT (ou variantes) para classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
        "\n",
        "Importante:\n",
        "- Deve-se implementar o próprio laço de treinamento.\n",
        "- Implementar o acumulo de gradiente.\n",
        "\n",
        "Dicas:\n",
        "- BERT geralmente costuma aprender bem uma tarefa com poucas épocas (de 3 a 5 épocas). Se tiver demorando mais de 5 épocas para chegar em 80% de acurácia, ajuste os hiperparametros.\n",
        "\n",
        "- Solução para erro de memória:\n",
        "  - Usar bfloat16 permite quase dobrar o batch size\n",
        "\n",
        "Opcional:\n",
        "- Pode-se usar a função trainer da biblioteca Transformers/HuggingFace para verificar se seu laço de treinamento está correto. Note que ainda assim é obrigatório implementar o laço próprio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers[torch]\n",
        "%pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ozXD-xYCcrT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Variáveis Globais e inicialização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seed\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Colab environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if (IN_COLAB):\n",
        "    # Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    project_folder=\"/content/drive/MyDrive/Classes/IA024/Aula_4_5\"\n",
        "    os.chdir(project_folder)\n",
        "    !ls -la\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbnfzst5O3k",
        "outputId": "bebda5c0-5614-4cd0-a2f4-5754cdb9c336"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"aclImdb.tgz\"):\n",
        "    print(\"Downloading Imdb dataset\")\n",
        "    !wget -nc http://files.fast.ai/data/aclImdb.tgz\n",
        "    !tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIN_xLI_TuT",
        "outputId": "787fc595-88b1-486a-8c0c-bcde36396793"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "#y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "#y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "y_train = [1] * len(x_train_pos) + [0] * len(x_train_pos)\n",
        "y_test = [1] * len(x_test_pos) + [0] * len(x_test_pos)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(x_train[0])\n",
        "print(y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenização do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bert base model\n",
        "model_name = \"bert-base-uncased\"\n",
        "# max sequence length\n",
        "max_length = 512\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=max_length)\n",
        "valid_encodings = tokenizer(x_valid, truncation=True, padding=True, max_length=max_length)\n",
        "test_encodings = tokenizer(x_test, truncation=True, padding=True, max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train_encodings[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classe do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImdbDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# convert our tokenized data into a torch Dataset\n",
        "train_dataset = ImdbDataset(train_encodings, y_train)\n",
        "valid_dataset = ImdbDataset(valid_encodings, y_valid)\n",
        "test_dataset = ImdbDataset(test_encodings, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelo do HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,   # batch size per device during training\n",
        "    per_device_eval_batch_size=20,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "    logging_steps=400,               # log & save weights each logging_steps\n",
        "    save_steps=400,\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # Model\n",
        "    args=training_args,                  # training args\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=valid_dataset,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluate the current model after training\n",
        "trainer.evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA024",
      "language": "python",
      "name": "ia024"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
