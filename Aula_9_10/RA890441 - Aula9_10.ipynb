{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Fabio Grassiotto  \n",
    "RA: 890441\n",
    "\n",
    "# Aula 9_10 - Implementar a ReAct  com LLaMa 3 70B (groq)\n",
    "\n",
    "- Testar no dataset do IIRC - 50 primeiras perguntas com resposta (test_questions.json em anexo)  \n",
    "- Usar o prompt do LLaMAIndex: https://github.com/run-llama/llama_index/blob/a87b63fce3cc3d24dc71ae170a8d431440025565/llama_index/agent/react/prompts.py  \n",
    "- Salvar as respostas finais das 50 perguntas no JSON para exercício futuro de avaliação  \n",
    "- Instruir o modelo a seguir a sequência Thougth, Action, Input, Observation (a observação não é do próprio modelo, mas resultado da busca)  \n",
    "- É necessário usar o parâmetro stop_sequence=\"Observation:\", para o modelo parar de gerar texto e esperar o retorno da busca. Implementem o código da busca e retornem os top-k documentos pro modelo (sugestão: k=5).  \n",
    "- Instruir o modelo agir passo-a-passo (decomposição da pergunta).  \n",
    "- Podem usar o LangChain, LLaMAindex ou outro framework. Ou implementar na mão.  \n",
    "- Usar a busca como ferramenta  \n",
    "- Usar o BM25 como buscador (repetir indexação do exercício passado)  \n",
    "- Usar a indexação do Visconde: https://github.com/neuralmind-ai/visconde/blob/main/iirc_create_indices.ipyn  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q torch\n",
    "%pip install groq\n",
    "%pip install -U sentence-transformers\n",
    "%pip install faiss-cpu\n",
    "%pip install spacy\n",
    "%pip install pandas\n",
    "%python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import groq\n",
    "from groq import Groq\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Colab environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if (IN_COLAB):\n",
    "    # Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "    project_folder=\"/content/drive/MyDrive/Classes/IA024/Aula_9_10\"\n",
    "    os.chdir(project_folder)\n",
    "    !ls -la\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_groq_key():\n",
    "    try:\n",
    "        # Open and read the entire content of the file\n",
    "        with open(\"groq-key.txt\", 'r') as file:\n",
    "            contents = file.read()\n",
    "        \n",
    "        return contents\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file does not exist.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Handle other potential exceptions (e.g., permission errors)\n",
    "        print(f\"An error occurred while reading the file: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "groq_key = load_groq_key()\n",
    "os.environ[\"GROQ_API_KEY\"] = groq_key\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def groq_chat(content):\n",
    "    try:\n",
    "\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            #\n",
    "            # Required parameters\n",
    "            #\n",
    "            messages=[\n",
    "                # Set an optional system message. This sets the behavior of the\n",
    "                # assistant and can be used to provide specific instructions for\n",
    "                # how it should behave throughout the conversation.\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"you are a helpful assistant.\"\n",
    "                },\n",
    "                # Set a user message for the assistant to respond to.\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": content,\n",
    "                }\n",
    "            ],\n",
    "\n",
    "            # The language model which will generate the completion.\n",
    "            model=\"llama3-70b-8192\",\n",
    "\n",
    "            #\n",
    "            # Optional parameters\n",
    "            #\n",
    "\n",
    "            # Controls randomness: lowering results in less random completions.\n",
    "            # As the temperature approaches zero, the model will become deterministic\n",
    "            # and repetitive.\n",
    "            temperature=0,\n",
    "\n",
    "            # The maximum number of tokens to generate. Requests can use up to\n",
    "            # 32,768 tokens shared between prompt and completion.\n",
    "            #max_tokens=10,\n",
    "\n",
    "            # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "            # likelihood-weighted options are considered.\n",
    "            top_p=1,\n",
    "\n",
    "            # A stop sequence is a predefined or user-specified text string that\n",
    "            # signals an AI to stop generating content, ensuring its responses\n",
    "            # remain focused and concise. Examples include punctuation marks and\n",
    "            # markers like \"[end]\".\n",
    "            stop=None,\n",
    "\n",
    "            # If set, partial message deltas will be sent.\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "    except groq.APIConnectionError as e:\n",
    "        print(\"The server could not be reached\")\n",
    "        print(e.__cause__)  # an underlying Exception, likely raised within httpx.\n",
    "    except groq.RateLimitError as e:\n",
    "        print(\"A 429 status code was received; we should back off a bit.\")\n",
    "    except groq.APIStatusError as e:\n",
    "        print(\"Another non-200-range status code was received\")\n",
    "        print(e.status_code)\n",
    "        print(e.response)\n",
    "    \n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUESTIONS = 50\n",
    "model_name = \"sentence-transformers/msmarco-MiniLM-L-6-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IIRC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset  = json.load(open('dataset/test_questions.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': {'type': 'span',\n",
       "  'answer_spans': [{'text': 'sky and thunder god',\n",
       "    'passage': 'zeus',\n",
       "    'type': 'answer',\n",
       "    'start': 83,\n",
       "    'end': 102}]},\n",
       " 'question': 'What is Zeus know for in Greek mythology?',\n",
       " 'context': [{'text': 'he Palici the sons of Zeus',\n",
       "   'passage': 'main',\n",
       "   'indices': [684, 710]},\n",
       "  {'text': 'in Greek mythology', 'passage': 'main', 'indices': [137, 155]},\n",
       "  {'text': 'Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion',\n",
       "   'passage': 'Zeus',\n",
       "   'indices': [0, 110]}],\n",
       " 'question_links': ['Greek mythology', 'Zeus'],\n",
       " 'title': 'Palici'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the first 50 questions with an answer\n",
    "questions_to_ask = []\n",
    "questions_found = 0\n",
    "\n",
    "documents = []\n",
    "all_titles = []\n",
    "\n",
    "for item in test_dataset:\n",
    "  \n",
    "  question = item['question']\n",
    "  answer = item['answer']\n",
    "  answer_type = answer['type']\n",
    "\n",
    "  if answer_type == 'binary' or answer_type == 'value':\n",
    "    final_answer = answer['answer_value']\n",
    "  elif answer_type == 'span':\n",
    "    final_answer = answer['answer_spans'][0]['text']\n",
    "  elif answer_type == 'none':\n",
    "    final_answer = 'none'\n",
    "  else:\n",
    "    final_answer = 'An error perhaps, bad type'\n",
    "    print(answer_type)\n",
    "\n",
    "  if (final_answer == 'none'):\n",
    "    # Skip this one.\n",
    "    continue\n",
    "  else:\n",
    "    # Thats a good question, grap the document and title associated with it.\n",
    "    \n",
    "    context_list = item['context']\n",
    "    for context in context_list:\n",
    "      text = context['text']\n",
    "      # clean up html\n",
    "      soup = BeautifulSoup(text, 'html.parser')\n",
    "      clean_text = soup.get_text()\n",
    "\n",
    "      documents.append({\n",
    "              \"title\": item['title'],\n",
    "              \"content\": clean_text\n",
    "          }\n",
    "      )\n",
    "      all_titles.append(item['title'].lower())\n",
    "\n",
    "    questions_to_ask.append({\"Question\": question, \"Answer\": final_answer})\n",
    "    questions_found += 1\n",
    "    if (questions_found == NUM_QUESTIONS):\n",
    "      # found our questions\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Palici', 'content': 'he Palici the sons of Zeus'},\n",
       " {'title': 'Palici', 'content': 'in Greek mythology'},\n",
       " {'title': 'Palici',\n",
       "  'content': 'Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
